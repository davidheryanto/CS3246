0001	Preliminary Report-International Algebraic Language		1958	Perlis, Samelson	
0002	Extraction of Roots by Repeated Subtractions for Digital Computers		1958	Sugai	
0003	Techniques Department on Matrix Program Schemes		1958	Friedman	
0004	Glossary of Computer Engineering and Programming Terminology		1958		
0005	Two Square-Root Approximations		1958	Wadey	
0006	The Use of Computers in Inspection Procedures		1958	Muller	
0007	Glossary of Computer Engineering and Programming Terminology		1958		
0008	On The Equivalence and Transformation of Program Schemes		1958	Friedman	
0009	Proposal for an UNCOL		1958	Conway	
0010	Glossary of Computer Engineering and Programming Terminology		1958		
0011	The Problem of Programming Communication with Changing Machines A Proposed Solution-Part 2		1958	Strong, Wegstein, Tritter, Olsztyn, Mock, Steel	
0012	Error Estimation in Runge-Kutta Procedures		1958	Call, Reeves	
0013	Glossary of Computer Engineering and Programming Terminology		1958		
0014	The Problem of Programming Communication with Changing Machines A Proposed Solution (Part 1)		1958	Strong, Wegstein, Tritter, Olsztyn, Mock, Steel	
0015	Recursive Curve Fitting Technique		1958	Giblin	
0016	Secant Modification of Newton's Method		1958	Jeeves	
0017	On Programming of Arithmetic Operations		1958	Ershov	
0018	Simple Automatic Coding Systems		1958	Adams, Schlesinger	
0019	Glossary of Computer Engineering and Programming Terminology		1958		
0020	Accelerating Convergence of Iterative Processes	A technique is discussed which, when applied to an iterative procedure for the solution of an equation, accelerates the rate of convergence if the iteration converges and induces convergence if the iteration diverges.  An illustrative example is given.	1958	Wegstein	
0021	Algebraic Formulation of Flow Diagrams		1958	Voorhees	
0022	Unusual Applications Department--Automatic Implementation of Computer Logic		1958	Morris, Wohr	
0023	Binary and Truth-Function Operations on a Decimal Computer with an Extract Command		1958	Kautz	
0024	An Improved Decimal Redundancy Check		1958	Sisson	
0025	General Purpose Programming Systems		1958	Holt	
0026	A Subroutine Method for Calculating Logarithms		1958	Bemer	
0027	Note On Empirical Bounds For Generating Bessel Functions		1958	Randels, Reeves	
0028	Request for Methods or Programs		1958	Corley	
0046	Multiprogramming STRETCH: Feasibility Considerations	The tendency towards increased parallelism in computers is noted.  Exploitation of this parallelism  presents a number of new problems in machine design and in programming systems.  Minimum requirements  for successful concurrent execution of several independent problem programs are discussed.  These requirements  are met in the STRETCH system by a carefully balanced combination of built-in and programmed logic.   Techniques are described which place the burden of the programmed logic on system programs (supervisory  program and compiler) rather than on problem programs.	1959	Codd, Lowry, McDonough, Scalzi	
0086	Survey of Progress and Trend of Development and Use of Automatic Data Processing in Business  and Management Control Systems of the Federal Government, as of December 1957		1959		Controller General of the United States to the Congress of the United States
0087	A Note on a Method for Generating Points Uniformly on N-Dimensional Spheres		1959	Muller	
0088	An Efficient Method for Generating Uniformly Distributed Points on the Surface of an n-Dimensional Sphere		1959	Hicks, Wheeling	
0089	A Routine to Find the Solution of Simultaneous Linear Equations with Polynomial Coefficients		1959	Larson, Marshall	
0090	Binary Arithmetic for Discretely Variable Word Length in a Serial Computer		1959	Ercoli, Vacca	
0091	A Mathematical Procedure for Machine Division		1959	Gilman	
0092	A Checklist of Intelligence for Programming Systems	A remarkable variation exists in the degree of sophistication of various programming systems.   A particular manifestation is the jungle of assorted devices for reproducing limited human decision  procedures.  An attempt is made here to begin a systematic classification of the various devices for  educating the computer to take over the decision-making functions of one or many human operators, both  those that have been demonstrated feasible to date and those that are highly desirable for the future.	1959	Bemer	
0093	From Formulas to Computer Oriented Language	A technique is shown for enabling a computer to translate simple algebraic formulas into a  three address computer code.	1959	Wegstein	
0094	An Iterative Method for Fitting the Logistic Curve	An iterative method is given for finding a logistic curve of best least squares fit to a set  of two-dimensional points.	1959	Howell	
0095	Elimination of Special Functions from Differential Equations	A set of ordinary differential equations which contains mathematical functions requiring the  use of subroutines for numerical solution by electronic computer, tabular data for numerical solution  by hand calculation or function generators when analog methods are applied can sometimes be expanded  to an equivalent set of equations which do not contain the functions.  This is practical if these functions  satisfy sufficiently simple differential equations.  Thus among those functions which can be eliminated  by this procedure are the trigonometric, inverse trigonometric, exponential, and many other transcendental  functions.	1959	Powers	
0096	On Computing Radiation Integrals	The relative merit and cost of four ways of evaluating typical radiation integrals containing  spherical Bessel functions are investigated.  These methods are desk machine evaluation of a finite series,  integration of the appropriate differential equation by a Reeves Electronic Analog Computer and by a  Litton 40 IBM 704 computer.  Results are generally applicable to equations separated from a Helmholtz  or wave equation.	1959	Hansen, Bailin, Rutishauser	
0097	Signal Corps Research and Development on Automatic Programming of Digital Computers		1959	Luebbert, Collom Jr.	
0098	The Arithmetic Translator-Compiler of the IBM FORTRAN Automatic Coding System		1959	Sheridan	
0099	Possible Modifications to the International Algebraic Language		1959	Green	
0100	Recursive Subscripting Compilers and List-Types Memories		1959	Carr III	
0101	Nuclear Reactor Codes		1959	Nather, Sangren	
0102	A Comparison of 650 Programming Methods		1960	Curtz, Riordan, Spohn	
0103	COPE (Console Operator Proficiency Examination)*	Each year electronic computers become more sophisticated, and the programs they must process  become more complex.  Because of this,dependence of those in computing on the skill and experience of  operators is increasing.  At the same time, selection and training of qualified operators grows more  difficult.  To meet the need for a quick, accurate, uniform operator test and training aid, the authors  have developed COPE (Console Operator Proficiency Examination), outlined below.  While this examination  is programmed specifically for the IBM 705 Model II with two Tape Record Coordinators, similar programs  could be developed for other computers.	1960	Farbman, Ketover	
0104	Digital Simulation of Discrete Flow Systems*	The discrete flow systems discussed are characterized by the movement of randomly arriving  items along interacting channels.  Programing a digital computer to simulate such systems utilizes some  techniques not common in other approaches to physical problems.  The principal portion of the paper is  a discussion of two simulation studies that illustrate some of the programming problems involved. One  is of an extensive package-handling plant, with the objective being optimization of parameters such as  storage capacities and processing rates.  In the other, air traffic flow and control procedures are simulated  to compare the effects of alternative control decisions.	1960	Moore, Lewis	
0105	Two Methods for Word Inversion on the IBM 709		1960	Price, Jardins	
0106	A Method for Overlapping and Erasure of Lists	An important property of the Newell-Shaw-Simon scheme for computer storage of lists is that  data having multiple occurrences need not be stored at more than one place in the computer.  That is,  lists may be "overlapped."  Unfortunately, overlapping poses a problem for subsequent erasure.  Given  a list that is no longer needed, it is desired to erase just those parts that do not overlap other lists.   In LISP, McCarthy employs an elegant but inefficient solution to the problem.  The present paper describes  a general method which enables efficient erasure.  The method employs interspersed reference counts to  describe the extent of the overlapping.	1960	Collins	
0107	Multiple Precision Arithmetic		1960	Pope, Stein	
0108	Programmed Error Correction in Project Mercury		1960	Dimsdale, Weinberg	
0109	A Note on Approximating e^x		1960	Lubkin	
0110	Fibonaccian Searching		1960	Ferguson	
0111	On Programming the Numerical Solution of Polynomial Equations	Numerical techniques are presented for computing the roots of polynomial equations.  By applying  the recommended scaling and inversion rules, the basic Bairstow and Newton-Raphson iterative techniques  can be applied with great reliability.  Both a high degree of accuracy and rapid convergence are realized.   Numerical examples are shown to illustrate the pitfalls and to show how these are circumvented by application  of the recommended procedures.	1960	Ellenberger	
0112	Numerical Solution of the Polynomial Equation (Algorithm 30)		1960	Ellenberger	
0113	Survey of Coded Character Representation		1960	Bemer	
0114	Survey of Punched Card Codes		1960	Smith, Williams	
0115	Optimizers: Their Structure		1960	Wheeling	
0116	The Sumador Chino	On a recent motor trip through Mexico, the writer came across on adding device which was referred  to as a sumador chino (Chinese adder).  A survey of the more available literature on the history of mathematics  and on instruments of calculation has uncovered no reference to such a device.  The purpose of this communication  is to enlist the help of other members in bringing to light whatever may be known concerning the evolution  and present status of the sumador chino.	1960	Rogers	
0117	An Estimation of the Relative Efficiency of Two Internal Sorting Methods		1960	Nagler	
0118	Character Scanning on the IBM 7070		1960	Speckhard	
0119	Note on Eigenvalue Computation		1960	Andrus	
0120	A Simple Technique for Coding Differential Equations		1960	Sefton, Vaillancourt	
0121	Over-all Computation Control and Labelling		1960	Holt	
0122	Least Squares Fitting of a Great Circle Through Points on a Sphere		1960	DeWitte	
0123	Compilation for Two Computers with NELIAC	NELIAC, a compiler based on ALGOL, was developed at the U. Navy Electronics Laboratory, San  Diego,California, as a"boot-strap" compiler for the Remington Rand Univac COUNTESS computer. This compiler  was used to generate a version of itself which, running as a COUNTESS program, generated machine code  for the Control Data Corporation CDC-1604.  All three versions of NELIAC accepted essentially identical  input language.	1960	Masterson Jr.	
0124	An Algorithm for the Assignment Problem	The assignment problem is formulated and briefly discussed.  An efficient algorithm for its  solution is presented in ALGOL code.  An empirical relation between solution time and the size of the  problem is given, based on extensive experiments carried out on a digital computer.	1960	Silver	
0125	Polynomial Transformer (Algorithm 29)		1960	Mackinney	
0126	Least Squares Fit By Orthogonal polynomials (Algorithm 28)		1960	Mackinney	
0127	ASSIGNMENT (Algorithm 27)		1960	Silver	
0128	ROOTFINDER III (Algorithm 26)		1960	Herroit	
0129	ROOTFINDER II (Algorithm 15)		1960	Forsythe, Herriot	
0130	Real Zeros of an Arbitrary Function (Algorithm 25)		1960	Leavenworth	
0131	Solution of Tri-Diagonal Linear Equations (Algorithm 24)		1960	Leavenworth	
0132	Math Sort (Algorithm 23)		1960	Feurzeig	
0133	Riccati-Bessel Functions of First And Second Kind (Algorithm 22)		1960	Oser	
0141	Some Thoughts on Parallel Processing		1960	Yarbrough	
0217	Soviet Computer Technology-1959		1960		
0218	Computer Preparation of a Poetry Concordance		1960	Painter	
0219	Marriage-with Problems		1960	Shuchter	
0220	A New Method of Computation of Square Roots Without Using Division		1960	Traub	
0221	The Basic Side of Tape Labeling		1960	Logan	
0222	Coding Isomorphisms	The coding of external symbols into symbols internal to a compute can sometimes be carried  out in such a way that relevant informational properties are preserved, but in a form much more easily  dealt with.  A case in point is presented.	1960	Lynch	
0223	Selfcipher: Programming		1960	Pelta	
0239	Inefficiency of the Use of Boolean Functions for Information Retrieval Systems		1961	Verhoeff, Goffman, Belzer	
0268	Stochastic Evaluation of a Static Storage Allocation		1961	Cohen	
0392	Comment on A Paper on Parallel Processing		1961	Nekora	
0405	An Algorithm for Coding Efficient Arithmetic Operations	Most existing formula translation schemes yield inefficient coding.  A method is described  which reduces the number of store and fetch operations, evaluates constant subexpressions during compilation,  and recognizes many equivalent subexpressions.	1961	Floyd	
0440	Record Linkage	Special difficulties are encountered in devising reliable systems for searching and updating  any large files of documents that must be identified primarily on the basis of names and other personal  particulars.  The underlying problem is that of making nearly maximum use of items of identifying information  that are individually unreliable but that may collectively be of considerable discriminating power.   Rules that can be applied generally to name retrieval systems have been developed in a methodological  study of the linkage of vital and health records into family groupings for demographic research purposes.   These rules are described, and the ways in which information utilization for matching may be optimized  are discussed.	1962	Newcombe, Kennedy	
0634	Manipulation of Trees in Information Retrieval*		1962	Salton	
0757	Simulation of a Traffic Network		1963	Katz	
0863	Multiphase Sorting		1963	Manker	
0944	Storage and Search Properties of a Tree-Organized Memory System	A memory with list properties [1] may be used to construct numeric, alphabetic or alphanumeric  trees.  Such trees have information storage and retrieval properties applicable to problems involving  large quantities of data or to problems where the quantity, word length and distribution of stored information  is not known a priori, or changes rapidly during the processing.  The purpose of this paper is to examine  the storage and search properties of a tree-organized storage system assuming that a memory possessing  certain list properties is available.  Of prime interest is the application where a symbol table, dictionary  or similar file is to be stored and searched.	1963	Scidmore, Weinberg	
0950	Parallel Methods for Integrating Ordinary Differential Equations	This paper is dedicated to the proposition that, in order to take full advantage for real-time  computations of highly parallel computers as can be expected to be available in the near future, much  of numerical analysis will have to be recast in a more "parallel" form.  By this is meant that serial  algorithms ought to be replaced by algorithm which consist of several subtasks which can be computed  without knowledge of the results of the other subtasks.  As an example, a method is proposed for "parallelizing"  the numerical integration of an ordinary differential equation, which process, by all standard methods,  is entirely serial.	1964	Nievergelt	
0963	Relative Effects of Central Processor and Input-Output Speeds Upon Throughput on the Large Computer	Presented in this paper is a technique for determining the relative effects of the internal  speed of the computer and the speed of the input-output units upon the overall speed of the system. Equations  are derived which permit the determination of these effects from hardware usage measurements.	1964	White	
1032	Theoretical Considerations in Information Retrieval Systems	Information storage and retrieval systems are composed of three major components: (a) identification  of information and tagging it for effective retrieval, (b) searching strategy, how to enter the file  to circumvent the scanning of nonrelevant material, and (c) file organization to make access to information  efficient.  For identification of information the paper suggests that a metalanguage (recently discussed  in a paper by Goffman, Verhoeff and Belzer) associated with an object language be used.  For searching  strategy, a linear model for an evaluation function of relevancy is developed which rewards the system  for retrieving relevant documents and not retrieving the nonrelevant, and penalizes the system for the  escaped relevant documents and false drops.  The inadequacies of a linear model are indicated.  Two approaches  to file organization are discussed.  One is self-organization of the file based on its history and past  performance, and the second is a self-generating subset of the file with a high probability of being  relevant.	1964	Belzer, Goffman	
1043	Talk-A High-Level Source Language Debugging Technique With Real-Time Data Extraction	TALK, meaning Take A Look, is a debugging technique which aids substantially in debugging complex  real-time programming systems by interrupting the users program at desired points to extract previously  specified data.  The extracted data is later edited, listing the associated data with its high-level  source language identification.	1964	verSteeg	
1112	A Technique for Computer Detection and Correction of Spelling Errors*	The method described assumes that a word which cannot be found in a dictionary has at most  one error, which might be a wrong, missing or extra letter or a single transposition.  The unidentified  input word is compared to the dictionary again, testing each time to see if the words match-assuming  one of these errors occurred.  During a test run on garbled text, correct identifications were made for  over 95 percent of these error types.	1964	Damerau	
1134	Some Effects of the 6600 Computer on Language Structures*	The problem of compiling efficient 6600 codes prompted the development of an intermediate language  reflecting the structure of the machine, that is more easily manipulated in improving object program  efficiency.  The subject of this paper is the intermediate language and methods of manipulating it.   Compilations of a series of arithmetic statements are discussed.  It is assumed that all functions and  exponentials have been removed from these statements, and replaced by simple variables.  For purposes  of simplicity the treatment of subscripts is ignored.  A simplified 6600 structure is presented to illustrate the compiling method.  Several assumptions are made for purposes of simplification, although there are  cases in which the assumptions are violated in the actual machine.	1964	Allard, Wolf, Zemlin	
1152	Floating-Point Arithmetic with 84-Bit Numbers	A classic and straightforward technique is presented which is not limited to the size or type  of number representation used or multiple precision arithmetic.	1964	Gregory, Raney	
1158	Program Structures for Parallel Processing	Constructs for organizing and explicating parallel program segments are discussed as extensions  to ALGOL 60.  The constructs serve as meta-commands and are motivated by equipment having multiprocessing  capability.	1965	Anderson	
1170	Analyzing English Syntax with a Pattern-Learning Parser	A dependency analysis system based on pattern recognition and learning logic was developed  to infer word classes and rules of syntactic combination from experience with text which had been analyzed.   The characteristics used to form word classes are the depth in the dependency tree of each word, the  direction of its governor and the same features for each of its immediate neighbors. Syntactic rules  of combination show the relation of a word to its governor in the depth pattern of the sentence.  The  system was tested on 400 elementary basic English sentences including 300 used earlier by Knowlton in  a different learning parser of all 400 sentences.  After experience with 300 sentences it was able to  generalize with 77 percent accuracy to the next 100.  In accumulative learning trials after the first  200 sentences it averaged a probability of .9 for accurately parsing each new sentence it encountered.   It was concluded that the system is adequate for learning to parse the bulk of basic English but that  further development is required before conclusions about its application to ordinary English can be stored.   The system is operational and available on the ARPA/SDC time-shared computing system.	1965	McConlogue, Simmons	
1188	An ALGOL-like Computer Design Language	The idea of constructing a computer design language by making use of an ALGOL-like programming  language is presented.  A computer designer can benefit from using a design language at a higher level  just as a computer user can benefit from a higher level programming language.  The purposes and requirements  of the design language are enumerated.  To achieve most of the purposes a translator is required to translate  a design of computer logic into a set of Boolean equations.  The design language is presented in terms  of vocabulary, statements, sequences and microprogram.  Included are examples of identifiers, expressions  with both unary and binary operators, declaration statements, transfer statements, terminal statements,  exchange statements, if statements, do statements, go to statements, several sequences and a microprogram.	1965	Chu	
1198	Solution of a Problem in Concurrent Programming Control	A number of mainly independent sequential-cyclic processes with restricted means of communication  with each other can be made in such a way that at any moment one and only one of them is engaged in the  "critical section" of its cycle.	1965	Dijkstra	
1223	High Speed Compilation of Efficient Object Code	A three-pass compiler with the following properties is briefly described:  The last two passes  scan an intermediate language produced by the preceding pass in essentially the reverse of the order  in which it was generated, so that the first pass is the only one which hasto read the bulky problem-oriented  input.  The double scan, one in either direction, performed by the first two passes, allows the compiler  to remove locally constant expressions and recursively calculable expressions from loops and to do the  important part of common subexpression recognition.  Optimization such as the effective use of index  registers, although as important, is not discussed since the object code which would be most efficient  is highly machine dependent.  The discussion is in terms of a FORTRAN-like language, although the technique  is applicable to most algebraic languages.	1965	Gear	
1231	Peephole Optimization	Redundant instructions may be discarded during the final stage of compilation by using a simple  optimizing technique called peephole optimization. The method is described and examplesare given.	1965	McKeeman	
1235	A Stochastic Approach to the Grammatical Coding of English	A computer program is described which will assign each word in an English text to its form  class or part of speech.  The program operates at relatively high speed in only a limited storage space.   About half of the word-events in a corpus are identified through the use of a small dictionary of function  words and frequently occurring lexical words.  Some suffix tests and logical-decision rules are employed  to code additional words.  Finally, the remaining words are assigned to one class or another on the basis  of the most probable form classes to occur within the already identified contexts.  The conditional probabilities  used as a basis for this coding were empirically derived from a separate hand-coded corpusn preliminary  trials, the accuracy of the coder was 91% to 93%, with obvious ways of improving the algorithm being  suggested by an analysis of the results.	1965	Stolz, Tannenbaum, Carstensen	
1236	The SMART Automatic Document Retrieval System-An Illustration	A fully automatic document retrieval system operating on the IBM 7094 is described.  The system  is characterized by the fact that several hundred different methods are available to analyze documents  and search requests.  This feature is used in the retrieval process by leaving the exact sequence of  operations initially unspecified, and adapting the search strategy to the needs of individual users.   The system is used not only to simulate an actual operating environment, but also to test the effectiveness  of the various available processing methods.  Results obtained so far seem to indicate that some combination  of analysis procedures can in general be relied upon to retrieve the wanted information.  A typical search  request is used as an example in the present report to illustrate systems operations and evaluation procedures.	1965	Salton, Lesk	
1262	Procedure-Oriented Language Statements to Facilitate Parallel Processing	Two statements are suggested which allow a programmer writing in a procedure-oriented language  to indicate sections of program which are to be executed in parallel.  The statements are DO TOGETHER  and HOLD.  These serve partly as brackets in establishing a range of parallel operation and partly to  define each parallel path within this range.  DO TOGETHERs may be nested.  The statements should be particularly  effective for use with computing devices capable of attaining some degree of compute-compute overlap.	1965	Opler	
1265	On the Relative Efficiencies of Context-Free Grammar Recognizers	A number of diverse recognition procedures that have been proposed for parsing sentences with  respect to a context-free grammar are described in this paper by means of a common device.  Each procedure  is defined by giving an algorithm for obtaining a nondeterministic Turing Machine recognizer that is  equivalent to a given context-free grammar.  The formalization of the Turing Machine has been chosen  to make possible particularly simple description of the parsing procedures considered.  An attempt has  been made to compare recognition efficiencies for the procedures defined.  For a few simple grammars  and sentences a formal comparison has been made.  Empirical comparison of the recognition of more realistic  programming languages such as LISP and ALGOL has been made by means of a program which simulates the  Turing Machine on the Univac M-460 computer.  Several algorithms for producing grammars equivalent to  a given context-free grammar have been considered, and the increase in recognition efficiency they afford  has been empirically investigated.	1965	Griffiths, Petrick	
1306	A Class of Unambiguous Computer Languages	Discussed in this paper is the concept of a fully nested computer language which may be one  means of designing computer languages which would be completely free of ambiguities.  Several suggestions  are also given here for the redefinition of ALGOL as a fully nested language.	1965	Johnston	
1314	The Organization of Symbol Tables	An efficient symbol table organization is an important feature in the design of any compiler.   During the construction of the Virginia ALGOL 60 compiler for the Burroughs B205, the primary consideration  in the symbol table design was that the recognition of identifiers and reserved words should be as rapid  as possible.  The general features of the technique are described.	1965	Batson	
1324	Answering English questions by Computer: A Survey	Fifteen experimental English language question-answering systems which are programmed and operating  are described and reviewed.  The systems range from a conversation machine to programs which make sentences  about pictures and systems which translate from English into logical calculi.  Systems are classified  as list-structured data-based, graphic data-based, text-based and inferential.  Principles and methods  of operations are detailed and discussed.  It is concluded that the data-base question-answer has passed  from initial research into the early developmental phase.  The most difficult and important research  questions for the advancement of general-purpose language processors are seen to be concerned with measuring,  dealing with ambiguities, translating into formal languages and searching large tree structures.	1965	Simmons	
1350	The Augmented Predictive Analyzer for Context-Free Languages-Its Relative Efficiency	It has been proven by Greibach that for a given context-free grammar G, a standard-form grammar  Gs can be constructed, which generates the same languages as is generated by G and whose rules are all  of the form Z --> cY(1) ... Y(m), (m >= O) where Z and Y(i) are intermediate symbols and c a terminal  symbol.  Since the predictive analyzer at Harvard uses a standard-form grammar, it can accept the language  of any context-free Grammar G, given an equivalent standard-form grammar Gs.  The structural descriptions  SD(Gs,X) assigned to a given sentence X by the predictive analyzer, however, are usually different from  the structural descriptions SD(G,X) assigned to the same sentence by the original context-free grammar  G from which Gs is derived.  In Section 1, an algorithm, originally due to Abbott is described standard-form  grammar each of whose rules is in standard form, supplemented by additional information describing its  derivation from the original context-free grammar.  A technique for performing the SD(Gs,X) to SD(G,X)  transformation effectively is also described.  In section 2, the augmented predictive analyzer as a parsing  algorithm for arbitrary context-free languages is compared with two other parsing algorithms: a selective  top-to-bottom algorithm similar to Irons' "error correcting parse algorithm" and an immediate constituent  analyzer which is an extension of Sakai-Cocke's algorithm for normal grammars.  The comparison is based  upon several criteria of efficiency, covering core-storage requirements, complexities of the programs  and processing time.	1966	Kuno	
1358	Syntax Macros and Extended Translation	A translation approach is described which allows one to extended the syntax and semantics of  a given high-level base language by the use of a new formalism called a syntax-macro.  Syntax-macros  define string transformations based on syntactic elements of the base language.  Two types of macros  are discussed, and examples are given of their use.  The conditional generation of macros based on options  and alternatives recognized by the scan are also described.	1966	Leavenworth	
1380	SIMULA-an ALGOL-Based Simulation Language	This paper is an introduction to SIMULA, a programming language designed to provide a systems  analyst with unified concepts which facilitate the concise description of discrete event systems.  A  system description also serves as a source language simulation program.  SIMULA is an extension of ALGOL  60 in which the most important new concepts is that of quasi-parallel processing.	1966	Dahl, Nygaard	
1396	Survey of Formula Manipulation	The field of formula manipulation is surveyed, with particular attention to the specific capabilities  of differentiation, integration and the supporting capabilities of simplification, displays and input/output  editing, and precision arithmetic.  General systems-both batch and online-are described.  Finally, some  programs to solve specific applications are discussed.	1966	Sammet	
1408	The Eschenbach Drum Scheme	The prime function of a drum, operating in real time, is to perform accesses quickly.  The  usual means for increasing this capacity is to incorporate engineering or hardware improvements.  In  this paper the problem is attacked not by changing the drum, but rather by modifying the manner in which  it operates.  At the outset, a drum is given a functional definition.  Then a simple design scheme (Eschenbach)  is introduced which enormously increases the rate of accessing for drums so defined.  This is shown to  enable a system to perform a job by employing fewer or less expensive drums.  It is suggested that although  the design scheme has a specific use, the method underlying it has more general applicability.  The question  of the efficacy of the drum scheme is then raised.  To deal with this, a standard of efficiency is developed  in light of realistic real-time circumstances.  The drum scheme is then modelled in a manner which permits  it to be analyzed as a problem in queueing theory.  Thus one is enabled to ascertain whether the drum  scheme is efficient enough for its application.  Again, whereas the analysis of the drum scheme has a  specific use, the methods underlying it have more general applicability.	1966	Weingarten	
1456	Storage and Retrieval of Aspects of Meaning in Directed Graph Structures	An experimental system that uses LISP to make a conceptual dictionary is described.  The dictionary  associates with each English word the syntactic information, definitional material, and references to  the contexts in which it has been used to define other words.  Such relations as class inclusion, possession,  and active or passive actions are used as definitional material.  The resulting structure serves as a  powerful vehicle for research on the logic of question answering.  Examples of methods of inputting information  and answering simple English questions are given.  An important conclusion is that, although LISP and  other list processing languages are ideally suited for producing complex associative structures, they  are inadequate vehicles for language processing on any large scale-at east until they can use auxiliary  memory as a continuous extension of core memory.	1966	Simmons	
1457	Data Manipulation and Programming Problems in Automatic Information Retrieval	Automatic information retrieval programs require the manipulation of a variety of different  data structures, including linear text, sparse matrices, and tree or list structures.  The main data  manipulations to be performed in automatic information systems are first briefly reviewed.  A variety  of data representations which have been used to describe structured information are then examined, and  the characteristics of various processing languages are outlined in the light of the procedures requiring  implementation.  Advantages of these programming languages for the retrieval application are examined,  and suggestions are made for the design of programming facilities to aid in information retrieval.	1966	Salton	
1471	Programming Semantics for Multiprogrammed computations	The semantics are defined for a number of meta-instructions which perform operation essential  to the writing of programs in multiprogrammed computer systems.  These meta-instructions relate to parallel  processing, protection of separate computations, program debugging, and the sharing among users of memory  segments and other computing objects, the names of which are hierarchically structured.  The language  sophistication contemplated is midway between an assembly language and an advanced algebraic language.	1966	Dennis, VanHorn	
1491	EULER: A Generalization ALGOL, and its Formal Definition: Part I*	A method for defining programming languages is developed which introduces a rigorous relationship  between structure and meaning.  The structure of a language is defined by a phrase structure syntax,  the meaning in terms of the effects which the execution of a sequence of interpretation rules exerts  upon a fixed set of variables, called the Environment.  There exists a one-to-one correspondence between  syntactic rules and interpretation rules is determined by the sequence of corresponding syntactic reductions  which constitute a parse.  The individual interpretation rules are explained in terms of an elementary  an d obvious algorithmic notation.  A constructive method for evaluating a text is provided, and for  certain decidable classes of languages their unambiguity is proved.  As an example, a generalization  of ALGOL is described in full detail to demonstrate that concepts like block-structure, procedures, parameters,  etc. can be defined adequately and precisely by this method.	1966	Wirth, Weber	
1514	On the Expected Gain From Adjust ing Matched Term Retrieval Systems	A file adjustment procedure based on maximizing the Bayes expected gain proposed for matched  term retrieval systems.  The expected gain and its probability distribution are derived as a function  of: (1) the prior proportion of omitted terms, and (2) the coefficient of separation between two distributions  corresponding to values of an adjustment statistic.  An example evaluates the gain parameters for a typical  information retrieval system.	1967	Shumway	
1515	A Computer System for Inference Execution and Data Retrieval	This paper presents a RAND project concerned with the use of computers as assistants in the  logical analysis of large collections of factual data.  A system called Relational Data File was developed  for this purpose.  The Relational Data File is briefly detailed and problems arising from its implementation  are discussed.	1967	Levien	
1518	An Experimental Model of System/360	The problem of predicting the performance of modern computer systems is formidable.  One general  technique which can ease this problem is macroscopic simulation.  This paper reports on the applicability  of that technique to System/360.  The paper describes an experimental model of System/360-its hardware,  software, and its environment.  The measures of system performance produced by the model consist of statistics  relating to turnaround time, throughput, hardware utilization, software utilization, and queueing processes.   The model is mechanized in SIMSCRIPT and consists of some 1750 statements.  An auxiliary programs, the  Job Generator, creates automatically the properties of System/360 jobs that get simulated.	1967	Katz	
1523	SHARER, a Time Sharing System for the CDC 6600	A time sharing system embedded within the standard batch processing system for the CDC 6600  is described.  The system is general purpose and file-based, providing facilities for file input, manipulation,  editing, compilation, and conversational execution.  It uses a simple scheme for system extension for  a machine with only one relocation and memory bound register.  No attempt was made to use reentrant code,  or to simulate segmentation or paging.  Implementation time was approximately six man-years, with the  majority of the code being written in FORTRAN.	1967	Harrison, Schwartz	
1526	Multiprogramming under a Page on Demand Strategy	A model of multiprogramming for a particular computer system using a page on demand strategy  is developed.  Analysis of this model is used to predict performance (measured by the average usage of  the CPU) when user programs are typical of those arising from an interactive time sharing environment.   The effect of several hardware modifications is also analyzed.  A parameter, readily calculated from  the hardware characteristics and the program statistics, is proposed for gauging the effect of multiprogramming.	1967	Smith	
1533	A Marovian Model of the University of Michigan Executive System	A mathematical model of a computer's executive system is postulated and its parameters estimated  with the aid of extensive data on the system's operation.  Although simplifying assumptions are made,  the results predicted by the model agree reasonable well with actual results.  The model is used to study  the effects of changes in the executive system and in one of its compilers.  Further applications of  the model are discussed.	1967	Foley	
1535	A Comment on Index Register Allocation	A technique is presented to reduce the enumeration required by a known procedure for optimal  index register allocation in straight-line programs.  This technique is based on the construction of  a link diagram, which shows at any step the future occurrences of indexes which must be loaded into index  registers.  This diagram determines in advance the required register configuration at certain steps of  the program, so that the program is subdivided into separate portions to which the allocation procedure  may be applied independently.	1967	Luccio	
1543	Computer Formulation of the Equations of Motion Using Tensor Notation	A means is described for extending the area of application of digital computers beyond the  numerical data processing stage and reducing the need for human participation in the formulation of certain  types of computer problems.  By the use of tensor calculus and a computer language designed to facilitate  symbolic mathematical computation, a method has been devised whereby a digital computer can be used to  do non-numeric work, that is, symbolic algebraic manipulation and differentiation. To illustrate the  techniques involved, a digital computer has been used to derive the equations of motion of a point mass  in a general orthogonal curvilinear coordinate system.  Since this operation involves a formulation in  terms of first- and second-order differential coefficients, it provides a good demonstration of a computer's  capability to do non-numeric work and to assist in the formulation process which normally precedes the  numerical data processing stage.  Moreover, this particular problem serves to illustrate the advantages  of the mathematical techniques employed.  With the program prepared for this purpose the computer will  derive the equations of motion in any coordinate system requested by the user.   Results are presented  for the following coordinate systems: cylindrical polar, spherical polar, and prolate spheroidal.	1967	Howard	
1551	On Compiling Algorithms for Arithmetic Expressions	This paper deals with algorithms concerning arithmetic expressions used in a FORTRAN IV compiler  for a HITAC-5020 computer having n accumulators.  The algorithms generate an object code which minimizes  the frequency of storing and recovering the partial results of the arithmetic expressions in cases where  there are several accumulators.	1967	Nakata	
1563	A Method for Finding Hamilton Paths and Knight's Tours	The use of Warnsdorff's rule for finding a knight's tour is generalized and applied to the  problem of finding a Hamilton path in a graph.  A graph-theoretic justification for the method is given.	1967	Pohl	
1572	The Simulation of Time sharing Systems	The development of new large scale time-sharing systems has raised a number of problems for  computation center management.  Not only is it necessary to develop an appropriate hardware configuration  for these systems, but appropriate software adjustments must be made.  Unfortunately, these systems often  do not respond to changes in the manner that intuition would suggest, and there are few guides to assist  in the analysis of performance characteristics.  The development of a comprehensive simulation model  to assist in the investigation of these questions is described in this paper.  The resulting model has  a general purpose design and can be used to study a variety of time-sharing systems.  It can also be  used to assist in the design and development of new time-sharing algorithms or techniques.  For the sake  of efficiency and greater applicability, the model was implemented in a limited FORTRAN subset that is  compatible with most FORTRAN IV compilers. The use of the simulation is demonstrated by a study of the  IBM 360/67 time-sharing system.	1967	Nielsen	
1601	Parallel Numerical Methods for the Solution of Equations	Classical iterative procedures for the numerical solution of equations provide at each stage  a single new approximation to the root in question.  A technique is given for the development of numerical  procedures which provide, at each stage, several approximations to a solution of an equation.  The s8everal  approximations obtained in any iteration are computationally independent, making the methods of interest  in a parallel processing environment.  Convergence is insured by extracting the "best information" at  each iteration.  Several families of numerical procedures which use the technique of the procedures in  a parallel processing environment are developed and measurements of these statistics are reported.  These  measurements are interpreted in a parallel processing environment.  In such an environment the procedures  obtained are superior to standard algorithms.	1967	Shedler	
1613	One-Pass Compilation of Arithmetic Expressions for a Parallel Processor	Under the assumption that a processor may have a multiplicity of arithmetic units, a compiler  for such a processor should produce object code to take advantage of possible parallelism of operation.   Most of the presently known compilation techniques are inadequate for such a processor because they  produce expression structures that must be evaluated serially.  A technique is presented here for compiling  arithmetic expressions into structures that can be evaluated with a high degree of parallelism.  The  algorithm is a variant of the so-called "top-down" analysis technique, and requires only one pass of  the input text.	1967	Stone	
1625	On the Automatic Simplification of Source-Language Programs	Methods of simplification that can be applied automatically to programs written in an ALGOL-like  language are discussed.  The simplifications are based on the form of the program and the knowledge obtained  by a processor, without any understanding of what the program is supposed to do.  These methods have  been implemented in a processor called SURE that accepts a program written in JOVIAL and outputs an equivalent  JOVIAL program that may be shorter and may be executed faster than the original.  SURE is described,  some of the problems encountered in automatic improvement at the source-language level are discussed,  and further types of automatic program improvement are suggested.	1967	Clark	
1653	System Performance Evaluation: Survey and Appraisal	The state of the art of system performance evaluation is reviewed and evaluation goals and  problems are examined.  Throughput, turnaround, and availability are defined as fundamental measures  of performance; overhead and CPU speed are placed in perspective.  The appropriateness of instruction  mixes, kernels, simulators, and other tools is discussed, as well as pitfalls which may be encountered  when using them.  Analysis, simulation, and synthesis are presented as three levels of approach to evaluation,  requiring successively greater amounts of information.  The central role of measurement in performance  evaluation and in the development of evaluation methods is explored.	1967	Calingaert	
1675	A Note on a Relevance Estimate and Its Improvement	In this paper the effect of iterating the improvement procedure is examined.  It is shown that  applications of the improvement factor beyond the first time are ineffectual, and that the factor is  but a scale factor.	1968	Korfhage	information retrieval, relevance, indexing, classification
1681	Easy English,a Language for Information Retrieval Through a Remote Typewriter Console	Easy English is a natural command language designed to simplify communication between man and  machine through remote typewriter console.  It has been developed for retrieval of documents from a computerized  data base, the Moore School Information Systems Laboratory files.  Requests are formulated in a standardized  syntactical form (examples of which are presented), and this form is then transformed into an equivalent  query expressed in the retrieval system's original Symbolic Command Language, which is briefly described.   Operation of easy English is detailed by illustration of the transformations performed upon a sample  request up to the point at which the request string is sent to the system.  A macro flowchart of Easy  English is included, and an Appendix provides the printout of a retrieval demonstration.	1968	Rubinoff, Bergman, Cautin, Rapp	natural language communication, on-line searching, remote console communication, information retrieval, man-machine communication, remote terminal communication, translator, document retrieval, conversational  mode, information retrieval language, symbolic command language
1683	Boolean matrix Methods for the Detection of Simple Precedence Grammars	A mechanical procedure is derived for determining whether a given context-free phrase structure  grammar is a simple precedence grammar.  This procedure consists of elementary operations on suitably  defined Boolean matrices.  Application of the procedure to operator grammars is also given.	1968	Martin	syntax analysis, precedence analysis, simple precedence grammar, simple precedence language, operator  grammar, operator precedence, compilers, bounded-context syntactic analysis, Boolean matrices, relations
1696	An Algorithm for Identifying the Ergodic Subchains and Transient States of a Stochastic Matrix	An algorithm for identifying the ergodic subchains and transient states of a stochastic matrix is presented.  Applications in Markov renewal programming and in the construction of variable length  codes are reviewed, and an updating procedure for dealing with certain sequences of stochastic matrices  is discussed.  Computation times are investigated experimentally and compared with those of another recently  propose method.	1968	Fox, Landi	stochastic matrix, ergodic, chain identification
1698	A Statistical Model for Console Behavior in Multiuser Computers	The ability of a computer system to communicate with the outside world efficiently is as important  as its ability to perform computations efficiently.  It is quite difficult to characterize a particular  user, but rather easy to characterize the entire user community.  Based on the properties of this community  we have postulated a hypothetical "virtual console."  No claim is made that a virtual console behaves  like any actual console, but the entire collection of virtual consoles models the collection of actual  consoles.  Using the model we answer questions like:  How many processes are suspended waiting for console  input?  What is the maximum rate at which a process can execute?  What bounds can be set on overall buffer  requirements?  Answers to these and similar questions are needed in certain aspects of operating system  design.	1968	Denning	statistical models for input-output, operating system design, input-output design
1700	PEEKABIT, Computer Offspring of Punched Card PEEKABOO, for Natural Language Searching	The "peekaboo" idea from punched card information retrieval methods has been mated with the  idea of superimposed punching to produce a programming technique which cuts computer run time in half  on a test search of 33,000 subject index entries.  A search program using the device has been operational  since late 1963.  As an item is entered in the store, an 18-byte mask is created from the item's meaningful  words using the inclusive OR operation.  If, at search time, the logical product (using the AND operation)  of this mask and a similarly constructed question mask is not equal to the question mask, then one or  more question words are not present in the store item.  An equality is in conclusive; the words of the  store item must be unpacked and compared with question words.  The present store is made up of over 600,000  subject index entries estimated to average 60 characters each.  Longer texts, such as abstracts, could  be handled by multiple masks.	1968	Hutton	peekaboo, superimposed coding, natural language searching text searching, information compaction, computer search technique
1719	A Methodology for Calculating and Optimizing Real-Time System Performance	The continually increasing size, complexity, number of types, and cost of data processing systems  are causing serious re-examination within government and industry of the criteria for and methods of  calculating and optimizing data processing system cost and performance.  Real-time data processing systems  as typified by the automated airline reservation system are discussed in this paper.  Criteria for evaluating  performance are described; a methodology for calculating and optimizing is outlined; and the method is  illustrated by carrying out a portion of the performance calculation and the optimization of a drum-oriented  message switching system.	1968	Stimler, Brons	real-time system analysis, real-time system design, real-time system performance criteria, real-time  system cost performance ratio
1723	Computer Construction of Project Networks	Project networks are used in PERT and CPM.  An algorithm is given for constructing project  networks directly from the project precedence relations.  The algorithm creates "dummy" activities and  topologically orders the arcs and nodes.  The number of nodes created is minimal for the given precedence  relations.  It has been experimentally programmed in FORTRAN II for the IBM 7094.	1968	Fisher, Liebman, Nemhauser	project networks, PERT, CPM, topological ordering, network construction by computer
1724	A Generalized Partial Pass Block Sort	The design of a partial pass block sort with arbitrary range of key and number of work files  is described. The design is a generalization of the Partial Pass Column Sort by Ashenhurst and the Amphisbaenic  Sort by Nagler. The power of the sort is tabulated for various sizes of input file and number of work  files. consideration is given to the problem of combining a block sort with internal sorts, and to the  best use of direct access storage devices.	1968	Bayes	block sort, partial pass sort, direct access devices, column sort, chaining, reverse chaining, sort, amphisbaenic
1725	A Simple Proof of Lewin's Ordered-Retrieval Theorem for Associative Memories	An efficient method of ordered retrieval of binary words from an associative memory, as described by Lewin, is based on the use of special readout circuits which indicate the digit values present in  the individual digit columns of the memory.  Thus the circuits indicate whether the individual digit  columns contain digits of both values, or of only one value, or contain no digits at all (i.e. that the  memory is empty).  The use of these circuits, which in this paper are termed column value indicators,  reduces considerably the number of memory accesses necessary to retrieve in order a number of distinct  words from the memory.  Lewin proves that, for the readout by the described method of m distinct binary  words, 2m - 1 memory accesses are necessary.  (Thus he proves that the number of necessary memory accesses  of his method, unlike those of other methods, is independent of the word length.)  In this paper a very  simple proof of this theorem derived from some elementary aspects of the structure of sets of binary  numbers is presented.	1968	Wolinsky	associative memories, content-addressed memories, ordered lists, ordered information retrieval, ordered retrieval theorem, column digit values, digit value variety, column sensing arrangement, digit  value readout, digit variety readout, memory access, memory access frequency, ordered retrieval efficiency, access frequency proof, retrieval theorem proof
1726	Preliminary Investigation of Techniques for Automated Reading of Unformatted Text	Methods for converting unstructured printed material into computer code are experimentally  investigated.  An operator-controlled mode, depending on human demarcation of the various regions of  the page for guiding the scanner, is implemented by means of a joystick and a CRT display.  This mode,  for which some performance figures are obtained, is thought to be suitable for processing very complicated  material, such as technical journals.  For simpler material, for instance the "claims" sections of patents,  and in applications where the utmost accuracy is not necessary, an unsupervised mode is advocated.  Here,  the textual portions of the page are located during a rapid prescan by a rudimentary form of frequency  analysis.  These areas are then rescanned at a higher resolution suitable for character recognition.   Error rates of the order of 0.1 percent are obtained in a simple problem involving photographs of telephone  company meter boards.  Other matters related to the design of a general purpose page reader, such as  the segmentation of printed text, the possibility of time-sharing the scanner, interactive man-machine  operation, and the facsimile reproduction of illustrations, are discussed.	1968	Nagy	pattern recognition, character recognition, text reading, information retrieval, unformatted text  operator-controlled reader, online reader, text-image discrimination, reading machine
1727	One Way of Estimating Frequencies of Jumps in a Program	For the segmentation of a program it is useful to have a reasonable estimation of the values  of S(ij), where S(ij) is the mean value of the number of jumps from the i-th instruction on to the j-th  instruction in the run time.  In the cases where the S(ij) are estimated directly, the structure of the  whole program must be generally taken into account; therefore it is very difficult for the programmer  and/or the translator to obtain a good estimation of the S(ij).  It is easier to estimate not S(ij) but  the quantities P(ij)=S(ij)*C(i)/SUM[S(ij), j=1,N], where C(i) is an arbitrary positive constant for each  i.  Although the P(ij) are, for each i, proportional to S(ij), the estimation of P(ij) is easier, because  we must estimate only the "probabilities" of events where instruction i is executed after instruction  I(i).  This estimation can often be done without considering the structure of the whole program.  In  the first part of the paper, using the theory of the Markov chains, an algorithm for the computation  of the S(ij) from the P(ij) is found, and some ways of obtaining estimates of the P(ij) are given.  In  the second part a variant of this algorithm is derived, avoiding the necessity of computation involving  large matrices.	1968	Kral	object program reduction, supervisor calls decreasing, jump frequencies estimation, control transfers  estimation, optimal program segmentation, Markov chain program correspondence, program graph, one-entry  subgraph, locally estimated jump frequencies, supervisor overhead decreasing, program segmentation algorithm, jump frequencies, program segmentation problem
1728	Further Experimental Data on the Behavior of Programs in a Paging Environment	Results are summarized from an empirical study directed at the measurement of program operating  behavior in those multiprogramming systems in which programs are organized into fixed length pages.   The data collected from the interpretive execution of a number of paged programs are used to describe  the frequency of page faults, i.e. the frequency of those instants at which an executing program requires  a page of data or instructions not in main (core) memory.  These data are used also for the evaluation  of page replacement algorithms and for assessing the effects on performance of changes in the amount  of storage allocated to executing programs.	1968	Coffman, Varian	paging systems, paging, dynamic program behavior, program behavior, virtual memory systems, single-level  storage, one-level storage, operating system simulation, operating systems, supervisor simulation, machine  language program interpretation
1729	Minit Algorithm for Linear Programming (Algorithm 333 [H])		1968	Salazar, Sen	linear programming, dual simplex method, primal problem, dual problem
1730	Jacobi Polynomials (Algorithm 332 [S22])		1968	Witte	Jacobi polynomials, orthogonal polynomials, three-term recurrences, special functions
1731	Gaussian Quadrature Formulas (Algorithm 331 [D1])		1968	Gautschi	quadrature, Gaussian quadrature, numerical integration, weight function, orthogonal polynomials
1732	Factorial Analysis of Variance (Algorithm 330 [G1])		1968	Oliver	factorial variance analysis, variance, statistical analysis
1733	Distribution of Indistinguishable Objects into Distinguishable slots (Algorithm [G6])		1968	Fenichel	object distributions, combinations, distribution numbers
1734	Chebyshev Solution to an Overdetermined Linear System (Algorithm 328 [F4])		1968	Bartels, Goulub	Chebyshev solutions, overdetermined linear systems, linear equations, exchange algorithm
1735	A Futures Market in Computer time	An auction method is described for allocating computer time that allows the price of computer  time to fluctuate with the demand and the relative priority of users to be controlled so that more important  projects get better access.  This auction is free of the periodic fluctuation in computer use often associated  with monthly time allocation schemes.	1968	Sutherland	computer scheduling, auction, time allocation, operating efficiency
1736	Heading Format for Data Transmission (A USAAI Tutorial -- Standards)		1968		data transmission heading format, heading format, message format, data transmission, message headings
1737	A Global Parser for Context-Free Phrase Structure Grammars		1968	Unger	
1738	Writing an Outline Debugging Program for the Experienced User	Presently available online debugging routines are often unsatisfactory for the experienced  user because they require unnecessarily rigid and complicated typing formats, make it difficult for the  user to correct typing errors, and consume excessive memory with intricate features.  In a debugging  program it is of prime importance that the program be simple, flexible, and highly efficient to use.   Communication between the user and the debugging program can be improved by using certain techniques  applicable to most online debugging programs.  These techniques are presented and are illustrated by  their use in OPAK (octal package), a debugging program coded for the PDP-5/8 and the SDS-930.  The compromise  between economy of utility program core storage and incorporation of elegant debugging features is discussed.	1968	Brady	debugging, utility program, programming languages
1739	Regular Expression Search Algorithm	A method for locating specific character strings embedded in character text is described and  an implementation of this method in the form of a compiler is discussed.  The compiler accepts a regular  expression as source language and produces an IBM 7094 program as object language.  The object program  then accepts the text to be searched as input and produces a signal every time an embedded string in  the text matches the given regular expression.  Examples, problems, and solution are also presented.	1968	Thompson	search, match, regular expression
1740	An Inexpensive Braille Terminal Device	The active use of time-shared facilities for blind programmers requires a braille terminal  system.  Details are given for the construction of a brailler from a model 33 teletype by modifying the  print head and increasing the resiliency of the platen.  A description of the programming needed to drive  the brailler is presented.	1968	Anderson, Rogers	blind communication, blind programming aid, braille, braille computer communication, braille output, braille teletype, braille terminal, braille type head, embosser, tactile computer communication, tactile  teletype, tactile terminal
1741	BRAD: The Brookhaven Raster Display	A multiconsole computer display system has been designed that provides very rich displays at  low unit cost.  Each BRAD (Brookhaven Raster Display) console can plot tens of thousands of points, or  up to 4000 characters at 30 frames per second.  After an initial display system investment of $50,000  each display, with teletype, costs less than $3,000.  The technique employed is that of programmatically  generating a binary image of the desired display in a computer.  The image is written on a rotating drum  memory.  Independent read heads continuously display the picture, which is generated by swept horizontal  lines.  A standard TV monitor serves as the display device.  The technique has two drawbacks.  A computer  must compute any image to be displayed.  Also, the "pointing" interaction is more difficult.  This is  because the pointing function gives only the coordinates of the point on the screen.  The inverse of  the map generation process is required to calculate the coordinates of the point on the screen.  The  inverse of the map generation process is required to calculate the coordinates at the selected point  in the input space.	1968	Ophir, Rankowitz, Shepherd, Spinrad	computer display, computer graphics, computer raster display, TV display console, digital TV display, swept raster computer display, swept raster TV computer display, TV graphics terminal, multiconsole computer  graphics, inexpensive graphic terminal
1742	On the Design of Display Processors	The flexibility and power needed in the data channel for a computer display are considered.   To work efficiently, such a channel must have a sufficient number of instructions that it is best understood  as a small processor rather than a powerful channel.  As it was found that successive improvements to  the display processor design lie on a circular path, by making improvements one can return to the original  simple design plus one new general purpose computer for each trip around.  The degree of physical separation  between display and parent computer is a key factor in display processor design.	1968	Myer	display processor design, display system, computer graphics, graphic terminal, displays, graphics, display generator, display channel, display programming, graphical interaction, remote displays
1743	Reliable Full-Duplex file Transmission over Half-Duplex Telephone Lines	A field-proven scheme for achieving reliable duplex transmission over a half-duplex communication  line is presented, and to demonstrate the difficulty of the problem, another similar scheme, which is  only slightly unreliable, is also presented.  A flowchart for the reliable scheme and some interesting  examples are given.	1968	Lynch	telephone communication, half duplex, transmission, error correction, full duplex, telephone errors
1744	Stable Numerical Methods for Obtaining the Chebyshev Solution to an Overdetermined System of  Equations	An implementation of Stiefel's exchange algorithm for determining a Chebyshev solution to an  overdetermined system of linear equations is presented, that uses Gaussian LU decomposition with row  interchanges.  The implementation is computationally more stable than those usually given in the literature.   A generalization of Stiefel's algorithm is developed which permits the occasional exchange of two equations  simultaneously.	1968	Bartels, Golub	Chebyshev solutions, overdetermined linear systems, linear equations, exchange algorithm
1745	A Position Paper on Computing and Communications	The effective operation of free enterprise in creating the envisioned information service industry  is dependent upon three accomplishments: (1) the restructuring of our information processing industry  so that a clear division of costs is made among computing, communications, and the development of information  services; (2) the wide use of multiaccess system concepts so that information services may share in the  use of computer installations and so that the cost of their construction is reasonable; and (3) the development  of public, message-switched communications services so that adequate provisions are made for information  security.	1968	Dennis	information networks, information systems, computing and free enterprise, computing economics, computer installation management, government regulation, communications services, distributed data base, program leasing
1746	Protection in an Information Processing Utility	One of the critical problems in the design of an information processing utility that permits  flexible sharing of user information is privacy.  One solution for this problem is discussed.	1968	Graham	protection, privacy, information processing utility, time-sharing, multi-user, multiprogramming, multiprocessing, security, shared information, controlled access, reliable operation, segmentation
1747	Three Criteria for Designing Computing Systems to Facilitate Debugging	The designer of a computing system should adopt explicit criteria for accepting or rejecting  proposed system features.  Three possible criteria of this kind are input recordability, input specifiability,  and asynchronous reproducibility of output.  These criteria imply that a user can, if he desires, either  know or control all the influences affecting the content and extent of his computer's output.  To define  the scope of the criteria, the notion of an abstract machine of a programming language and the notion  of a virtual computer are explained.  Examples of applications of the criteria concern the reading of  a time-of-day clock,  the synchronization of parallel processes, protection in multiprogrammed systems,  and the assignment of capability indexes.	1968	Van Horn	computer design, computer design criteria, computer systems, computer systems design, input equipment, input equipment design, operating systems, operating systems design, multiprogramming, multiprogrammed  systems, multiprogrammed system design, virtual computers, programming languages, programming language  design, program semantics, programming language semantics, determinism, reproducibility, repeatability, deterministic computers, protection, memory protection, information security, information privacy, computing  reliability, debugging, program debugging, program testing, parallel processing, parallel programming, multiprocessing
1748	A Scheduling Philosophy for Multiprocessing Systems	A collection of basic ideas is presented, which have been evolved by various workers over the  past four years to provide a suitable framework for the design and analysis of multiprocessing systems.   The notions of process and state vector are discussed, and the nature of basic operations on processes  is considered.  Some of the connections between processes and protection are analyzed.  A very general  approach to priority-oriented scheduling is described, and its relationship to conventional interrupt  systems is explained.  Some aspects of time-oriented scheduling are considered. The implementation of  the scheduling mechanism is analyzed in detail and the feasibility of embodying it in hardware established.   Finally, several methods for interlocking the execution of independent processes are presented and compared.	1968	Lampson	time-sharing, multiprocessing, process, scheduling, interlocks, protection, priority, interrupt  systems
1749	The Structure of the "THE"-Multiprogramming System	A multiprogramming system is described in which all activities are divided over a number of  sequential processes.  These sequential processes are placed at various hierarchical levels, in each  of which one or more independent abstractions have been implemented.  The hierarchical structure proved  to be vital for the verification of the logical soundness of the design and the correctness of its implementation.	1968	Dijkstra	operating system, multiprogramming system, system hierarchy, system structure, real-time debugging, program verification, synchronizing primitives, cooperating sequential processes, system levels, input-output  buffering, multiprogramming, processor sharing, multiprocessing
1750	Considerations in the Design of a Multiple Computer System with Extended Core Storage	The use of large quantities of addressable (but not executable) fast random access memory to  heighten the multiprogramming performance of a multicomputer system is discussed.  The general design  of the hardware arrangement and the software components and functions of such a system are based on a  planned configuration of dual CDC 6600's that share one million words of extended core storage.  In the  generalization of such a design, special emphasis is placed on estimating expected gains when compared  with the traditional configuration of separate and independent computers without extended core storage.   An observation is made on the use of conventional, slower speed, random access storage devices in place  of the faster memory.	1968	Fuchel, Heller	multiple computer systems, extended core storage, multiprogrammed operating systems, multiprocessor  operating systems, control data corporation 6600, operating system with ECS
1751	The Working Set Model for Program Behavior	Probably the most basic reason behind the absence of a general treatment of resource allocation  in modern computer systems is an adequate model for program behavior.  In this paper a new model, the  "working set model," is developed. The working set of pages associated with a process, defined to be  the collection of its most recently used pages, provides knowledge vital to the dynamic management of  paged memories.  "Process" and "working set" are shown to be manifestations of the same ongoing computational  activity; then "processor demand" and "memory demand" are defined; and resource allocation is formulated  as the problem of balancing demands against available equipment.	1968	Denning	general operating system concepts, multiprocessing, multiprogramming, operating systems, program  behavior, program models, resource allocation, scheduling, storage allocation
1752	Resource Management for a Medium Scale Time-Sharing Operating system	Task scheduling and resource balancing for a medium size virtual memory paging machine are  discussed in relation to a combined batch processing and time-sharing environment.  A synopsis is given  of the task scheduling and paging algorithms that were implemented, and the results of comparative simulation  are given by tracing the development of the algorithms through six predecessor versions.  Throughout  the discussion particular emphasis is placed on balancing the system performance relative to the characteristics  of all the system resources.  Simulation results relative to alternate hardware characteristics and the  effects of program mix and loading variations are also presented.	1968	Oppenheimer, Weizer	time-sharing, operating systems, resource management, task scheduling, paging, system simulation, memory management, virtual memories
1753	Virtual Memory, Processes, and Sharing in MULTICS	Some basic concepts involved in the design of the MULTICS operating system are introduced.   MULTICS concepts of processes, address space, and virtual memory are defined and the use of paging and  segmentation is explained.  The means by which users may share procedures and data is discussed and the  mechanism by which symbolic references are dynamically transformed into virtual machine addresses is  described in detail.	1968	Daley, Dennis	virtual memory, information sharing, shared procedures, data sharing, dynamic linking, segmentation, paging, multiprogramming, storage management, storage hierarchies, file maintenance
1754	Dynamic Storage Allocation Systems	In many recent computer system designs, hardware facilities have been provided for easing the  problems of storage allocation.  A method of characterizing dynamic storage allocation systems-according  to the functional capabilities provided and the underlying techniques used-is presented.  The basic purpose  of the paper is to provide a useful perspective from which the utility of various hardware facilities  may be assessed.  A brief survey of storage allocation facilities in several representative computer  systems is included as an appendix.	1968	Randell, Kuehner	segmentation, paging, multiprogramming, storage allocation, storage management, virtual memories, storage fragmentation, storage hierarchies, addressing mechanisms
1755	Proceedings of the ACM Symposium on Operating system Principles		1968		ACM Special Interest Committee
1756	Hollerith Punched Card Code* (Proposed USA Standard)		1968		USA Standard, card code, punched card, punched card code, hole-patterns, hole-patterns assignment, punched card systems
1757	Data Code for Calendar Date for Machine-to-Machine Data Interchange* (Proposed USA Standard)		1968		USA Standard, data code, calendar date, machine-to-machine data interchange, recording calendar  date, data group identifier
1758	Symmetric Polynomials, (Algorithm 305 [C1])		1968	McKay	symmetric polynomials, symmetric sum, unitary symmetric functions, Schur functions
1759	Transportation Problem (Algorithm 293 [H])		1968	Bayer	transportation problem, linear programming
1760	Normal Curve Integral (Algorithm 304 [S15])		1968	Bergson	normal curve integral, probability, special functions
1761	Chi-Squared Integral (Algorithm 299 [S15])		1968	O'Brien, Wood	chi-squared integral, probability, special functions
1762	Dilogarithm (Algorithm 327 [S22])		1968	Kolbig	dilogarithm function, special functions
1763	Roots of Low-Order Polynomial Equations (Algorithm 326 [C2])		1968	Nonweiler	root finders, polynomial equation roots, quadratic equation roots, cubic equation roots, biquadratic  equation roots, polynomial zeros
1764	Panel Discussion on Computer Appreciation	Session 19 of the ACM 20 th Anniversary Conference on August 31, 1967, was entitled Education,  Design Experiments, and Computer Appreciation.  Its second half consisted of a panel discussion on computer  appreciation, organized and chaired by Elliot I. Organick.  The four panelists were Charles H. Davidson,  Bernard A. Galler, Richard, W. Hamming, and Alan J. Perlis.  After making prepared statements, the panelists  were joined in discussion by Andries van Dam and Arthur Bohn, who had presented papers in the first  half.  This is a transcript of the panel discussion, condensed by Dr. Organick and edited by him and  the panelists.  Some remarks referred to papers by van Dam and Kahn or to the discussion during the first  half of the session.  Pertinent papers are included in the references.	1968	Organick	computer appreciation, students' liberal arts courses, survey courses, beginning programming, course  content, computer courses dropout rates, college versus precollege, teaching and social responsibility
1765	Expenditures, Sources of Funds, and Utilization of Digital Computers for Research and Instruction  in Higher Education: 1964-65 with Projections for 1968-69	The Southern Regional Education Board published a complete report on a survey it conducted  to determine the funding and characterize the utilization of computers used for research and instruction  in institutions of higher education in the United States. The sampling survey is described and the estimates  for this total population are presented.	1968	Hamblen	computing centers, research, instruction, utilization, expenditures, support, sources of funds, higher education, post secondary education, colleges, universities
1766	Quasilinearization and the Estimation of Differential Operators from Eigenvalues	Given a linear ordinary differential operator containing several unknown constants and a number  of its eigenvalues, the values of the unknown constants are estimated.  A precise formulation is provided,  and an effective numerical procedure for solution is indicated.   The results of some computational experiments  are given.	1968	Bellman, Kagiwada, Kalaba, Vasudenvan	quasilinearization, eigenvalues, differential operators, nonlinear boundary-value problems, inverse  problems, differential equations, system identification
1767	A General Purpose Graphic Language	Interactive use of computers with graphic terminals will permit many new problems to be solved  using machines.  In order to handle a variety of applications, it is expedient to develop a general purpose  graphic language that is useful on a number of graphic devices.  A system has been designed to produce  such a language quickly and cheaply.  A model graphic language which has been developed with the system  is presented.	1968	Kulsrud	graphic language, interactive, incremental compilation, language design, metacompiler, syntax specified  language
1768	A Global Parser for Context-Free Phrase Structure Grammars	An algorithm for analyzing any context-free phrase structure grammar and for generating a program  which can then parse any sentence in the language (or indicate that the given sentence is invalid) is  described. The parser is of the "top-to-bottom" type and is recursive . A number of heuristic procedures  whose purpose is to shorten the basic algorithm by quickly ascertaining that certain substrings of the  input sentence cannot correspond to the target nonterminal symbols are included.  Both the generating  algorithm and the parser have been implemented in RCA SNOBOL and have been tested successfully on a number  of artificial grammars and on a subset of ALGOL.  A number of the routines for extracting data about  a grammar, such as minimum lengths of N-derivable strings and possible prefixes, are given and may be  of interest apart from their application in this particular context.	1968	Unger	parser, syntax-directed compiler, context-free grammars, syntactic analysis, translators
1769	The Expanding World of Computers	The onward sweep of automatic processing of information is impeded by nine principal barriers:  geography, cost, problem complexity, man-machine communication, inadequate sensors, lack of understanding,  distance, time, and size.  The main incentive for breaching these barriers is the universal need for  processing information, ever more urgent as the greater part of human work activity changes from production  to service.  Computer developments in hardware, programming, time-sharing, education, data communication,  and displays are judged by how effectively they remove these barriers, and their barrier-smashing potentialities  indicate continued rapid expansion.  Problem-oriented languages are particularly effective over the entire  front.  Online computers and time-sharing also rate high by this measure.  Education and increased understanding  are basic to all progress with the computer.  This complex but powerful tool is the most important one  available to governments and scientists to use in studying the problems being created by the population  explosion, and in analyzing possible solutions.	1968	Harder	barriers, philosophy, developments, computer-aided design, problem-oriented languages, data communication, education, computer science, forecast, survey, introduction
1770	Rules of Ethics in Information Processing	The background and motivation for the adoption by the ACM Council on November 11, 1966, of  a set of Guidelines for Professional Conduct in Information Processing are described.  A brief his tory  is given of ethical codes in other professions.  Some reasons for and against adoption of ethical rules  are considered, and several sections of the ACM Guidelines are analyzed.  The purpose is to inform about  this important aspect of our profession, as well as to stimulate thought and interest.	1968	Parker	ethics, professional conduct, code of ethics, ACM guidelines, professionalism, professional societies, unethical conduct
1771	CURRICULUM 68 -- Recommendations for Academic Programs in Computer Science -- A Report of the  ACM Curriculum Committee on Computer science	This report contains recommendations on academic programs in computer science which were developed  by the ACM Curriculum Committee on Computer Science.  A classification of the subject areas contained  in computer science is presented and twenty-two courses in these areas are described.  Prerequisites,  catalog descriptions, detailed outlines, and annotated bibliographies for these courses are included.   Specific recommendations which have evolved from the Committee's 1965 Preliminary Recommendations are  given for undergraduate programs.  Graduate programs in computer science are discussed and some recommendations  are presented for the development of master's degree programs.  Ways of developing guidelines for doctoral  programs are discussed, but no specific recommendations are made. The importance of service courses,  minors, and continuing education in computer science is emphasized.  Attention is given to the organization,  staff requirements, computer resources, and other facilities needed to implement computer science educational  programs.	1968		computer science courses, computer science curriculum, computer science education, computer science  academic programs, computer science graduate programs, computer science undergraduate programs, computer  science course bibliographies
1772	USASCSOCR Dual Case Keyboard Arrangement* (Proposed USA Standard)		1968		
1773	General Purpose Alphanumeric Keyboard Arrangement for Information Interchange* (Proposed USA  Standard)		1968		
1774	Program Overlay Techniques	The general features of program overlay systems are described.  Three main types -- automatic,  semiautomatic and nonautomatic -- are classified, and the programming techniques are explained as a function  of machine hardware and other system features.  The implementation of semiautomatic overlay facility  in a multiprogrammed system on the CDC 6600 is described in detail, with special reference to real time  applications.	1968	Pankhurst	loaders, multiprogramming, overlay techniques, storage allocation and segmentation
1775	Adjustment of the Inverse of a Symmetric Matrix when Two Symmetric Elements are Changed (Algorithm  325 [F1])		1968	Zielke	symmetric matrix, matrix inverse, matrix perturbation, matrix modification
1776	Maxflow (Algorithm 324 [H])		1968	Bayer	network, liner programming, maximum flow
1777	Generation of Permutations in Lexicographic Order (Algorithm 323 [G6])		1968	Ord-Smith	permutations, lexicographic order, lexicographic generation, permutation generation
1778	F-Distribution (Algorithm 322 [S14])		1968	Dorrer	Fisher's  F-distribution, Student's t-distribution
1779	t-Test Probabilities (Algorithm [S14])		1968	Morris	T-test, Student's t-statistic, distribution function
1780	Harmonic Analysis for Symmetrically Distributed Data (Algorithm 320 [C6])		1968	Hunter	harmonic analysis, cosine series, sine series, function approximation, curve fitting, trigonometric  series
1781	Translator Writing systems	A critical review of recent efforts to automate the writing of translators of programming languages  is presented.  The formal study of syntax and its application to translator writing are discussed in  Section II.  Various approaches to automating the post syntactic (semantic) aspects of translator writing  are discussed in Section III, and several related topics in Section IV.	1968	Feldman, Gries	compiler, compiler-compiler, translator, translator writing systems, metacompiler, syntax, semantics, syntax-directed, meta-assembler, macroprocessor, parser, syntactic analysis, generator
1782	A Numerical Integration Formula Useful in Fourier Analysis	A numerical integration formula is presented which uses unequal sampling intervals.  The intervals  are equally spaced on a log scale.  Such a formulation is useful in Fourier analysis to improve accuracy  and ease of usage.  A complete set of formulas for numerical Fourier analysis is given.	1968	Meisel	numerical integration, Fourier analysis, integration
1783	In-and-Out Conversions	Byan in-and-out conversion we mean that a floating-point number in one base is converted into  a floating-point number in another base and then converted back to a floating-point number in the original  base.  For all combinations of rounding and truncation conversions the question is considered of how  many significant digits are needed in the intermediate base to allow such in-and-out conversions to return  the original number (when possible), or at least significant digit.	1968	Matula	floating-point numbers, significance, base conversion, rounding, truncation
1784	Practical Error Coefficients for Estimating Quadrature Errors for Analytic Functions	All published error coefficients for estimating quadrature errors for analytic functions were  computed on the assumption that the quadrature rule was exact for polynomials up to a given degree.   Since these rules use rounded values for the abscissas and weights and since the true values of the integrals  of some of the polynomials in question have an infinite binary expression, the quadrature rule is not  exact.  Hence these errors must be taken into consideration in computing practical error coefficients.	1968	Rabinowitz	numerical integration, quadrature, truncation error, theoretical error coefficients, practical  error coefficients, integration analytical functions, roundoff error
1785	Scatter Storage Techniques	Scatter storage techniques as a method for implementing the symbol tables of assemblers and  compilers are reviewed and a number of ways of using them more effectively are presented.  Many of the  most useful variants of the techniques are documented.	1968	Morris	scatter storage, hash addressing, searching, file searching, file addressing, storage layout
1786	An Improved Hash Code for Scatter Storage	Introduced is a hash coding method based on fixed-point division rather than multiplication  or logical operations.  This new method allows the hash table to have almost any length.  Also a new  method of handling collisions is discussed.  Known as quadratic search, this method is faster than random  search and free from the "clusters" that build up with a linear search.	1968	Maurer	hash code, hash table, scatter storage, searching
1787	Use of Transition Matrices in Compiling	An algorithms is described which constructs from a suitable BNF grammar an efficient left-right  recognizer for sentences of the corresponding language.  The type of recognizer, used in a number of  compilers, operates with a pushdown stack and with a transition matrix.  Two examples illustrate how  such recognizers may be used effectively for other purposes besides the usual syntax checking.	1968	Gries	transition matrices, compilation, translation, grammar, context-free language, formal language, parsing
1788	Toward a General Processor for Programming Languages	Many efforts have been made to develop a better way of implementing a higher level programming  language than by the construction of a whole new compiler, but so far none has proved generally satisfactory.   In this paper, it is contended that a programming language is best described functionally as a body  of macro instructions, and that the macro call constitutes a canonical form in terms of which a programming  notation may be described.  A supporting discussion of the logical and his torical role of the macro instruction  is presented.  Also discussed are the conflict between machine independence and object program efficiency,  and the question of where the greatest difficulties lie in compiler construction.	1968	Halpern	programming language translator, programming language processor, general translator, general processor, macro instruction processor; meta processor, meta language translator, meta language processor, compiler-compiler, writing system, translator writing system
1789	Logarithm of Gamma Function (Algorithm 291 [S14])		1968	Hoare	
1790	Muller's Method for Finding roots of an Arbitrary Function  (Algorithm 196 [C5])		1968	Whitley	equation roots, function zeros
1791	Triangular Factors of Modified Matrices (Algorithm 319 [F1])		1968	Green	matrix decomposition, matrix factors, matrix modifier, matrix perturbation
1792	Exploratory Experimental Studies Comparing Online and Off line Programming Performance	Two exploratory experiments were conducted at System Development Corporation to compare debugging  performance of programmers working under conditions of on-line and off line access to a computer.  These  are the first known studies that measure programmers' performance under controlled conditions for standard  tasks.  Statistically significant results of both experiments indicated faster debugging under online  conditions, but perhaps the most important practical finding involves the striking individual differences  in programmer performance.  Methodological problems encountered in designing and conducting these experiments  are described; limitations of the findings are pointed out; hypotheses are presented to account for results;  and suggestions are made for further research.	1968	Sackman, Erikson, Grant	online vs. off line performance, programmer/computer communication, programming experimental-empirical  studies, programming cost effectiveness, programming performance, debugging effectiveness, time sharing  vs. batch processing, factor analysis application, programmer trainee performance, basic programming  knowledge test, experienced programmer study, analysis of variance, programmer individual differences
1793	Presentation of Alphameric Characters for Information Processing* (Proposed American National  Standard)		1969		alphameric, handwritten input, encoding transcription, numerals, upper case, hand printed
1794	A Fast Random Number Generator for IBM 360		1969	Seraphin	pseudorandom number, modulus, period, float, normalization, characteristic, chi-square test
1795	Optimal Code for Serial and Parallel Computation		1969	Fateman	code optimization, sequencing of operations, detection of common subexpressions
1796	Index by Subject to Algorithms, 1969		1969		This 1969 index is the first supplement to the Index by Subject to Algorithms, 1960 1968 (Comm.  ACM 11, 12 (Dec. 1968), 827 830).
1797	Solution of Linear programs in 0-1 (Algorithm 341 [H])		1969	Proll	linear programming, zero-one variables, partial enumeration
1798	Coulomb Wave Functions (Algorithm 300 [S22])		1969	Kolbig	Coulomb wave functions, wave functions, special functions, function evaluation
1799	Elementary Functions by Continued Fractions (Algorithm 229 [B1])		1969	Bray	continued factions, Pade table
1800	PSIF (Algorithm 147 [S14])		1969	Parsons	gamma function, logarithmic derivative, factorial function, psi function
1801	Analysis of Variance for Balanced Experiments (Algorithm 367 [G2])		1969	Claringbold	analysis of variance, analysis of covariance, regression analysis, experimental design, balanced  experiment, missing data, interblock estimate, intra block estimate
1802	Regression Using Certain Direct Product Matrices (Algorithm 366 [G2])		1969	Claringbold	analysis of variance, analysis of covariance, regression analysis, experimental design, matrix  direct product, protection operator, orthogonal matrix
1803	Complex Root Finding (Algorithm 365 [C5])		1969	Bach	downhill method, complex relaxation method, complex iteration, complex equation, transcendental  complex equation, algebraic complex equation
1804	Coloring Polygonal Regions (Algorithm 364 [Z])		1969	Herriot	coloring polygonal regions, coloring planar surfaces, drawing pictures, shading enclosed regions
1805	Productivity of Multiprogrammed Computers-Progress in Developing an Analytic Prediction Method	Multiprogramming as it is discussed here is a mode of computer operation in which two or more programs are concurrently in processor memory and proceeding, each using the same central processor unit  (CPU) and input-output (I/O) channels.  These programs are actually proceeding intermittently and singly,  according to eligibility (readiness to proceed) and priority.  It is useful to be able to represent them  as proceeding continuously and simultaneously, each at an effective rate, which may be a fraction of  that which it would enjoy in the absence of the other programs.  The effective progress rate of each  program is sensitive to many detailed characteristics of itself and its co-residents and simulation has  been the best available method of predicting it.  This paper presents the results of progress in developing  an alternative to simulation, a simulation-tested iterative computation of these rates under certain  situations.  The algorithm is sensitive to most of the factors that control the phenomenon, including  nonquantitative or topological features of the programs' structures.	1969	Lasser	productivity, prediction, multiprogramming, simulation, equipment  evaluation, hardware, evaluation, monitor, operating system, system software, supervisors, performance, time sharing, time slicing
1806	On the Downhill Method	The downhill method is a numerical method for solving complex equations f(z) = 0 on which the  only restriction is that the function w = f(z) must be analytical.  An introduction to this method is  given and a critical review of relating literature is presented.  Although in theory the method always  converges, it is shown that a fundamental dilemma exists which may cause a breakdown in practical applications.   To avoid this difficulty and to improve the rate of convergence toward a root, some modifications of  the original method are proposed and a program (FORTRAN) based on the modified method is given in Algorithm  365.  Some numerical examples are included.	1969	Bach	downhill method, complex relaxation method, complex iteration, complex equation, transcendental  complex equation, algebraic complex equation
1807	Optimization of Expressions in Fortran	A method of optimizing the computation of arithmetic and indexing expressions of a Fortran  program is presented.  The method is based on a linear analysis of the definition points of the variables  and the branching and DO loop structure of the program.  The objectives of the processing are (1) to  eliminate redundant calculations when references are made to common subexpression values, (2) to remove  invariant calculations from DO loops, (3) to efficiently compute subscripts containing DO iteration variables,  and (4) to provide efficient index register usage.  The method presented requires at least a three-pass  compiler, the second of which is scanned backward.  It has been used in the development of several FORTRAN  compilers that have proved to produce excellent object code without significantly reducing the compilation  speed.	1969	Busam, England	FORTRAN, optimization, expressions, compilers, compilation, subscripts, register allocation, DO  loops, common subexpressions, invariant calculations
1808	Advanced Cryptographic Techniques for Computers	Cryptographic techniques which can be used to maintain the confidentiality of information processed  by computers are dealt with.  Special emphasis is paid to the unique characteristics of computer files  that make many cryptographic methods of little use.  Relative security, costs, and preferred methods  are included in this paper.	1969	Van Tassel	cryptographic, cryptanalysis, ciphers secrecy systems, security systems, confidential information  processing
1809	Numerical Analysis in a Ph. Computer Science Program	Numerical Analysis is the study of methods and procedures used to obtain "approximate solutions"  to mathematical problems.  Much of the emphasis is on scientific calculation.  The difficulties of education  in such a broad area center around the question of background and emphasis.  The Numerical Analysis program  in the Computer Science Department should emphasize an awareness of the problems of computer implementation  and experimental procedures.  Nevertheless, there is a need for a solid background in applied mathematics.	1969	Parter	Ph. program, numerical analysis, course separation, education
1810	Is Automatic "Folding" of Programs Efficient Enough To Displace Manual?	The operation of "folding" a program into the available memory is discussed.  Measurements  by Brown et al. and by Nelson on an automatic folding mechanism of simple design, a demand paging unit  built at the IBM Research Center by Belady, Nelson, O'Neil, and others, permitting its quality to be  compared with that of manual folding, are discussed, and it is shown that given some care in use the  unit performs satisfactorily under the conditions tested, even though it is operating across a memory-to-storage  interface with a very large speed difference.  The disadvantages of prefolding, which is required when  the folding is manual, are examined, and a number of the important troubles which beset computing today  are shown to arise from, or be aggravated by, this source.  It is concluded that a folding mechanism  will probably become a normal part of most computing systems.	1969	Sayre	paging, automatic paging, demand paging, folding, automatic folding, storage hierarchies, memory  hierarchies, replacement algorithms, performance, measurement
1811	A Case Study in Programming for Parallel-Processors	An affirmative partial answer is provided to the question of whether it is possible to program  parallel-processor computing systems to efficiently decrease execution time for useful problems.  Parallel-processor  systems are multiprocessor systems in which several of the processors can simultaneously execute separate  tasks of a single job, thus cooperating to decrease the solution time of a computational problem. The  processors have independent instruction counters, meaning that each processor executes its own task program  relatively independently of the other processors.  Communication between cooperating processors is by  means of data in storage shared by all processors.  A program for the determination of the distribution  of current in an electrical network was written for a parallel-processor computing system, and execution  of this program was simulated.  The data gathered from simulation runs demonstrate the efficient solution  of this problem, typical of a large class of important problems.  It is shown that, with proper programming,  solution time when N processors are applied approaches 1/N times the solution time for a single processor,  while improper programming can actually lead to an increase of solution time with the number of processors.   Stability of the method of solution was also investigated.	1969	Rosenfeld	parallel-processor, parallelism, parallel programming, multiprocessor, multiprogramming, tasking, storage interference, electrical network, simulation, relaxation, Jacobi, Gauss-Seidel, convergence
1812	More on Fortran Random Number Generators		1969	Grosenbaugh	random number generation, Monte Carlo, simulation
1813	Generation of Permutations in Pseudo-Lexicographic Order (Algorithm 308 [G6])		1969	Ord-Smith	permutations, lexicographic order, lexicographic generation, permutation generation
1814	Direct Search (Algorithm 178 [E4])		1969	Smith	function minimization, search, direct search
1815	Direct Search (Algorithm 178 [E4])		1969	Tomlin, Smith	function minimization, search direct search
1816	Generalized Least Squares Fit By Orthogonal Polynomials (Algorithm 296 [E2])		1969	Watson	least squares, curve fitting, orthogonal polynomials, three-term recurrence, polynomial regression, approximation, Forsythe's method
1817	Computation of Fourier Coefficients (Algorithm 255 [C6])		1969	Hall, Ray	numerical integration, Fourier coefficients, Filon's method
1818	Associated Legendre Functions of the First Kind for Real or Imaginary Arguments (Algorithm 47  [S16])		1969	Cobb	Legendre function, associated Legendre function, real or imaginary arguments
1819	Complex Error Function (Algorithm 363 [S15])		1969	Gautschi	error function for complex argument, Voigt function, Laplace continued fraction, Gauss-Hermite  quadrature, recursive computation
1820	Generation of Random Permutations (Algorithm 362 [G6])		1969	Robson	permutation, random permutation, transposition
1821	Permanent Function of a Square Matrix I and II (Algorithm 361 [G6])		1969	Shriver, Eberlein, Dixon	matrix, permanent, determinant
1822	Shortest-Path Forest with Topological Ordering (Algorithm [H])		1969	Dial	shortest path, tree, network, directed graph
1823	Factorial Analysis of Variance (Algorithm [G1])		1969	Howell	factorial variance analysis, variance, statistical analysis
1824	APAREL-A Parse-Request Language	APAREL is described: this language is an extension to an algorithmic language (PL/I) that provides  the pattern-matching capabilities normally found only in special purpose languages such as SNOBOL4 and  TMG.  This capability is provided through parse-requests stated in a BNF-like format.  These parse-requests  form their own programming language with special sequencing rules.  Upon successfully completing a parse-request,  an associated piece of PL/I code is executed.  This code has available for use, as normal PL/I strings  the various pieces (at all levels) of the parse.  It also has available as normal PL/I variables, the information concerning which of the various alternatives were successful.  Convenient facilities for  multiple input-output streams, the initiation of sequences of parse-requests as a subroutine, and parse-time  semantic checks are also included.  APAREL has proven convenient  in building a powerful SYNTAX and FUNCTION  macro system, an algebraic language preprocessor debugging system, an on-line command parser, a translator  for Dataless Programming, and as a general string manipulator.	1969	Balzer, Farber	text processing, string processing, symbol manipulation, PL/I, BNF, syntax, parser, translator, pattern matching
1825	A Practical Method for Constructing LR(k) Processors	A practical method for constructing LR(k) processors is developed.  These processors are capable  of recognizing and parsing an input during a single no-backup scan in a number of steps equal to the  length of the input plus the number of steps in its derivation.  The technique presented here is based  on the original method described by Knuth, but decreases both the effort required to construct the processor  and the size of the processor produced.  This procedure involves partitioning the given grammar into  a number of smaller parts.  If an LR(k) processor can be constructed for each part (using Knuth's algorithm)  and if certain conditions relating these individual processors are satisfied, then an LR(k) processor  for the entire grammar can be constructed for them.  Using this procedure, an LR(1) parser for ALGOL  has been obtained.	1969	Korenjak	LR(k) grammar, syntactic analysis, parser, deterministic language, syntax-directed compiler, language processor, context-free language ALGOL
1826	A LISP Garbage-Collector for Virtual-Memory Computer Systems	In this paper a garbage-collection algorithm for list-processing systems which operate within  very large virtual memories is described.  The object of the algorithm is more the compaction of active  storage than the discovery of free storage.  Because free storage is never really exhausted, the decision  to garbage collect is not easily made; therefore, various criteria of this decision are discussed.	1969	Fenichel, Yochelson	garbage-collector, virtual memory, list-processing, storage-allocation
1827	Performance Monitoring in a Time-Sharing System	A software measurement facility which is part of a general purpose time-sharing system is described.   The Date Collection Facility (DCF) has been implemented in the Michigan Terminal System (MTS) for the  System/360 model 67.  It exists for the purpose of monitoring operating system and user program behavior  and performance.  The overall structure of MTS is outlined in order to explain the implementation of  the DCF.  Events in the system are identified and recorded from within the supervisor, and dumped to  magnetic tape by an auxiliary program for off-line processing.  Events in user programs which are unrelated  to system actions are recorded with a supervisor call.  The time of occurrence of each event is accurately  recorded, and data items are further identified by job and type.  The overhead associated with data collection  and its interference with normal jobs is carefully analyzed, and both are shown to be minimal.  Several  examples are given of information obtained with the facility and of applications in which it has been  useful.  Some general guidelines are offered for the construction of future monitoring programs.	1969	Pinkerton	performance monitoring, performance measurement, program behavior, performance data, multiprogramming  performance, software measurement, time-sharing performance, system evaluation, software monitor, software  instrumentation
1828	Synchronization in a Parallel-Accessed Data Base	The following problem is considered:  Given a data base which can be manipulated simultaneously  by more than one process, what are the rules for synchronization which will maximize the amount of parallel  activity allowed.  It is assumed that the data base can be represented as a graph.  An example of such  a data base is a hierarchy of directories for an on-line file system.  Methods for synchronization of  processes are examined; their validity is discussed and their performance compared.	1969	Shoshani, Bernstein	parallel accessing, parallel search, file search, data base, synchronization, locking, deadlock
1829	An Interactive Graphical Display Monitor in a Batch-Processing Environment with Remote Entry	A graphic monitor program is described.  It was developed at Carnegie-Mellon University for  the CDC G21 computer, which is a general purpose, batch-processing system with remote entry.  The existing  G21 system and the graphics hardware are described.  The graphic monitor is a resident auxiliary monitor  which provides comprehensive managerial capability over the graphical system in response to commands  from the human user.  It also will respond to commands from a user program through a similar interface,  where routine calls take the place of manual actions.  Thus the human and program can interact on a symmetrical  and equal basis through the medium of the graphic monitor.  The choice made in designing the graphic  monitor, given the constraints of the existing hardware and computer system, are discussed.  The structure  of the monitor program and the human and program interfaces are described.  There is also a transient  swapping version with a small resident part, and provision for swapped used submonitors.	1969	Bond, Rightnour	graphic monitor, man/machine interaction, graphic interface, graphic in batch environment, design  of graphical system
1830	Retrieval Times for a Packed Direct Access Inverted File		1969	Bayes	information retrieval, direct access memory, data base, inverted list
1831	A Comment on Optimal Tree Structures		1969	Stanfel	information retrieval, file searching, tree structures, double chaining
1832	Minimax Logarithmic Error		1969	Dunham	logarithmic error, transformed rational approximation, square root
1833	An Ambiguity in the Description of ALGOL 60		1969	Herriot	Ising problem, zero-one sequences
1834	An Axiomatic Basis for Computer Programming	In this paper an attempt is made to explore the logical foundations of computer programming  by use of techniques which were first applied in the study of geometry and have later been extended to  other branches of mathematics.  This involves the elucidation of sets of axioms and rules of inference  which can be used in proofs of the properties of computer programs.  Examples are given of such axioms  and rules, and a formal proof of a simple theorem is displayed.  Finally, it is argued that important  advantages, both theoretical and practical, may follow from a pursuance of these topics.	1969	Hoare	axiomatic method, theory of programming, proofs of programs, formal language definition, programming  language design, machine-independent programming, program documentation
1835	The IITRAN Programming Language	The IITRAN language, developed to be used by students, and its important important features  are described. IITRAN is a procedure-oriented language with a one-level block structure and a variety  of data types.  Several novel and powerful features are included.  A discussion of design principles  to be followed in a student language is given.	1969	Dewar, Hochsprung	languages programming languages, student programming systems, language design, high school programs, college courses
1836	A New Method for Determining Linear Precedence Functions for Precedence Grammars	The precedence relations of a precedence grammar can be precisely described by a two-dimensional  precedence matrix.  Often the information in the matrix can be represented more concisely by a pair of  vectors, called linear precedence functions.  A new algorithm is presented for obtaining the linear precedence functions when given the precedence matrix; this algorithm is shown to possess several computational  advantages.	1969	Bell	Boolean matrices, syntax, precedence grammar context-free parsing, transition matrix, precedence  functions
1837	An Algol Convolution Procedure Based on the Fast Fourier Transform (Algorithm 345 [C6])		1969	Singleton	fast Fourier transform, complex Fourier transform, multivariate Fourier transform, Fourier series, harmonic analysis, spectral analysis, orthogonal polynomials, orthogonal transformation, convolution, auto covariance, autocorrelation, cross-correlation, digital filtering, permutation
1838	Normal Curve Integral (Algorithm 304 [S15])		1969	Adams	normal curve integral, probability, special functions
1839	Singular Value Decomposition of a Complex Matrix (Algorithm 358 [F1, 4,5])		1969	Businger, Goulub	singular values, matrix decomposition, least squares solution, pseudoinverse
1840	An Efficient Prime Number Generator (Algorithm 357 [A1])		1969	Singleton	prime numbers, factoring, number theory
1841	A Prime Number Generator Using The Treesort Principle (Algorithm 356 [A1])		1969	Singleton	prime numbers, number theory, sorting
1842	An Algorithm for Generating Ising Configurations (Algorithm 355 [Z])		1969	Simoes	Ising problem, zero-one sequences
1843	The Choice of Base	A digital computer is considered, whose memory words are composed on N r-state devices plus  two sign bits (two state devices).  The choice of base B for the internal representation of floating-point  numbers on such a computer is discussed.  It is shown that in a certain sense B= r is best.	1969	Brown, Richman	floating-point, accuracy, base choice, number representations
1844	A Modular Computer Sharing System	An alternative approach to the design and organization of a general purpose interactive multiterminal  computing system is presented.  The system organization described is a conceptually simple arrangement  of a bank of interchangeable computers, each of which is a memory/processor pair, that are assigned to  process terminal jobs as they arrive.  One of the computers serves as the master or control computer  and supervises the collection and distribution of messages from and to the remote terminals.  In the  simplest form there is a disk drive for each connected terminal.  A crosspoint switching network allows  any such disk drive to be connected to any computer.  Thus, while each active terminal user "occupies"  a dedicated disk drive, he may share the computer with many other terminal users in a simple manner.   The ratio of users to computers is dependent on both the size and power of the machines used and the  computation requirements of the particular mix of users.  This system organization is inherently a simpler  and therefore more reliable approach to time-sharing computers and has the potential of a highly available  system at relatively low cost.  Economic configurations are possible for a range of systems sizes that  span at least one order of magnitude.  Finally, problem programs developed by remote terminal users can  be run on a dedicated batch system if compatible computers are used.	1969	Baskin, Horowitz, Tennison, Rittenhouse	multiple terminal systems, terminal oriented systems, multiple processor systems, high availability, conversational systems, general purpose time-sharing systems, real-time response system, modular constructed  systems, modular computer-sharing systems, graphics, file switch, intercomputer communications, control  computer, problem computer, roll-in, roll-out
1845	Loader Standardization for Overlay Programs	The overlay capability is described for four of the third generation computer systems: CDC-6000,  GE-635, IBM-360, and UNIVAC-1108.  A critique of the first three systems is based on actual experience  with a large overlaid trajectory simulation program; a short history and description of this program  is presented.  A standardization of minimum capabilities for loaders is recommended so that programs  which must operate under more than one computer system may be easily converted and maintained.  A proposal  that overlay software incorporates a memory occupation specification concept instead of the conditional  tree structure is delineated.  This concept provides more efficient and cost-effective utilization of  the memory as well as increased flexibility in program structure.	1969	Lanzano	loader, overlay, partition, region, segmentation, linkage, linkage editor, standardization, memory  utilization, memory occupation, tree structure, CDC-6000, GE-635, IBM-360, UNIVAC-1108
1846	On Simulating Networks of Parallel Processes in Which Simultaneous Events May Occur	Some of the problems of simulating discrete event systems, particularly computer systems, on  a conventional digital computer are dealt with.  The systems are assumed to be described as a network  of interconnected sequential processes.  Briefly reviewed are the common techniques used to handle such  simulations when simultaneous events do not occur, can be ignored, or can be handled by simple priority  rules.  Following this, the problem of dealing with simultaneous events in separate processes is introduced.   An abstraction of this problem is developed which admits solution for a majority of commonly encountered problems.  The technique will either find a method of simulating the parallel events or report that none  can be found.  In some of the latter cases it is shown to be possible to find a solution by extending  the information available to the solution technique, but in many cases the technique becomes computationally  unfeasible when the additional information is provided.	1969	Parnas	simulation, parallel processes, simultaneous events, picture processing, computer system simulation
1847	An Algorithm for Finding a Fundamental Set of Cycles of a Graph	A fast method is presented for finding a fundamental set of cycles for an undirected finite  graph.  A spanning tree is grown and the vertices examined in turn, unexamined vertices being stored  in a pushdown list to await examination.  One stage in the process is to take the top element v of the  pushdown list and examine it, i.e. inspect all those edges (v,z) of the graph for which z has not yet  been examined.  If z is already in the tree, a fundamental cycle is added; if not, the edge (v,z) is  placed in the tree.  There is exactly one such stage for each of the n vertices of the graph.  For large  n, the store required in creases as n^2 and the time as n^g where g depends on the type of graph involved.   g is bounded below by 2 and above by 3, and it is shown that both bounds are attained.  In terms of  storage our algorithm is similar to that of Gotlieb and Corneil and superior to that of Welch; in terms  of speed it is similar to that of Welch and superior to that of Gotlieb and Corneil.  Testsshow our  algorithm to be remarkably efficient (g=2) on random graphs.	1969	Paton	fundamental cycle set, graph, algorithm, cycle, spanning tree
1848	The Damped Taylor's Series Method for Minimizing a Sum of Squares and for Solving Systems of  Nonlinear Equations (Algorithm 315 [E4, C5])		1969	Silverman	solution of equations, least squares approximation, Newton's method
1849	Function Minimization (Algorithm 251 [E4])		1969	Hamilton, Boothroyd	function minimization
1850	Generation of Permutations in Lexicographic Order (Algorithm 323 [G6])		1969	Leitch	permutations, direct lexicographic order, reverse lexicographic order, lexicographic generation
1851	Generator of Spanning Trees (Algorithms 354 [H])		1969	McIlroy	spanning trees, trees, graphs
1852	A Base for a Mobile Programming System	An algorithm for a macro processor which has been used as the base of an implementation, by  bootstrapping, of processors for programming languages is described.  This algorithm can be easily implemented  on contemporary computing machines.  Experience with programming languages whose implementation is based on this algorithm indicates that such a language can be transferred to a new machine in less than one  man-week without using the old machine.	1969	Orgass, Waite	bootstrapping, macro processing, machine independence, programming languages, implementation techniques
1853	Compact List Representation: Definition, Garbage Collection, and System Implementation	Compact lists are stored sequentially in memory, rather than chained with pointers.  Since  this is not always convenient, the Swym system permits a list to be chained, compact, or any combination  of the two.  A description is given of that list representation and the operators implemented (most are  similar to those of LISP 1.5).  The system garbage collector attempts to make all lists compact; it relocates  and rearranges all of list storage using temporary storage.  This unique list-compacting garbage collection  algorithm is presented in detail.  Several classes of the macros used to implement the system are described.   Finally, consideration is given to those design factors essential to the success of a plex processing  system implementation.	1969	Hansen	data structure, data representation, list structure, list representation, list, compact list, garbage  collection, relocation, storage reclamation, macro, primitive list operations, plex processing, plex, pointer, list processing system, LISP, free storage
1854	On Multiprogramming, Machine Coding, and Computer Organization	The author feels that the interrupt feature which is available in most modern computers is  a potent source of programming pitfalls and errors, and that it therefore may heavily contribute to the  unreliability of programs making use of it.  A programming scheme is presented which avoids the concept  of the interrupt and permits the specification of concurrent (or pseudoconcurrent) activities in a supposedly  more perspicuous manner.  It is intended to serve as a basis for the construction of operating systems,  which are prime examples of programs with concurrent activities.  The scheme includes a set of basic  instructions for the generation, termination, and synchronization of parallel processes.  A set of routines  representing these instructions and thereby simulating a hypothetical machine organization has been implemented  and test on the IBM System/360.  Two programs using these instructions, written in PL360, are presented.	1969	Wirth	multiprogramming, parallelism, interrupt, input-output, computer organization, file handling, PL360
1855	A Program for the Syntactic Analysis of English Sentences	A program is described which produces syntactic analyses of English sentences with respect  to a transformational grammar.  The main features of the analyzer are that it uses only a limited dictionary  of English words and that it pursues all analysis paths simultaneously while processing the sentence  from left to right.  The form of representation used for the dictionary and the grammar is indicated  and an outline account is given of the analysis procedure.  Techniques for keeping the size of the analysis  record within reasonable limits and for avoiding the need for dynamic application of certain transformational  rules are described.   A number of examples of output produced by the program are given.  The output  includes timing information.	1969	Dewar, Bratley	syntactic analysis, language processing, language analysis, parsing, analysis procedure, recognition  procedure, English sentences, linguistics, psycholinguistics, transformational grammar, limited dictionary, predictive analysis
1856	The Teachable Language Comprehender:  A Simulation Program and Theory of Language	The Teachable Language Comprehender (TLC) is a program designed to be capable of being taught  to "comprehend" English text.  When text which the program has not seen before is input to it, it comprehends  that text by correctly relating each (explicit or implicit) assertion of the new text to a large memory.   This memory is a "semantic network" representing factual assertions about the world.  The program also  creates copies of the parts of its memory which have been found to relate to the new text, adapting and  combining these copies to represent the meaning of the new text.  By this means, the meaning of all text  the program successfully comprehends is encoded into the same format as that of the memory.  In this  form it can be added into the memory.  Both factual assertions for the memory and the capabilities for  correctly relating text to the memory's prior content are to be taught to the program as they are needed.   TLC presently contains a relatively small number of examples of such assertions and capabilities, but  within the system, notations for expressing either of these are provided.  Thus the program now corresponds  to a general process for comprehending language, and it provides a methodology for adding the additional  information this process requires to actually comprehend text of any particular kind.  The memory structure  and comprehension process of TLC allow new factual assertions and capabilities for relating text to such  stored assertions to generalize automatically.  That is, once such an assertion or capability is put  into the system, it becomes available to help comprehend a great many other sentences in the future.   Thus the addition of a single factual assertion or linguistic capability will often provide a large  increment in TLC's effective knowledge of the world and in its overall ability to comprehend text.  The  program's strategy is presented as a general theory of language comprehension.	1969	Quillian	natural language processing, natural language comprehension, teachable computer program, psychological  simulation, human memory simulation, computer linguistics, linguistic performance theory
1857	Filon Quadrature (Algorithm [D1])		1969	Chase, Fosdick	quadrature, Filon quadrature, integration, Filon integration, Fourier coefficients, Fourier series
1858	An Algorithm for Filon Quadrature	An algorithm for Filon quadrature is described.  Considerable attention has been devoted to  an analysis of the round-off and truncation errors.  The algorithm includes an automatic error control  feature.	1969	Chase, Fosdick	quadrature, Filon quadrature, integration, Filon integration, Fourier coefficients, Fourier series
1859	Error Bounds for Periodic Quintic Splines	Explicit error bounds for periodic quintic spline interpolation are developed.  The first (third)  derivative of the periodic spline is shown to be a sixth (fourth) order approximation at the mesh points  to the first (third) derivative of the function being interpolated.	1969	Hall	spline, interpolation, error bounds
1860	An Algol-Based Associative Language	A high level programming language for large, complex associative structures has been designed  and implemented.  The underlying data structure has been implemented using a hash-coding technique.   The discussion includes a comparison with other work and examples of applications of the language.	1969	Feldman	ALGOL, associative, programming language, data structure
1861	The MAD Definition Facility	One of the first definition facilities for higher level languages is described.  Users of the  language can define new operators and/or data types into the MAD language, so that their use appears  as if they were predefined.  Information is given on how one writes definitions, as well as on much of  the motivation behind the form in which definitions are written. Some conclusions are drawn about future  definitional facilities.	1969	Arden, Galler, Graham	MAD, definitions, operators, macros, higher level language
1862	Computing Capabilities at Argentine and Chilean Universities	The author reports on a trip to universities in Argentina and Chile during November 1968, describing  university conditions and computing activities.  As elsewhere, these universities are experiencing student  discontent with the status quo and the solutions they are attempting contrast: Argentina is excluding  students from participating in university government; Chile is allowing such participation.  University  computing service and academic activities are limited.  The number of computers is small and so is the  capacity, none larger than an IBM 360/40; with some exception, computing science academic programs are  rare. This situation is by no means attributable to those responsible for computing developments, who  strive for excellence; rather the "system" is hard to over-come.  Universities, especially those with  strong European traditions, adapt slowly to new academic resources and disciplines; superimposed are  the severe technological and economic constraints of the developing nation.  Consequently, in the absence  of conscious government emphasis on strengthening computing capabilities, future progress may be retarded.	1969	Finerman	university education, computing science academic programs, university computing centers, surveys  of computing centers, university computing capabilities, university-student relationship, Argentine universities, Chilean universities, South American universities, developing nations
1863	Minit Algorithm for Linear Programming (Algorithm 333 [H])		1969	Messham	linear programming, dual simplex method, primal problem, dual problem
1864	Generation of Hilbert Derived Test Matrix (Algorithm 274 [F1])		1969	Boothroyd	test matrix, Hilbert matrix
1865	Algol 60 Reference Language Editor (Algorithm 268 [R2])		1969	Sauer	symbol manipulation
1866	Characteristic Values and Associated Solutions of Mathieu's Differential Equation (Algorithm  352 [S22])		1969	Clemm	Mathieu's differential equation, Mathieu function, characteristic value, periodic solution, radial  solution
1867	On the Expected Lengths of Sequences Generated in Sorting by Replacement Selecting	In the replacement-selecting technique of sorting, one is interested in the ratio L(j) of the  expected length of the j-th sequence generated by the technique to the number of memory cells used.   Using complex-variable theory, it is shown that L(j) -> 2 and that, asymptotically, the average interval  between sign changes of L(j)-2 is 2.6662.	1969	Hooker	replacement selecting, sorting, sequence lengths, asymptotic expected length, recursion relation, generating function, meromorphic function
1868	On Obtaining Correct Input:A New Approach	Most information put into machine readable form, whether from scientific or business origins,  is still keypunched.  This paper is addressed toward the difficulty of obtaining correctly keypunched  and key verified data and an alternative method is suggested in which the computer itself is used to rule  out the possibility of errors in input.  This technique is explained and illustrated by reference to  a working program which involves essentially two phases: in the first phase errors are detected by the  machine, and subsequently, in the second phase, they are corrected by it.	1969	Kennedy	correct data, correct input, data correction, keypunch, key verifier, verifier
1869	Block Structures, Indirect Addressing, and Garbage Collection	Programming languages have included explicit or implicit block structures to provide a naming  convenience for the programmer.  However, when indirect addressing is used, as in SNOBOL, naming constraints  may be introduced.  Two modifications to SNOBOL are described, resulting in two desirable consequences:  (1) naming constraints disappear even when there is indirect addressing within function definitions;  and (2) there is a significant saving in the number of calls to the garbage collector, because some garbage  is collected, at little expense, each time a function returns to its calling program.  These modifications  have been implemented as an extension to a SNOBOL dialect.	1969	Kain	block structures, indirect addressing, garbage collection, local names, SNOBOL
1870	Some Techniques for Using Pseudorandom Numbers in Computer Simulation	An algorithm is described by which uniform pseudorandom integers may be used to construct binary  "numbers" in which the probability that each bit in the word is a 1-bit and can assume any desired parameter  value.  Techniques for making use of such "numbers" in simulation programming are described.	1969	Donnelly	random numbers, simulation, Boolean algebra, bit manipulation
1871	Automatic Contour Map	Some methods for contour mapping by means of a digital plotter are discussed, and a new method  is presented that is simple enough to be implemented by programs with a rather small number of instructions  (about 120 FORTRAN IV instructions are required).  Comparisons with some methods proposed by other authors  are also performed,  A FORTRAN IV program implementing the proposed method is available at the Istituto  di Elettrotecnica ed Elettronica, Politencnico di Milano.	1969	Cottafava, LeMoli	contour map, level lines, digital plotting, function scanning
1872	Chebyshev Interpolation and Quadrature Formulas of Very High Degree (Errata)		1969	Salzer	
1873	Accelerating LP Algorithms	It is shown how a novel method for computing (related) inner products can accelerate the pricing  phase of LP algorithms.  Other LP applications are indicated.	1969	Fox	linear programming, revised simplex method, multiple pricing, inner product
1874	Generating Pseudorandom Numbers on a Two's Complement Machine such as the IBM 360	The familiar multiplicative congruential generator is examined in the context of the type of  two's complement arithmetic used in the IBM 360 series.  Different sequences of residues are considered  and relationships established among them.  It is shown that a sequence of positive and negative residues  may be produced more simply and economically than with the conventional approach and yet have twice the  period of the latter without loss of desirable statistical properties.  Another easily generated sequence  involving absolute values is also shown to have twice the period but with less attractive statistical  properties.  The statistical properties of these sequences are given and related to previously established  criteria.	1969	Hemmerle	random number, uniform distribution, pseudo-random number, random number generator, multiplicative  congruential generator, power residue, two's complement arithmetic, IBM 360 arithmetic
1875	Polynomial and Spline Approximation by Quadratic Programming	The problem of approximation to a given function, or of fitting a given set of data, where  the approximating function is required to have certain of its derivations of specified sign over the  whole range of approximation, is studied.  Two approaches are presented, in each of which quadratic programming  is used to provide both the constraints on the derivatives and the selection of the function which yields  the best fit.  The first is a modified Bernstein polynomial scheme, and the second is a spline fit.	1969	Amos, Slater	constant sign derivatives, Bernstein polynomials, linear concavity constraints, quadratic programming  splines
1876	Generation of Test Matrices Having Certain Sign Patterns and Prescribed Positive Spectra	A class of orthogonal transformations is presented whose members transform a given positive  diagonal matrix into a matrix having one of four special sign patterns.	1969	Schneider	test matrices, positive matrices, sign patterns, orthogonal transformations, positive eigenvalues
1877	Prevention of System Deadlocks	A well-known problem in the design of operating systems is the selection of a resource allocation  policy that will prevent deadlock.  Deadlock is the situation in which resources have been allocated  to various tasks in such a way that none of the tasks can continue.  The various published solutions  have been somewhat restrictive: either they do not handle the problem in sufficient generality or they  suggest policies which will on occasion refuse a request which could have been safely granted.  Algorithms  are presented which examine a request in the light of the current allocation of resources and determine  whether or not the granting of the request will introduce the possibility of a deadlock.  Proofs given  in the appendixes show that the conditions imposed by the algorithms are both necessary and sufficient  to prevent deadlock.  The algorithms have been successfully used in the THE system.	1969	Habermann	multiprogramming, time-sharing, scheduling, resource allocation
1878	Recovery of Reentrant List Structures in SLIP	One consequence of the reference-count-based space-recovery system employed by SLIP is that  reentrant list structures are not recovered even when explicitly erased.  LISP-like garbage-collection  schemes are free of this impediment.  They however, depend on being able to find and mark nodes that  are reachable from program variables.  By tracing all descendants from program variables may then be  identified and collected.  The list-creating function LIST of SLIP may be amended to mark those lists  for which the programmer wishes to assume responsibility.  Given this modification, a LISP-like garbage  collector that recovers abandoned reentrant list structures may then be appended to the SLIP system.	1969	Weizenbaum	list processing, SLIP, garbage-collection
1879	A Note on Storage Fragmentation and Program Segmentation	The main purpose of this paper is the presentation of some of the results of a series of simulation  experiments investigating the phenomenon of storage fragmentation.  Two different types of storage fragmentation  are distinguished: (1) external fragmentation, namely the loss in storage utilization caused by the inability  to make use of all available storage after it has been fragmented into a large number of separate blocks;  and (2) internal fragmentation, the loss of utilization caused by rounding up a request for storage,  rather than allocating only the exact number of words required.  The most striking result is the apparently  general rule that rounding up requests for storage, to reduce the number of different sizes of blocks  coexisting in storage, causes more loss of storage by increased internal fragmentation than is saved  by decreased external fragmentation.  Described also are a method of segment allocation and an accompanying  technique for segment addressing which take advantage of the above result.  Evidence is presented of  possible advantages of the method over conventional paging techniques.	1969	Randell	storage allocation, storage fragmentation, paging, segmentation, addressing
1880	Chebyshev Solution to an Overdetermined Linear System (Algorithm 328 [F4])		1969	Golub	Chebyshev solutions, over-determined linear systems, linear equations, exchange algorithm
1881	Transpose Vector Stored Array (Algorithm 302 [K2])		1969	MacLeod	matrix transposition, array transposition, vector stored array
1882	Determination of the Square Root of a Positive Definite Matrix (Algorithm 298 [F1])		1969	Duke	matrix, symmetric matrix, positive definite matrix, matrix square root
1883	Modified Romberg Quadrature(Algorithm [D1])		1969	Fair weather	numerical integration, Romberg quadrature, trapezoid values, rectangle values, error bound
1884	An Anomaly in Space-Time Characteristics of Certain Programs Running in a Paging Machine	The running time of programs in a paging machine generally increases as the store in which  programs are constrained to run decreases.  Experiments, however, have revealed cases in which the reverse  is true: a decrease in the size of the store is accompanied by a decrease in running time.  An informal  discussion of the anomalous behavior is given, and for the case of the FIFO replacement algorithm a formal  treatment is presented.	1969	Belady, Nelson, Shedler	paging machines, demand paging, replacement algorithm
1885	A Computer System for Transformational Grammar	A comprehensive system for transformational grammar has been designed and implemented on the  IBM 360/67 computer.  The system deals with the transformational model of syntax, along the lines of  Chomsky's Aspects of the Theory of Syntax. The major innovations include a full,formal description of  the syntax of a transformational grammar, a directed random phrase structure generator, a lexical insertion  algorithm, an extended definition of analysis, and a simple problem-oriented programming language in  which the algorithm for application of transformations can be expressed.  In this paper we present the  system as a whole, first discussing the general attitudes underlying the development of the system, then  outlining the system and discussing its more important special features.  References are given to papers  which consider some particular aspect of the system in detail.	1969	Friedman	transformational grammar, natural language syntax, language processing, language analysis, sentence  generation, lexical insertion, computational linguistics, syntax
1886	Generation of Optimal Code for Expressions via Factorization	Given a set of expressions which are to be compiled, methods are presented for increasing the  efficiency of the object code produced by first factoring the expressions, i.e. finding a set of subexpressions  each of which occurs in two or more other expressions or subexpressions.  Once all the factors have been  ascertained, a sequencing procedure is applied which orders the factors and expressions such that all  information is computed in the correct sequence and factors need be retained in memory a minimal amount  of time.  An assignment algorithm is then executed in order to minimize the total number of temporary  storage cells required to hold the results of evaluating the factors.  In order to make these techniques  computationally feasible, heuristic procedures are applied, and hence global optimal results are not  necessarily generated.  The factorization algorithms are also applicable to the problem of factoring  Boolean switching expressions and of factoring polynomials encountered in symbol manipulating systems.	1969	Breuer	factorization algorithms, code optimization, sequencing of operations, detection of common subexpressions, factorization of Boolean expressions
1887	A Recursive Relation for the Determinant of a Pentadiagonal Matrix	A recursive relation, relating leading principal minors, is developed for the determinant of  a pentadiagonal matrix.  A numerical example is included to indicate its use in calculating eigenvalues.	1969	Sweet	pentadiagonal matrix, quindiagonal matrix, quindiagonal matrix, band matrix, determinant, characteristic  polynomial, eigenvalues
1888	Spline Function Methods for Nonlinear Boundary-Value Problems	The solution of the nonlinear differential equation Y"=F(x,Y,Y') with two-point boundary conditions  is approximated by a quintic or cubic spline function y(x).  The method is well suited to nonuniform  mesh size and dynamic mesh size allocation.  For uniform mesh size h, the error in the quintic spline  y(x) is O(h^4), with typical error one-third that from Numerov's method.  Requiring the differential  equation to be satisfied at the mesh points results in a set of difference equations, which are block  tridiagonal and so are easily solved by relaxation or other standard methods.	1969	Blue	boundary value problems, differential equations, finite differences, functional approximation, iterative methods, nonlinear equations, spline functions
1889	Introducing Computing to Smaller Colleges and Universities -- A Progress Report	By technical means that are now routine, computer service for smaller colleges and universities  can be provided by remote terminals of a central facility.  Access, however, is not enough-effective  organizational and educational methodology for introducing computing at such institutions must also be  developed.  The experience of two years with a statewide network involving-41 institutions is discussed.   Lessons include the importance of a separate organization representing the small colleges, the necessity  for on-campus training for the institutions, the need for some special programming and documentation  to support such users,and the development of curriculum by evolutionary means.	1969	Parker Jr., Gallie, Brooks Jr., Ferrel	regional network, introducing computing, under-graduate education, instructional usage, academic  applications, curriculum development, orientation project, regional center, consortium, remote computing
1890	Simulation of Traffic Flows in a Network	A computer simulation program which deals with traffic flows in the network of a large area  is described.  Each road is segmented into blocks of several ten-meter lengths and is represented by  a bidirectional list in computer memory.  The movement of cars, i.e. the transfer of cars from one block  to the next, is expressed by a proper formula.  This formula is based on the supposition that the speed  of cars in a block is determined only by the density of cars in the block, and this speed-versus-density  curve is empirically given the numerical values.  This simulation scheme has its excellent point in that  it makes it possible to trace the dynamic behavior of traffic flows in a variety of situations, some  examples of which are given for an actual area of the city of Kyoto, Japan.	1969	Sakai, Nagao	traffic simulation, traffic flow, traffic network traffic control, traffic density, intersection, signal setting, vehicle, road network, list structure, computer simulation
1891	Three-Dimensional Computer Display	A stereographic display terminal has been produced using the raster display (BRAD) recently  developed at Brookhaven.  The system uses a rotating refresh memory to feed standard television monitors.   To produce a stereographic display the computer calculates the projected video images of an object,  viewed from two separate points.  The resulting video maps are stored on separate refresh bands of the  rotating memory.  The two output signals are connected to separate color guns of a color television monitor,  thus creating a superimposed image on the screen.  Optical separation is achieved by viewing the image  through color filters.  The display is interactive and can be viewed by a large group of people at the  same time.	1969	Ophir, Shepherd, Spinrad	computer graphics, three-dimensional display, swept raster display, interactive stereographic terminal, video map, color separation
1892	Degree of Multiprogramming in Page-on-Demand Systems	A simple stochastic model is described which offers a base for understanding the relationship  between the number of programs permitted to share memory (the degree of multiprogramming), drum traffic  rates, and central processing unit utilization in page-on-demand, multiprogrammed, time-shared computer  systems.  The model preserves, as a key feature, the property of page-demand statistics which implies  a "burst" of page demands at the beginning of any job or quantum execution.  The model, a Markov chain,  is analyzed numerically and the results are presented graphically for a wide range of key environment-descriptive  parameters.  Implications of the results to time-shared system design and programming are discussed,  and a calculation of the optimal degree of multiprogramming for a wide range of parameters is presented  graphically.	1969	Wallace, Mason	page-on-demand, demand paging, time-sharing multiprogramming, Markovian computer models, scheduling  strategies, operating systems, memory management
1893	Roots of Polynomials by a Root-Squaring and Resultant routine (Algorithm 340 [C2])		1969	Noltemeier	root finders, roots of polynomial equations, polynomial zeros, root-squaring operations, Graeffe  method, resultant procedure, subresultant procedure, testing of roots, acceptance criteria
1894	Normal Random Deviates (Algorithm 334 [G5])		1969	Knop	normal deviates, normal distribution, random number, random number generator, simulation, probability  distribution, frequency distribution, random
1895	Gaussian Quadrature Formulas (Algorithm 331 [D1])		1969	Hill	quadrature, Gaussian quadrature, numerical integration, weight function, orthogonal polynomials
1896	Regular Coulomb Wave Functions (Algorithm 292 S22])		1969	Gautschi	Coulomb wave functions, wave functions, regular Coulomb wave functions
1897	Coulomb Wave Functions (Algorithm 300 [S22])		1969	Kolbig	Coulomb wave functions, wave functions
1898	Regular Coulomb Wave Functions (Algorithm 292 [S22])		1969	Kolbig	Coulomb wave functions, wave functions, regular Coulomb wave functions
1899	Simplex Method Procedure Employing Lu Decomposition (Algorithm 350 [H])		1969	Bartels, Golub	simplex method, linear programming, LU decomposition, round-off errors, computational stability
1900	Clarification of Fortran Standards-Initial Progress	In 1966 after four years of effort, FORTRAN became the first programming language standardized  in the United States.  Since that initial achievement, study and application of the standard specifications  have revealed the need for maintenance of the standards.  As the result of work initiated in 1967, an  initial set of clarifying interpretations has been prepared.  The nature of the maintenance, corrections  to the standard specifications, and completed interpretations are reported.	1969		USA Standard, FORTRAN, Basic FORTRAN, programming language, standardization, language standard  specification, language standard maintenance, language standard clarification, language standard interpretation, standardization committee
1901	Dynamic Space-Sharing in Computer Systems	A formalization of relationships between space-shading program behavior, and processor efficiency  in computer systems is presented.  Concepts of value and cost of space allocation per task are defined  and then value and cost are combined to develop a single parameter termed value per unit cost.  The intent  is to illustrate a possible analytic approach to the investigation of the problems of space-sharing and  to demonstrate the method on sample problems.	1969	Belady, Kuehner	space-sharing, storage allocation, memory allocation, storage management, memory management, program  behavior, multiprogramming, computer system design, allocation strategies, replacement strategies, demand-paging, time-sharing
1902	An Automatic Grading Scheme for Simple Programming Exercises	A discussion is given of alterations that were made to a typical university operating system  to record the results of programming exercises in three different languages, including assembly language.   In this computer-controlled grading scheme provision is made for testing with programmer-supplied data  and for final runs with system-supplied data.  Exercises run under the scheme may be mixed with other  programs, and no special recognition of exercises by the operators is necessary.	1969	Hext, Winings	automatic grading program, programming exercises
1903	Chebyshev Interpolation and Quadrature Formulas of Very High Degree		1969	Salzer	Chebyshev polynomials, Chebyshev interpolation, Chebyshev quadrature, Chebyshev points, Chebyshev  zeros, interpolation, quadrature, definite integrals
1904	Rough and Ready Error Estimates in Gaussian Integration of Analytic Functions		1969	Rabinowitz	numerical integration, analytic functions, error estimates, Gaussian integration, tabulated error  coefficients, computable error coefficients, Cauchy integral formula, Chebyshev polynomials
1905	The Simplex Method of Linear Programming Using LU Decomposition	Standard computer implementations of Dantzig's simplex method for linear programming are based  upon forming the inverse of the basic matrix and updating the inverse after every step of the method.   These implementations have bad round-off error properties.  This paper gives the theoretical background  for an implementation which is based upon the LU decomposition, computed with row interchanges, of the  basic matrix.  The implementation is slow, but has good round-off error behavior.  The implementation  appears as CACM Algorithm 350.	1969	Bartels, Goulub	simplex method, linear programming, LU decomposition, round-off errors, computational stability
1906	Automated Printed Circuit Routing with a Stepping Aperture	A computer program for routing interconnections on a two-sided printed circuit board with a  regular pattern of lines, pins (terminals), and vias (feed-through holes) is described.  In this program,  each interconnection is given a planned routing-typically, down from the upper pin, through a via, and  horizontally to the lower pin.  From the top, a virtual aperture (i.e. a long horizontal slit) is stepped  down the board.  The planned routing is the basis for rerouting interconnections within the aperture  to resolve conflicts for lines and vias below the aperture and to maximize the effective line usage.   If a conflict has not been resolved before the aperture arrives at the lower pin,interconnections are  deleted to resolve the conflict.  Extensions of this technique to the control of crosstalk between routed  interconnections and to the problem of obtaining 100 percent interconnect are also discussed.	1969	Lass	routing, printed circuit, interconnections, aperture, stepping aperture, computer program, circuit  board, lines, vias, pins
1907	A Note on Reliable Full-Duplex Transmission over Half-Duplex Links	A simple procedure for achieving reliable full-duplex transmission over half-duplex links is  proposed. The scheme is compared with another of the same type, which has recently been described in  the literature.  Finally, some comments are made on another group of related transmission procedures  which have been shown to be unreliable under some circumstances.	1969	Bartlett, Scantlebury	data transmission, error correction, full-duplex, half-duplex, transmission control, communications
1908	Time-Sharing and Batch-Processing:  An Experimental Comparison of Their Values in a Problem -  Solving Situation	An experimental comparison of problem-solving using time-sharing and batch-processing computer  systems conducted at MIT is described in this paper.  This study is the first known attempt to evaluate  two such systems for what may well be the predominant user population within the next decade-the professionals  who, as nonprogrammers, are using the computer as an aid in decision-making and problem-solving rather  than as a programming end in itself.  Statistically and logically significant results indicate equal  cost for usage of the two computer systems; however, a much higher level of performance is attained by  time-sharing users.  There are indications that significantly lower costs would have resulted if the  time-sharing users had stopped work when they reached a performance level equal to that of the batch  users.  The users' speed of problem-solving and their attitudes made time-sharing the more favorable  system.	1969	Gold	time-sharing vs batch-processing, user performance, man/machine communications, cost effectiveness, on-line vs off-line performance, decision-making performance, user/programmer behavior, programming experimental  empirical studies, problem-solving, research in man/machine communications, man/machine symbiosis
1909	Computation of Jn(x) by Numerical Integration	It is shown to be practical to compute Jn(x) by numerical integration of its integral representation  using the trapezoidal rule. The error in this approximation was studied empirically.	1969	Stroud, Kohli	Bessel Function, numerical integration, trapezoidal rule
1910	An Algorithm for Solving a Special Class of Tridiagonal Systems of Linear Equations	An algorithm is presented for solving a system of linear equation Bu=k where B is tridiagonal  and of a special form.  It is shown that this algorithm is almost twice as fast as the Gaussian elimination  method usually suggested for solving such systems.  In addition, explicit formulas for the inverse and  determinant of the matrix B are given.	1969	Rose	tridiagonal, Gaussian elimination, central difference
1911	On Coordination Reduction and Sentence Analysis	A class of coordination phenomena in natural languages is considered within the frame work  of transformational theory.  To account for these phenomena it is proposed that certain machinery be  added to the syntactic component of a transformational grammar. This machinery includes certain rule  schemata, the conditions under which they are to be applied, and conditions determining the sequence  of subtrees on which they are to be performed.  A solution to the syntactic analysis problem for this  class of grammars is outlined.  Precise specification of both the generative procedure of this paper  and its inverse is given in the form of LISP function definitions.	1969	Petrick, Postal, Rosenbaum	natural languages, generative grammar, transformational theory, syntax, coordination, sentence  coordination, sentence coordination, coordination reduction, syntactic analysis, grammar testing program, rule testing
1912	Simulation of Outpatient Appointment Systems	An experimental computer program is described which simulates appointment systems employed  by outpatient departments of hospitals.  Both major kinds of appointment systems-individual and block-can  be simulated.  The purpose of the Simulator is to enable the user to evaluate the effectiveness of alternative  appointment systems in a given clinical environment.	1969	Katz	simulation, scheduling, appointment system, outpatient department, medicine, health, management science, operations research
1913	Polygamma Functions with Arbitrary Precision (Algorithm 349 [S14])		1969	Schwachheim	polygamma function, psi function, digamma function, trigamma function, tetragamma function, pentagamma  function, special functions
1914	Matrix Scaling by Integer Programming (Algorithm 348 [F1])		1969	Klimpel	integer programming, linear algebra, mathematical programming, matrix condition, matrix scaling
1915	An Algorithm for Hidden Line Elimination	The algorithm presented causes the elimination of hidden lines in the representation of a perspective  view of concave and convex plane-faced objects on the picture plane.  All the edges of the objects are  considered sequentially, and all planes which hide every point of an edge are found.  The computing time  increases roughly as the square of the number of edges.  The algorithm takes advantage of a reduced number  of concave points and automatically recognizes if only one object with no concave points is considered.  In this last case, the result is obtained in a much simpler way.	1969	Galimberti, Montanari	hidden line elimination, back line recognition, three-dimensional representation, plane-faced objects, perspective view, machine rendering of solids, automatic drawing, displaying techniques, computer graphics, man/machine interaction, man/machine communication, computer-aided design
1916	Analysis of Boolean Program Models for Time-Shared, Paged Environments	Directed graphs or their associated matrices are frequently used to represent the logical structure  of sequences of computer instructions.  Such techniques are used and, in addition, data references are  represented in a nondirected model. The complete structural specification of a program is represented  by a combined model.  A transformation of the combined model yields a new model in which additional timing  information is also contained.  Analysis of these models prior to execution yields information valuable  in determining segmentation of instructions and data for a time-shared environment, as well as for initial  page loading; during execution, the analysis may be used for "look ahead" control of page turning.	1969	Lowe	time-sharing, paging, segmentation, executive, compiler, monitor, program model
1917	An Algol Procedure for the Fast Fourier Transform with Arbitrary Factors (Algorithm 339 [C6])		1969	Singleton	fast Fourier transform, complex Fourier transform, multivariate Fourier transform, Fourier series, harmonic analysis, spectral analysis, orthogonal polynomials, orthogonal transformation, virtual core  memory, permutation
1918	Distribution of Indistinguishable Objects into Distinguishable Slots (Algorithm 329 [G6])		1969	Gray	
1919	An Efficient Algorithm for Sorting with Minimal Storage (Algorithm 347 [M1])		1969	Singleton	sorting, minimal storage sorting, digital computer sorting
1920	F-Test Probabilities (Algorithm 346 [S14])		1969	Morris	F-test, Snedecor F-statistic, Fisher Test, distribution function
1921	An Algol Convolution Procedure Based on the Fast Fourier Transform (Algorithm 345 [C6])		1969	Singleton	fast Fourier transform, complex Fourier transform, multivariate Fourier transform, Fourier series, harmonic analysis, spectral analysis, orthogonal polynomials, orthogonal transformation, convolution, auto covariance, autocorrelation, cross-correlation, digital filtering, permutation
1922	Proposed USA Standard (Data Communication Control Procedures for the USA Standarad Code for Information  Interchange)		1969		data communication, data communication control procedures, data communication establishment/termination  procedures, data communication message transfer procedures, data communication error control procedures, data communication polling/selection procedures, communication, communication control procedures, communication  establishment/termination procedures, communication message transfer procedures, communication error  control procedures, communication polling/selection procedures, link, link control procedures, link establishment/termination  procedures, link message transfer procedures, link error control procedures, link polling/selection procedures, data link, data link control procedures, data link  establishment/termination procedures, data link message  transfer procedures, data link error control procedures, data link polling/selection procedures
1923	Pseudofiles	An approach to system interfaces for high level languages using basic input/output support  facilities is described.  It is shown that this technique can provide potentially inexpensive methods  for programs to communicate with deeply embedded facilities such as command language processors.	1969	Rosin	operating systems, interfaces input-output, high level languages, command language
1924	Organizing Matrices and Matrix Operations for Paged Memory Systems	Matrix representations and operations are examined for the purpose of minimizing the page faulting  occurring in a paged memory system.  It is shown that carefully designed matrix algorithms can lead to  enormous savings in the number of page faults occurring when only a small part of the total matrix can  be in main memory at one time.  Examination of addition, multiplication, and inversion algorithms shows  that a partitioned matrix representation (i.e. one submatrix or partition per page) in most cases induced  fewer page faults than a row-by-row representation.  The number of page-pulls required by these matrix  manipulation algorithms is also studied as a function of the number of pages of main memory available  to the algorithm.	1969	McKellar, Coffman Jr.	matrix algorithms, array processing, paging algorithms, paged memory systems, virtual memory systems, array storage allocation, storage allocation
1925	Concepts of Use in Contour Map Processing	Generalized techniques whose use can simplify the solution of problems relating to contour  maps.  One of these techniques makes use of the topological properties of contour maps.  The topology  is represented by a graphical structure in which adjacent contour lines appear as connected nodes.  Another  generalized technique consists of utilizing geometrical properties to determine the characteristics of  straight lines drawn on the contour map.  Both of these techniques have been applied to the problem of  locating the ground track of an aircraft from elevation readings obtained during a flight.	1969	Morse	map, contour map, contour lines, topological properties, geometrical properties, graph of contour  map, navigation
1926	Description of FORMAT, a Text-Processing Program	FORMAT is a production program which facilitates the editing and printing of "finished" documents  directly on the printer of a relatively small (64k) computer system.  It features good performance, totally  free-form input, very flexible formatting capabilities including up to eight columns per page, automatic  capitalization, aids for index construction, and a minimum of nontext items.  It is written entirely  in FORTRAN IV.	1969	Berns	text processing, indexing, printing, documentation, text editing, formatting, frequency dictionary, right justification, vocabulary
1927	Information Science in a Ph. Computer Science Program	This report contains recommendations on a sample course curriculum in the general area of information  organization and information system design in a Ph. Computer Science Program.  The subject area is  first briefly described, followed by a listing of some desirable graduate-level courses.  Suitable bibliographies  are appended.	1969	Salton	course curriculum, graduate courses, university courses, computer science curriculum, information  science, information organization, information retrieval, data retrieval, language analysis, information  processing
1928	Exclusive Simulation of Activity in Digital Networks	A technique for simulating the detailed logic networks of large and active digital systems  is described.  Essential objectives sought are improved ease and economy in model generation, economy  in execution time and space, and a facility for handling simultaneous activities.  The main results obtained  are a clear and useful separation of structural and behavioral model description, a reduction of manual  tasks in converting Boolean logic into a structural model, the elimination of manual processes in achieving  exclusive simulation of activity, an event-scheduling technique which does not deteriorate in economy as the event queue grows in length, and a simulation procedure which deals effectively with any mixture  of serial and simultaneous activities.  The passage of time is simulated in a precise, quantitative fashion  and systems to be simulated may be combinations of synchronous and asynchronous logic.  Certain aspects  of the techniques described may be used for the simulation of network structures other than digital networks.	1969	Ulrich	simulation, logical simulation, digital simulation, large systems simulation, network structures, scheduling, queuing, simultaneous activities, parallel events
1929	Images from Computers and Microfilm Plotters	Digital computers are widely used for the processing of information and data of all kinds,  including the pictorial information contained in photographs and other graphical representations.  Efficient  conversion facilities for putting graphical information into the computer and retrieving it in graphical  form are therefore much needed.  One of the most commonly employed devices for obtaining permanent graphical  output from digital computers is the microfilm plotter.  Regrettably, present models have no provision  for producing images with a continuous gray scale or "half tones."  In this note several programming techniques  are described for obtaining half tone pictures from a microfilm plotter under the control of a digital  computer.  Illustrative examples of several methods are given.	1969	Schroeder	computer images, half tone pictures, microfilm plotters; processing
1930	Extremely Portable Random Number Generator	Extremely portable subroutines are sometimes needed for which moderate quality and efficiency  suffice.  Typically, this occurs for library functions (like random number generation and in core sorting)  which are not entirely universal or are not used in a standardized way.  The literature on random number  generators does not seem to contain an algorithm that meets requirements of this sort.  An extremely  portable 8-line FORTRAN program is provided which based on an important paper by Coveyou and MacPherson  (1967)sing their methods, Fourier analysis is applied to the probability function for the consecutive  n-tuples provided by our generator (with n less than or equal to 4).  While the small modulus which must  be used to maintain portability prevents the quality of the generator from being high, the generator  compares well with the bounds established in the above mentioned paper.	1969	Kruskal	random number generators, random numbers, random number analysis, random generators, linear sequential  generators, random number program, pseudo random numbers
1931	Interval Arithmetic Determinant Evaluation and Its Use in Testing for a Chebyshev System	Two recent papers, one by Hansen and one by Hansen and R. R. Smith, have shown how Interval  Arithmetic (I.) can be used effectively to bound errors in matrix computations.  In the present paper  a method proposed by Hasen and R. R. Smith is compared with straightforward use of I. in determinant  evaluation.  Computational results show the accuracy and running times that can be expected when using  I. for determinant evaluation.  An application using I. determinants in a program to test a set of  functions to see if they form a Chebyshev system is then presented.	1969	Smith	interval arithmetic, range arithmetic, error bounds, determinant evaluation, Chebyshev system, mathematical proof by computer
1932	The Logarithmic Error and Newton's Method for the Square Root	The problem of obtaining optimal starting values for the calculation of the square root using  Newton's method is considered.  It has been pointed out elsewhere that if relative error is used as the  measure of goodness of fit, optimal results are not obtained when the initial approximation is a best  fit.  It is shown here that if, instead, the so-called logarithmic error is used, then a best initial  fit is optimal for both types of error.  Moreover, use of the logarithmic error appears to simplify the  problem of determining the optimal initial approximation.	1969	King, Phillips	square root, Newton's method, relative error, logarithmic error, best fit, optimal approximation, maximal error, recurrence relation, integer root, error curve
1933	Coding the Lehmer Pseudo-random Number Generator	An algorithm and coding technique is presented for quick evaluation of the Lehmer pseudo-random  number generator modulo 2**31 - 1, a prime Mersenne number with produces 2**31 - 2 numbers, on a p-bit  (greater than 31) computer.  The computation method is extendible to limited problems in modular arithmetic.   Prime factorization for 2**61 - 2 and a primitive root for 2**61 - 1, the next largest prime Mersenne  number, are given for possible construction of a pseudo-random number generator of increased cycle length.	1969	Payne, Rabung, Bogyo	pseudo-random number, random number, modular arithmetic, uniform probability density, uniform frequency  function, simulation, prime factorization, primitive roots
1934	On Arithmetic Expressions and Trees	A description is given of how a tree representing the evaluation of an arithmetic expression  can be drawn in such a way that the number of accumulators needed for the computation can be represented  in a straightforward manner.  This representation reduces the choice of the best order of computation  to a specific problem under the theory of graphs.  An algorithm to solve this problem is presented.	1969	Redziejowski	arithmetic expression, compiler design, graph theory, programming, storage minimization, topological  ordering, tree
1935	Randomized Binary Search Technique	A mathematical model is developed for the mean and variance of the number of trials to recover  a given document in a randomly received list of files.  The search method described is binary in nature  and offers new potential for information retrieval systems.	1969	Arora, Dent	binary pattern, file examination, graph theory, information retrieval, mathematical model, partitioning, probabilistic method, random sequencing, search techniques, tree structures
1936	Variable Length Tree Structures Having Minimum Average Search Time	Sussenguth suggests in a paper (1963) that a file should be organized as a doubly-chained tree  structure if it is necessary both to search and to update frequently.  Such a structure provides a compromise  between the fast search/slow update characteristics of binary searching and the slow search/fast update  characteristics of serial searching.  His method, however, contains the limiting restriction that all  terminal nodes lie on the same level of the tree.  This paper considers the effect of relaxing this restriction.   First, trees which have the property that a priori the filial set of each node is well defined are studied.   It is proved that coding the nodes within each filial set with respect to the number of terminal nodes  reachable from each is necessary and sufficient to guarantee minimum average search time.  Then the more  general case (that is, where the entire structure of the tree is changeable) is treated.  A procedure  is developed for constructing a tree with a minimum average search time.  A simple closed expression  for this minimum average search time is obtained as a function of the number of terminal nodes.  The  storage capacity required to implement the doubly-chained tree structure on a digital computer is also  determined.  Finally, the total cost of the structure, using Sussenguth's cost criterion, is computed.   It is shown that significant improvements in both the average search time and the total cost can be  obtained by relaxing Sussenguth's restriction that all terminal nodes lie on the same level of the tree.	1969	Patt	information retrieval, file searching, tree structures, double chaining
1937	CODAS: A Data Display System	CODAS, a Customer Oriented Data System, is a user-oriented data retrieval and display system.   The command language of the system provides the user with an easy means for specifying data retrieval  and display requests.  Data is displayed as tables and graphs produced in a format ready for publication.   In this paper the statements of the request language and the general system design are described.	1969	Day, Mansfield, Ellis	data display, information retrieval, graphic display, command languages, report program generation, management data processing
1938	Some Criteria for Time-Sharing System Performance	Time-sharing systems, as defined in this article, are those multiaccess systems which permit  a terminal user to utilize essentially the full resources of the system while sharing its time with other  terminal users.  It is each terminal user's ability to utilize the full resources of the system that  makes quantitative evaluation of time-sharing systems particularly difficult.  Six criteria are described  which have been successfully used to perform first-level quantitative time-sharing system performance  evaluation.	1969	Stimler	time-sharing performance criteria, time-sharing system operation, time-sharing performance analysis
1939	Directed Random Generation of Sentences	The problem of producing sentences of a transformational grammar by using a random generator  to create phrase structure trees for input to the lexical insertion and transformational phases is discussed.   A purely random generator will produce base trees which will be blocked by the transformations, and  which are frequently too long to be of practical interest.  A solution is offered in the form of a computer  program which allows the user to constrain and direct the generation by the simple but powerful device  of restricted subtrees.  The program is a directed random generator which accepts as input a subtree  with restrictions and produces around it a tree which satisfies the restrictions and is ready for the  next phase of the grammar.  The underlying linguistic model is that at Noam Chomsky, as presented in  Aspects of the Theory of Syntax.  The program is written in FORTRAN IV for the IBM 360/67 and is part of a unified computer system for transformational grammar.  It is currently being used with several partial  grammars of English.	1969	Friedman	transformational grammar, natural language syntax, language processing, sentence generation, computational  linguistics, syntax
1940	Calculation of a Polynomial and its Derivative Values by Horner Scheme (Algorithm 337 [C1])		1969	Smith	function, evaluation, polynomial evaluation, ALGOL procedure, Horner's scheme
1941	F-Distribution (Algorithm 322 [S14])		1969	Field	Fisher's F-distribution, Students's t-distribution
1942	Finding a Solution of N Functional Equations in N Unknowns (Algorithm 314 [C5])		1969	Vandergraft, Mesztenyi	functional equations, interpolation, nonlinear equations, secant method
1943	Complete Elliptic Integrals (Algorithm 165 [S21])		1969	Farkas	special functions, complete elliptic integral of the first kind, complete elliptic integral of  the second kind
1944	Student's t-Distribution (Algorithm 344 [S14])		1969	Levine	Student's t-Distribution, t-test, small-sample statistics, distribution function
1945	The Role of Programming in a Ph. Computer Science Program	In this general paper the role of programming in advanced graduate training is discussed.   Subject matter related to programming as well as programming per se is considered.  The  importance and  application of formalism are considered and also the need for good empirical experimentation.  A brief  outline for a sequence of courses is included, and subject headings that have been obtained from an extensive  bibliography are given.  A bibliography of programming references is included.	1969	Arden	graduate-level programming, graduate programs, course content, course sequence, graduate curriculum, programming research topics, programming bibliography
1946	Computing Polynomial Resultants: Bezout's Determinant vs. Collins' Reduced P. Algorithm	Algorithms for computing the resultant of two polynomials in several variables, a key repetitive  step of computation in solving systems of polynomial equations by elimination, are studied.  Determining  the best algorithm for computer implementation depends upon the extent to which extraneous factors are  introduced, the extent of propagation of errors caused by truncation of real coefficients, memory requirements,  and computing speed.  Preliminary considerations narrow the choice of the best algorithm to Bezout's  determinant and Collins' reduced polynomial remainder sequence (p.r.s.) algorithm.  Detailed tests performed  on sample problems conclusively show that Bezout's determinant is superior in all respects except for  univariate polynomials, in which case Collins' reduced p.r.s. algorithm is somewhat faster.  In particular  Bezout's determinant proves to be strikingly superior in numerical accuracy, displaying excellent stability  with regard to round-off errors. Results of tests are reported in detail.	1969	Ku, Adler	resultant algorithm, g.c.d. algorithm, polynomial resultant, elimination, Bezout's determinant, Sylvester's determinant, reduced p.r.s. algorithm, Euclidean algorithm, multivariate polynomial equations
1947	Object code Optimization	Methods of analyzing the control flow and data flow of programs during compilation are applied  to transforming the program to improve object time efficiency.  Dominance relationships, indicating which  statements are necessarily executed before others, are used to do global common expression elimination  and loop identification.  Implementation of these and other optimizations in OS/360 FORTRAN H are described.	1969	Lowry, Medlock	compilers, data flow analysis, dominance, efficiency, FORTRAN, graph theory, loop structure, machine  instructions, object code, optimization, redundancy elimination, register assignment, System/360
1948	Computers in Group Theory: a Survey	Computers are being applied to an increasingly diverse range of problems in group theory.   The most important areas of application at present are coset enumeration, subgroup lattices, automorphism  groups of finite groups, character tables, and commutator calculus.  Group theory programs range from  simple combinatorial or numerical programs to large symbol manipulation systems.  In this survey the  more important algorithms in use are described and contrasted, and results which have been obtained using  existing programs are indicated.  An extensive bibliography is included.	1969	Cannon	group theory, coset enumeration, subgroup lattices, automorphism groups, character tables, commutator  calculus, topology, crystallography, permutation groups, Abelian groups, discrete mathematics, non-numerical  programming, symbol manipulation, survey
1949	Finiteness Assumptions and Intellectual Isolation of Computer Scientists		1970	Wagner	Algol vs. Fortran, finiteness assumptions, intellectual isolation, integer variable range, memory finiteness, finite word size
1950	Efficient Handling of Binary Data		1970	Raduchel	binary variables, dummy variables, bit strings, cross-tabulations
1951	Estimates of Distributions of Random Variables for Certain Computer Communications Traffic Models	A study of multiaccess computer communications has characterized the distributions underlying an elementary  model of the user-computer interactive process.  The model used is elementary in the sense that many of the random variables that generally are of interest in computer communications studies can be decomposed into the elements of this model.  Data were examined from four operational multiaccess systems, and the model is shown to be robust; that is each of the variables of the model has the same distribution independent of which of the four systems is being examined.  It is shown that the gamma distribution can be used to describe the discrete variables.  Approximations to the gamma distribution by the exponential distribution are discussed for the systems studied.	1970	Fuchs, Jackson	computer communications, time-sharing, operating systems, optimization models
1952	Index by Subject to Algorithms, 1970		1970		
1953	Exponential Integral Ei(x) (Algorithms 385 $S13))		1970	Redish	ANSI Fortran standard
1954	Eigenvalues and Eigenvectors of a Real Symmetric Matrix (Algorithm 384 $F2))		1970	Stewart	real symmetric matrix, eigenvalues, eigenvectors, QR algorithm
1955	Characteristic Values and Associated Solutions of Mathieu's Differential Equation (Algorithm 352 $S22))		1970	Sale	Mathieu's differential equation, Mathieu function, characteristic value, periodic solution, radial solution
1956	Optimum Merging from Mass Storage	An algorithm is displayed which yields the merge orders such that the total read time, defined to be the sum of seek time plus data-transfer time, is minimized for a sort using mass storage. The analysis is parameterized in terms of the ratio of seek time to the time it takes to fill available core with records, and the file size in units of core lengths; and thus it can be applied to any conventional CPU/mass storage combination.  An explicit formula for total read time is derived, in terms of the parameters, which correlates very well with the total read time calculated using the optimum merge orders yielded by the algorithm.  The formula involves the roots of a simple  transcendental equation.  A short table of these roots is included.  Numerical results are graphically displayed for a wide range of the parameters.  It is found that the normalized read time for optimum merging on a given hardware configuration is proportional to the file length times the logarithm of the file length.	1970	Black	sorting, merging, optimum merging, mass storage, sort timing, drum-merging, access time
1957	The List Set Generator: A Construct for Evaluating Set Expressions	The list set generator is defined and algorithms for its use are given.  The list set generator is a construct which may be added to a list processing system or any system that handles sets.  It efficiently generates the set which results from any expression involving sets and set operators.  The efficiency derives from evaluating the expression as a whole and in parallel, rather than evaluating subexpressions and then using those sets to arrive at the final result.	1970	Shapiro	set manipulating, list processing, set generation, sets, lists, file processing
1958	Improving Round-off in Runge-Kutta Computations with Gill's Method	A Runge-Kutta-Gill scheme in common use is based on an incomplete adaptation for floating point operations of Gill's method.  An improved version reduces round-off error significantly.  In this note the heart of the scheme is presented in Fortran language.  It is then shown how an improved version of the method can be obtained with the addition of two Fortran statements.  The two version is a significant improvement.  A numerical example comparing the two is included.	1970	Thompson	Runge-Kutta methods, ordinary differential equations, round-off error, error analysis
1959	An Interrupt Based Organization for Management Information Systems	A programming structure, language constructs, and a supervisory system  organization are proposed for the design and coding of large shared data base systems.  The bases for this organization are a generalized interrupt structure and the newly introduced concept of "file tagging," which is the process of associating program structures and interrupt generating conditions with items in the data base.  An algorithm for resolving conflicts which arise in scheduling the interrupt processing routines is presented.  DPL, a programming language and supervisory system in which these concepts are implemented, is used to illustrated the new organization which is proposed for management information systems.	1970	Morgan	management information systems, integrated data processing, supervisors, interrupts monitoring systems, supervisory systems, interrupt scheduling, parallel processing
1960	Process Management and Resource Sharing in the Multiaccess System ESOPE	The main design principles of the multiaccess system ESOPE are described. Emphasis is placed on basic ideas underlying the design rather than on implementation details.  The main features of the system include the ability given to any user to schedule his own parallel processes using system primitive operations, the file-memory relationship, and the allocation-scheduling policy, which dynamically takes into account recent information about user behavior.	1970	Betourne, Boulenger, Ferrie, Kaiser, Krakowiak, Mossiere	time-sharing, multiprogramming, process scheduling, resource allocation
1961	An Efficient Search Algorithm to Find the Elementary Circuits of a Graph	A theoretically most efficient search algorithm is presented which uses an exhaustive search to find all of the elementary circuits of a graph.  The algorithm can be easily modified to find all of the elementary circuits with a particular attribute such as length.  A rigorous proof of the algorithm is given as well as an example of its application.  Empirical bounds are presented relating the speed of the algorithm to the number of vertices and the number of arcs.  The speed is also related to the number of circuits in the graph to give a relation between speed and complexity. Extensions to undirected and s-graphs are discussed.	1970	Tiernan	algorithm, graph theory, circuit search algorithm, path search algorithm, searching
1962	GROOVE-A Program to Compose, Store, and Edit Functions of Time	A program which makes possible creating, storing, reproducing, and editing functions of time is described.  The functions are typical of those generated by human beings.  Multiple functions (up to 14) are produced for long periods of time (up to several hours) at sufficiently high sampling rates to describe fast human reactions (up to 200 samples per second).  The functions can be used for a variety of purposes such as the control of machine tools or sound synthesizers or anything a person normally controls.  The program operates on a small computer (DDP-224).  Functions are stored on a disk file.  Functions may be created by real-time human inputs to the computer which can interact with already stored functions and computed functions.  Real-time feedback from the process being controlled is an important link in the system.  The environment for effective man-machine interaction has been carefully nurtured.	1970	Mathews, Moore	
1963	Condition Numbers of PEI Matrices		1970	Rokne	matrices, condition numbers, Pei matrices, eigenvectors, eigenvalues
1964	Comment on the Working Set Model for Program Behavior		1970	Bernstein	demand paging, working set, paging rate, multiprogramming
1965	Correction to "Logical" Arithmetic on Computers with Two's Complement Binary Arithmetic		1970	Ehrman	binary arithmetic, unsigned operand arithmetic, maximum significance arithmetic, full-precision arithmetic
1966	A Generalized Method for Generating Argument/Function Values		1970	Wilson	mapping function, decision hierarchy, table look-up
1967	An Improved Algorithm to Produce Complex Primes (Algorithm 401 $A1))		1970	Bratley	number theory, prime numbers, complex numbers
1968	Eigenvalues and Eigenvectors of a Real General Matrix (Algorithm 343 $F1))		1970	Knight, Mersereau	eigenvalues, eigenvectors, latent roots, Householder's method, QR algorithm, inverse iteration
1969	Increasing the Efficiency of Quicksort (Algorithm 402 $M1))		1970	Van Emden	sorting, quicksort
1970	Unrecorded Magnetic Tape for Information Interchange (9 Track-200 and 800 CPI, NRZI and 1600 CPI, PE)* (Proposed American National Standard)		1970		input-output, magnetic tape, information interchange, measurement, instrumentation, phase encoded recording
1971	Recorded Magnetic Tape for Information Interchange (1600 CPI, Phase Encoded)* (Proposed American National Standard)		1970		input-output, magnetic tape, information interchange, measurement, instrumentation, phase encoded recording
1972	A  Nonrecursive List Compacting Algorithm	A simple nonrecursive list structure compacting scheme or garbage  collector suitable for both compact and LISP-like list structures is presented. The algorithm avoids the need for recursion by using the partial structure as  it is built up to keep track of those lists that have been copied.	1970	Cheney	list compacting, garbage collection, compact list, LISP
1973	The Linear Quotient Hash Code	A new method of hash coding is presented and is shown to possess desirable attributes.  Specifically, the algorithm is simple, efficient, and exhaustive, while needing little time per probe and using few probes per lookup.  Performance data and implementation hints are also given.	1970	Bell, Kaman	hashing, hash code, scatter storage, calculated address, search, table, lookup, symbol table, keys
1974	NEATER2: A PL/I Source Statement Reformatter	NEATER2 accepts a PL/I source program and operates on it to produce a reformatted version.  When in the LOGICAL mode, NEATER2 indicates the logical structure of the source program in the indentation pattern of its output.  Logic errors discovered through NEATER2 logical analysis are discovered much more economically than is possible through compilation and trial runs.  A number of options are available to give the user full control over the output format and to maximize the utility of NEATER2 as an aid during the early stages of development of a PL/I source deck.  One option, USAGE, causes NEATER2 to insert into each logical unit of coding a statement which will case the number of times each one is executed to be recorded during execution.  This feature is expected to provide a major aid in optimization of PL/I programs.	1970	Conrow, Smith	logical analysis of PL/I source, reformatting of PL/I source, documentation aid, execution time usage data
1975	A Multiple-Precision Division Algorithm	A generalized division algorithm for use with positive integral operands is  presented.  Depending upon the algebraic relationship of the first two ciphers of the divisor, one or at most two adjustments to the original  divisor and dividend must be performed before the division operation can be  initiated. The uniqueness of this method will cause each trial cipher in the  quotient to be either equal to or one greater than its final replacement.	1970	Mifsud	multiple-precision, division, adjustment, generalize
1976	Multi-attribute Retrieval with Combined Indexes	In this paper a file organization scheme designed to replace the use of the popular secondary index filing scheme (or inverted files on secondary key fields) is described. Through the use of redundancy and storing  keys (or access numbers of the records) that satisfy different combinations of secondary index values in "buckets," it is possible to retrieve all keys satisfying any input query derived from a subset of fields by a single access to an index file, although each bucket may be used for many combinations of values and a combination of buckets may be required for a given query.  The method which, in its degenerate case, becomes the conventional secondary index filing scheme works similarly but has the following advantages: (1) the elimination of multiple accesses in many cases; (2) the elimination of false drops; (3) the elimination of computer time to perform intersection of key sets each qualified for one secondary index field only; and (4) the avoidance of long strings of keys when an index field appearing in a query has very few possible values.  Redundancy, in some  cases, is the same as the secondary indexing method. In the general case,  trade-off between the number of accesses for query and redundancy exists.	1970	Lum	file organization, secondary index files, inverted files, information retrieval, data management, access method, secondary  keys, storage with buckets, rapid retrieval, balanced filing scheme, elimination of false drops, combining indexes, query, multi-attribute retrieval
1977	An Interactive Display for Approximation by Linear Programming	An interactive program with a graphical display has been developed for the approximation of data by means of a linear combination of functions (including splines) selected by the user.  The coefficients of the approximation are determined by linear programming so as to minimize the error in either the L1 or L-infinity norm.  Auxiliary conditions such as monotonicity or convexity of the approximation can also be imposed. This interactive system is described and several examples of its use are given.	1970	LaFata, Rosen	approximation, data fitting, functional approximation, linear programming, interactive graphical display, spline functions
1978	The Use of Interactive Graphics To Solve Numerical Problems	With the advent of on-line (time-sharing) computer systems and graphic terminals, we have available a new dimension in numerical problem solving capabilities.  Rather than simply use the new power to achieve fast turnaround, we can develop interactive routines which are easy to use and also take advantage of the insight and visual capabilities of the human problem solver.  Several on-line systems for general purpose mathematical problem solving have already been implemented as well as some special purpose systems for solving problems in a particular area such as ordinary differential equations.  The advantage of restricting the problem area is that the interface with a user can be greatly simplified. In this paper we discuss some of the advantages accrued by such systems and design considerations for interactive routines.  Furthermore, an implementation of an on-line least squares data-fitting program, PEG, is presented with results obtained from empirical data.  In conclusion, area for future work in this field are discussed.	1970	Smith	interactive graphics, computer graphics, graphics, least squares, data-fitting, interactive computing, on-line mathematics
1979	Numerical Inversion of Laplace Transforms (Algorithm 368 $D5))		1970	Stehfest	Laplace transform inversion, integral transformations, integral equations
1980	An Efficient Algorithm for Sorting with Minimal Storage (Algorithm 347 $M1))		1970	Peto	sorting, ranking, minimal storage sorting, digital computer sorting
1981	Normal Curve Integral (Algorithm 304 $S15))		1970	Holmgren	normal curve integral, probability, special functions
1982	Modified Havie Integration (Algorithm 400 $D1))		1970	Wallick	numerical integration, Havie integration, Romberg quadrature, modified Romberg-quadrature, trapezoid values, rectangle values
1983	Spanning Tree $H) (Algorithm 399)		1970	Seppanen	graph, tree, spanning tree
1984	Tableless Date Conversion $Z) (Algorithm 398)		1970	Stone	date, calendar
1985	An Integer Programming Problem $H) (Algorithm 397)		1970	Chang, Gill	integer programming, change making problem
1986	Student's t-Quantiles $S14) (Algorithm 396)		1970	Hill	Student's t-statistic, quantile, asymptotic approximation
1987	Student's t-Distribution $S14) (Algorithm 395)		1970	Hill	Student's t-statistic, distribution function, approximation, asymptotic expansion
1988	A Formalism for Translator Interactions	A formalism is presented for describing the actions of processors for programming languages-compilers, interpreters, assemblers-and their interactions in complex systems such as compiler-compilers or extendible languages. The formalism here might be used to define and answer such a question as "Can one do bootstrapping using a meta-compiler  whose metaphase is interpretive?"  In addition an algorithm is presented for deciding whether or not a given system can be produced from a given set of component processors.	1970	Earley	translator, compiler, interpreter, bootstrapping, language processor, compiler-compiler
1989	Transition Network Grammars for Natural Language Analysis	The use of augmented transition network grammars for the analysis of natural language sentences is described.  Structure-building actions associated with the arcs of the grammar network allow for the reordering, restructuring, and copying of constituents necessary to produce deep-structure representations of the type normally obtained from a transformational analysis, and conditions on the arcs allow for a powerful selectivity which can rule out meaningless analyses and take advantage of semantic information to guide the parsing.  The advantage of this model for natural language analysis are discussed in detail and illustrated by examples.  An implementation of an experimental parsing system for transition network grammars is briefly  described.	1970	Woods	computational linguistics, grammars, grammar models, linguistics, natural language analysis, parsing, semantic interpretation, transition network grammars, transformational grammars
1990	Numerical Constants (Algorithm)		1970	Dunham	numerical algorithm, numerical constants
1991	On the Number of Automorphisms of a Singly Generated Automaton		1970	Bavel	automata, finite automata, singly generated automata, automorphisms, generators, length of state, minimal-length generators, orbit
1992	Comment on Bell's Quadratic Quotient Method for Hash Code Searching		1970	Lamport	hashing, hash code, scatter storage, calculated address, clustering, search, symbol table, keys, table look-up
1993	Regular Coulomb Wave Functions (Algorithm 292 $S22))		1970	Cody, Paciorek	Coulomb wave functions, wave functions, regular Coulomb wave functions
1994	Decision Table Translation $H) (Algorithm 394)		1970	Dial	Decision table, decision table translation
1995	Special Series Summation with Arbitrary Precision $C6) (Algorithm 393)		1970	Abdali	function evaluation, series summation, approximation
1996	Systems of Hyperbolic PDE $D3) (Algorithm 392)		1970	Smith, McCall	hyperbolic p.d.e., characteristic, extrapolation, second order p.d.e., quasilinear p. d. e.
1997	Increasing the Efficiency of Quicksort	A method is presented for the analysis of various generalizations of quicksort.  The average asymptotic number of comparisons needed is shown  to be an log^2(n).  A formula is derived expressing a in terms of the probability distribution of the "bound" of a partition.  This  formula assumes a particularly simple form for a generalization already considered by Hoare, namely, choice of the bound as median of a random sample. The main contribution of this paper is another generalization of quicksort, which uses a bounding interval instead of a single element as bound.  This generalization turns out to be easy to implement in a computer program.  A numerical approximation shows that a = 1.140 for this version of quicksort compared with 1.386 for the original.  This implies a decrease in number of comparisons of  18 percent; actual tests showed about 15 percent saving in computing time.	1970	van Emden	sorting, quicksort, information content, entropy, distribution of median
1998	Complex Matrix Inversion Versus Real	A comparison of complex matrix with real matrix inversion is made.  It is shown that the complex inversion can be up to  twice as fast as the real inversion.  Further, the rounding error bound for complex inversion is about one-eighth that of real, for Gaussian elimination.  Using extended inner product accumulation the bound is half of the real system.	1970	Ehrlich	complex matrix inversion, matrix inversion, inversion, rounding errors, rounding error bound, error bounds, complex rounding error bounds
1999	Optimal Starting Approximations for Generating Square Root for Slow or No Divide	On machine with slow or no division, it is preferable to use an iterative scheme for the square root different from the classical Heron scheme.  The problem of optimal initial  approximants is considered, and some optimal polynomial initial  approximations are tabulated.	1970	Wilson	square root, Newton-Raphson iteration, optimal approximants
2000	A Variation of the Goodman-Lance Method for the Solution of Two-Point Boundary Value Problems	A recently published method for the interpolative solution of nonlinear equations is improved, and applied to give a significant variation of the Goodman-Lance method for the solution of two-point boundary value problems.  The resulting method applies in particular to the numerical solution of optimal control problems in the Euler-Lagrange formulation. Quantitative estimates are presented which indicate that the variation is nearly twice as fast on some problems in the latter context.	1970	Kimble	Goodman-Lance, boundary-value problems, Newton's method, nonlinear equations, optimal control, optimization, ordinary differential equations, secant method, interpolative solution, orthogonal matrices
2001	Integrating Square Roots	Differential equation of the (y')^2 = f(y) are difficult to integrate  numerically because of the singularity at points where f(y) vanishes.  A  simple trick removes the singularity.	1970	Moler, Solomon	quadrature, differential equations
2002	AMESPLOT-A Higher Level Data Plotting Software System	AMESPLOT is an extensible software system designed to make the display of  data as simple, painless, and neat as possible.  The system described is  hardware-independent and has been implemented on a variety of installations, of different manufacturers, having diverse configurations.  The elements  common to all types of data plots are outlined and the way in which these  elements may be combined into a system based on simple modules is demonstrated. These modules are specified independently and are independent of the axis systems or other attributes of the plot.  This enables plots of any complexity to be constructed by adding or replacing modules.  The basic syntax of AMESPLOT is outlined, and a brief description is given of its current utility software, consisting of "macros" to produce self-scaled plots, formal tablets of text-interspersed with subplots, map coastlines, and 3-D plots.  The system was formulate d in a way such that the user could supply the minimum of information, and it should be fully integrable with user's program written in most conventional higher languages.  The functions of positioning, locating, and scaling (in the layout of multiple subplots) of axes, labels, and all other elements of the plot are handled automatically by the software system unless the user specifies otherwise.  The structuring  of plots from multiple, independent, self-contained subplots is described. Transformation, projection, scaling, rotation, or shifting of entire plots or  subplots by the action of one or more simple modules is possible.  The user  may interact freely with AMESPLOT at three levels, enabling him to construct  his own data markers, alphabetic characters, and transformations, and to produce a variety of artistic and other effects.	1970	Hirschsoln	computer graphics, data potting, data display syntax, hardware independent software, display device independent software, plot elements, self-scaled plots, user interaction, tablet organization, map display, projection
2003	An Interactive Software System for Computers-Aided Design:  An Application to Circuit Project	The characteristics of an interactive software system, intended to constitute  an interface between designer and computer during various steps of the design process, are presented.  The main emphasis is given to the description of the features of the two high level user oriented languages, operating at different levels, on which the interaction is based.  The first one is IMOL, an interactive monitor language, which is designed to perform the overall and control functions of the software system; its design criteria provide the user with commands which are both simple and efficient in order to perform all the functions needed in computer-aided circuit design.  The second one is  COIF, a circuit oriented graphic language, which is designed to describe, generate, and manipulate graphic problem specifications;  it is an  extension of Fortran with graphic-type variables, so that the designer who is familiar with Fortran need not learn a new  language.  The application to computer-aided circuit design is in particular examined; on the other hand, the adopted design criteria provide sufficient generality to extend the use of the two languages  to different computer-assisted applications.	1970	Bracchi, Somalvico	interaction, graphics, computer graphics, computer-aided design, circuit design, software system, software organization, language, monitor language, graphic language, extended Fortran
2004	A Procedure for Generation of Three-dimensional Half-toned Computer Graphics Presentations	A description is given of an algorithm for producing computer generated  half-tone presentations of three-dimensional polygonal surface structures.   This algorithm achieves a significant increase in speed of computation over  the Warnock algorithm developed at the University of Utah and implemented also on the Coordinated Science Laboratory CDC 1604 computer system at the University of Illinois.  The history leading to the algorithm development and then the algorithm itself are described. Results are presented and are compared with computer runs achieved by the Warnock approach.  An extension of the procedure to variable position illumination sources is also given.	1970	Bouknight	half-tone computer graphics, line-scan image processing, hidden surface, polygonal surface structure presentations
2005	Proposed Revision of American National Standard X3.21-1967, "Rectangular Holes in Twelve-Row Punched Cards"*		1970		tabulating-card, Hollerith card, keypunch, information processing
2006	Proposed American National Standard		1970	Kerpelman	State identifiers, State abbreviation, States of the United States, data  elements, data codes, numeric codes, geopolitical subdivisions, geographic codes
2007	Algorithms Policy/Revised August 1970		1970		
2008	Gaussian Quadrature Formulas (Algorithm 331 $D1))		1970	Wise Jr.	quadrature, Gaussian quadrature, numerical integration, weight function, orthogonal polynomials, Newton's method, successive deflation
2009	Simpson's Rule for Multiple Integration (Algorithm 233 $D1))		1970	Proll	numerical integration, multiple integration, Simpson's rule
2010	Unitary Symmetric Polynomials $Z) (Algorithm 391)		1970	McKay	symmetric polynomials, unitary symmetric polynomials
2011	Sequency Ordered Walsh Functions $S22) (Algorithm 390)		1970	Hubner	Walsh functions, sequency ordered Walsh functions
2012	Binary Ordered Walsh Functions $S22) (Algorithm 389)		1970	Hubner	Walsh functions, binary ordered Walsh functions
2013	Rademacher Function $S22) (Algorithm 388)		1970	Hubner	Rademacher function
2014	Function Minimization and Linear Search $E4) (Algorithm 387)		1970	Fielding	function minimization, relative minimum, quasi-Newton method
2015	A Technique for Generating Almost Optimal Floyd-Evans Productions for Precedence Grammars	A technique is developed for generating almost optimal Floyd-Evans productions  given a precedence grammar. A graph formulation is used for the problem of  merging productions.  The productions generated correspond to the minimum cost  inverse-arborescence of that graph.  The validity of the technique is demonstrated for weak precedence grammars defined here, but the productions mechanically generated for any precedence grammar can often be modified in  such a way that correct, almost optimal parsers are obtained.	1970	Ichbiah, Morse	translator writing systems, syntactic analysis, Floyd-Evans productions, precedence grammars, translator optimization, merger algorithm, minimum cost inverse-arborescence, graph theory
2016	The Instrumentation of Multics	An array of measuring tools devised to aid in the implementation of a prototype computer utility is discussed.  These tools include special hardware clocks and data channels, general purpose programmed probing and recording tools, and specialized measurement facilities.  Some particular measurements of interest in a system which combines demand  paging with multiprogramming are described in detail.  Where appropriate,  insight into effectiveness (or lack there of) of individual tools is provided.	1970	Gintell, Saltzer	instrumentation, performance measurement, multiprogramming systems, measuring tools, system analysis, Multics, metering, event tracing, demand paging, script driven measurement
2017	Sorting in a Paging Environment	This sorting study was part of an extensive measurement project undertaken on the M44/44X, an experimental paging system which was conceived and implemented at IBM Research in order to explore the virtual machine concept.  The study was concerned with the implementation  of sorting procedures in the context of the dynamic paging environment characteristic of virtual memory machines.  Descriptions of  the experimental sort programs and analysis of the performance measurement results obtained for them are presented. The insight gained  from the experimental effort is used to arrive at a set of broad guidelines for writing sort programs for a paging environment.	1970	Brawn, Gustavson, Mankin	sorting, merging, virtual machines, paging systems, dynamic storage allocation, measurement of systems program behavior, performance evaluation, memory management
2018	Full Table Quadratic Searching for Scatter Storage	The quadratic residue search method for hash tables avoids much of the clustering experienced with a linear search method.  The simple quadratic search only accesses half the table.  It has been shown that when the length of the table is a prime of the form 4n+3, where n  is an integer, the whole table may be accessed by two quadratic searches plus a separate access for the original entry point. A search  method is presented which is computationally simple, has all the advantages  of the quadratic search, and yet accesses all the table in one sweep.	1970	Day	quadratic residue, search method, hash tables, scatter storage, dictionary look-up, quadratic search, searching, hashing, hash code, clustering, collisions
2019	Normalization Techniques for Hand printed Numerals	Family of pattern standardization techniques based on geometrical projection is applied to a file of digitized hand printed numerals obtained from sales clerks. The principle involves transforming a quadrilateral specified in terms of the convex hull of each pattern into a square. The amount of overlap within each class of characters versus the amount between classes is used to evaluate the degree of normalization achieved with respect to other  published methods including size and shear normalization through moments.	1970	Nagy, Tuong	pattern recognition, character recognition, normalization, projective transformation, central projection, hand printed characters, handwriting, linear transformation, size normalization, mapping, pattern preprocessor
2020	The Allocation of Computer Resources-Is Pricing the Answer?	The widespread use of complex third generation computing systems has led to a  much broader concern about the means by which the resources of these systems are allocated among the user community.  One means that is suggested more and more frequently is a pricing procedure.  In this paper the manner in which one would like to allocate computing resources is considered, and then the extent to which a pricing mechanism fits this mold  is discussed.  Inasmuch as pricing must serve as a rationing mechanism at  times, consideration is given to the means by which prices can be adjusted  flexibly in order to make a dynamic allocation of resources.  Consideration is  also given to the means by which users can be insulated from the harmful  effects of frequent price fluctuations.  Although the subject of pricing has been given a lot of attention recently, a number of misconceptions persist about its purpose and its operation.  An attempt is made to clarify  some of these misunderstandings and to highlight the advantages and  disadvantages and to highlight the advantages and disadvantages of pricing. Two illustrative pricing systems are also discussed in order to demonstrate the  applicability of pricing in quite different environments.	1970	Nielsen	allocation mechanisms, charging, computer pricing, costing, flexible pricing, pricing, priority charges, resource allocation, user motivation
2021	A Comment on Axiomatic Approaches to Programming		1970	Hunt	axiomatic method, proofs of programs, homomorphic structure in programming
2022	Note on an Anomaly in Paging		1970	Pomeranz	paging machines, demand paging, replacement algorithm
2023	A Note on Data Base Deadlocks		1970	Baecker	data base, synchronization, locking, deadlock, reference count
2024	Comments on a Paper by Lowe		1970	Sattley, Millstein	automatic segmentation, program connectivity
2025	Student's t-Distribution; Jacobi Polynomials; Modified Romberg Quadrature; Factorial Analysis of Variance; (Algorithms 332,344,351,359)		1970	Sale	Fortran standards
2026	Exponential Integral (Algorithm 385 $S13))		1970	Ng	
2027	Ricatti-Bessel Functions of First and Second Kind (Algorithm 22 $S17))		1970	Bray	Ricatti-Bessel functions, Bessel functions of fractional order, spherical Bessel functions
2028	Greatest Common Divisor of n Integers and Multipliers $A1) (Algorithm 386)		1970	Bradley	greatest common divisor, Euclidean algorithm, number theory, diophantine equations
2029	Exponential Integral $S13) (Algorithm 385)		1970	Paciorek	exponential integral, special functions, rational Chebyshev approximation
2030	Context-Sensitive Parsing	This paper presents a canonical form for context-sensitive derivations and a parsing algorithm which finds each context-sensitive analysis once and only once.  The amount of memory required by the algorithm is essentially no more than the required to  store a single complete derivation.  In addition, a modified version of the basic algorithm is presented which blocks infinite analyses  for grammars which contain loops.  The algorithm is also compared with several previous parsers for context-sensitive grammars and general rewriting systems, and the difference between the two types of analyses is discussed.  The algorithm appears to be complementary to an algorithm by S. Kuno in several respects, including  the space-time trade-off and the degree of context dependence involved.	1970	Woods	context-sensitive grammars, context-sensitive parsing, formal grammars, formal language theory, parsing, parsing algorithms, recognition algorithms
2031	Algorithm and Bound for the Greatest Common Divisor of n Integers	A new version of the Euclidean algorithm for finding the greatest common divisor of n integers a(i) and multipliers x(i) such that gcd = x(1)a(1) + ... + x(n)a(n) is presented.  The number of arithmetic operations and the number of storage locations are linear in n.  A theorem of Lame that gives a bound  for the number of iterations of the Euclidean algorithm for two integers  is extended to the case of n integers.  An algorithm to construct a minimal  set of multipliers is presented.  A Fortran program for the algorithm appears  as Comm. ACM Algorithm 386.	1970	Bradley	greatest common divisor, Euclidean algorithm, number theory, diophantine equations
2032	File Structures Using Hashing Functions	A general method of file structuring is proposed which uses a hashing function to define tree structure.  Two types of such trees are examined,  and their relation to trees studied in the past is explainedesults for the  probability distributions of path lengths are derived and illustrated.	1970	Coffman Jr., Eve	Data structures, tree structures, file structures, scatter tables, hashing functions, information retrieval
2033	Space/Time Trade-offs in Hash Coding with Allowable Errors	In this paper trade-offs among certain computational factors a given set of messages.  Two new hash-coding methods are examined and compared with a particular conventional hash-coding method. The computational factors considered are the size of the hash area (space), the time required to identify a message as a nonmember of the  given set (reject time), and an allowable error frequency.  The new methods  are intended to reduce the amount of space required to contain the hash-coded  information from that associated with conventional methods.  The reduction in  space is accomplished by exploiting the possibility that a small fraction of  errors of commission may be tolerable in some applications, in particular,  applications in which a large amount of data is involved and a core resident hash area is consequently not feasible using conventional methods.  In such  applications, it is envisaged that overall performance could be improved by using a smaller core resident hash area in conjunction with the new methods and, when necessary, by using some secondary and perhaps time-consuming test to "catch" the small fraction of errors associated with new methods.  An example is discussed which illustrates possible areas of application for the new methods.  Analysis of the paradigm problem demonstrates that allowing a small number of test messages to be falsely identified as members of the given set will permit a much smaller hash area to be used without increasing reject time.	1970	Bloom	hash coding, hash addressing, scatter storage, searching, storage layout, retrieval trade-offs, retrieval efficiency, storage efficiency
2034	The Mobile Programming System: STAGE2	STAGE2 is the second level of a bootstrap sequence which is easily implemented on any computer.  It is a flexible, provided by STAGE2 are summarized, and the implementation techniques which have made it possible to have STAGE2 running on a new machine with less than one man-week of effort are discussed.  The approach has been successful on over 15 machines of widely varying characteristics.	1970	Waite	bootstrapping, macro processing, machine independence, programming languages, implementation techniques
2035	Conversational Access to a 2048-Word Machine	LAP6 is an on-line system running on a 2048-word LINC which provides full  facilities for text editing, automatic filing and file maintenance, and  program preparation and assembly.  It focuses on the preparation and editing  of continuously displayed 23,040-character text strings (manuscripts) which can be positioned anywhere by the user and edited by simply adding and deleting lines as though working directly on an elastic scroll. Other features are available through a uniform command set which itself can  be augmented by the user.  The machine, although small, aids program design by providing display scope and premarked randomly addressable LINC tapes as standard items, in an environment similar to that of a sophisticated terminal.  The tapes are logically  similar to a disk.  Priority was given to the design of efficient tape algorithms to minimize the limitations of the small memory.  Techniques  developed for handling scroll editing, filing, and the layered system  structure are outlined.  LAP6 is used by about 2000 people in 11 countries.   Its design was strongly influenced by performance criteria established in  interviews held with LINC users themselves during the specification period.	1970	Wilkes	conversational computer access, display editing, display oriented system, filing algorithms, LAP6, layering, LINC, man-machine communication, on-line editing, on-line efficiency, on-line environment, scroll editing, small machine system, tape filing, tape  oriented system, text editing
2036	An Interactive Command Generating Facility	A facility to permit conversationally controlled tasks to be executed in a noninteractive environment is proposed. A means by which programs can generate interactive time-sharing commands and receive the corresponding output response is presented.  The commands  will be invoked as if they had been typed at a console keyboard.  It is  argued that this facility will help overcome some of the current limitations  in man-computer communication. A set of functions to accomplish the above  which could be embedded into any string processing language is suggested,  and necessary information pertinent to implementation of the facility on  existing time-sharing systems is given.	1970	Grant	time-sharing, command languages, pseudo-teletype, interaction, conditional job control, operating systems
2037	Permutations of a Set with Repetitions (Algorithm 383 $G6))		1970	Chase	permutations and combinations, permutations
2038	Combinations of M Out of N Objects (Algorithm 382 $G6))		1970	Chase	permutations and combinations, permutations
2039	Permanent Function of a Square Matrix I and II (Algorithm 361 $G6))		1970	Shriver, Eberlein, Dixon	matrix, permanent, determinant
2040	Modified Romberg Quadrature (Algorithm 351 $D1))		1970	Wallick	numerical integration, Romberg quadrature, modified Romberg quadrature, trapezoid values, rectangle values
2041	Shellsort (Algorithm 201 $M1))		1970	Chandler, Harrison	sorting, minimal storage sorting, digital computer sorting
2042	Treesort 3 (Algorithm 245 $M1))	The certification of an algorithm can take the form of a proof that the algorithm is correct.  As an illustrative but practical example, Algorithm 245, TREESORT 3 for sorting an array, is proved correct.	1970	London	proof of algorithms, debugging, certification, metatheory, sorting, in-place sorting
2043	Eigenvalues and Eigenvectors of a Real Symmetric Matrix $F2) (Algorithm 384)		1970	Stewart	real symmetric matrix, eigenvalues, eigenvectors, QR algorithm
2044	Permutations of a Set with Repetitions (Algorithm 383 $G6))		1970	Chase	permutations and combinations, permutations
2045	Combinations of M Out of N Objects (Algorithm 382 $G6))		1970	Chase	permutations and combinations, permutations
2046	A Relational Model of Data for Large Shared Data Banks	Future users of large data banks must be protected from having to know how the  data is organized in the machine (the internal representation).  A prompting service which supplies such information is not a satisfactory solution.  Activities of users at terminals and most application programs should remain unaffected when the internal representation of data is changed and even when some aspects of the external representation  are changed.  Change in data representation will often be needed as a result  of changes in query, update, and report traffic and natural growth in the  types of stored information.  Existing noninferential, formatted data systems  provide users with tree-structured files or slightly more general network  models of the data.  In Section 1, inadequacies of these models are discussed. A model based on n-ary relations, a normal form for data base relations, and the concept of a universal form for data base relations, and the concept of a universal data sublanguage are introduced.  In Section 2, certain operations on relations (other than logical inference) are discussed and applied to the problems of redundancy and consistency in the user's model.	1970	Codd	data bank, data base, data structure, data organization, hierarchies of data, networks of data, relations, derivability, redundancy, consistency, composition, join, retrieval language, predicate calculus, security, data integrity
2047	Incorporating Origin Shifts into the QR Algorithm for Symmetric Tridiagonal Matrices	The QR iteration for the eigenvalues of a symmetric tridiagonal matrix can  be accelerated by incorporating a sequence of origin shifts.  The origin shift may be either subtracted directly from the diagonal elements of the matrix or incorporated by means of an implicit algorithm.  Both methods have drawbacks: the direct method can unnecessarily degrade small  eigenvalues, while the implicit method can effectively loose the shift and  thereby retard the convergence.  This paper presents a new method which has  neither drawback.	1970	Stewart	eigenvalues, eigenvectors, QR method, origin shifts, symmetric tridiagonal matrix
2048	Comparison of Several Adaptive Newton-Cotes Quadrature Routines in Evaluating Definite Integrals with Peaked Integrands	This report compares the performance of five different adaptive quadrature  schemes, based on Newton-Cotes (2N + 1) point rules (N = 1, 2, 3, 4, 5), in approximating the set of definite integrals INTEGRAL$1/(x^2 + p^2)) dx with  relative accuracy e.	1970	Hillstrom	adaptive Newton-Cotes quadrature, quadrature scheme comparison, definite integral evaluation, adaptive numerical in tegration, Newton-Cotes  integration, peaked integrand integration, quadrature efficiently plot, New-Cotes rules applications, Newton-Cotes rules modifications
2049	Accurate Floating-Point Summation	This paper describes an alternate method for summing a set of floating-point numbers.  Comparison of the error bound for this method with that of the standard summation method shows that it is considerably less sensitive to propagation of round-off error.	1970	Linz	summation, floating-point addition, round-off error, round-off propagation
2050	Automatic Parsing for Content Analysis	Although automatic syntactic and semantic analysis is not yet possible for all of an unrestricted natural language text, some applications, of which  content analysis is one, do not have such a stringent coverage requirement. Preliminary studies show that the Harvard Syntactic Analyzer can produce correct and unambiguous identification of the subject and object of certain verbs for approximately half of the relevant occurrences.  This provides a degree of coverage for content analysis variables which  compares favorably to manual methods, in variables which compares favorably  to manual methods, in which only a sample of the total available text is  normally processed.	1970	Damerau	Content analysis, parsing, syntactic analysis, natural language processing, information retrieval, language analysis, text processing
2051	A PL/I Program to Assist the Comparative Linguist	A practical PL/I program is described which can assist comparative linguists to determine the regular sound correspondences between genetically related languages. The investigator must arrange data for input by aligning pairs of suspected cognates.  The program tabulates the correspondences, and uses list processing techniques to sort and count them. Each pair of words is then assigned a relative value that is a function of the total frequency in the data of each correspondence found in that pair of words.  The output is a list of all correspondence types with their frequency of occurrence in the data, and a separate listing of each correspondence with all word-pairs showing that correspondence (unless their relative value is below an arbitrarily  chosen cutoff point).  The article explains the usefulness, as well as the limitations, of the programs, and illustrates its use with a small portion of hypothetical data.	1970	Frantz	comparative linguistics, natural language processing, comparative method, historical linguistics, diachronic linguistics, genetic relationship, sound change, sound correspondence, regular correspondence, list processing
2052	Scheduling to Reduce Conflict in Meetings	Conflicts in scheduling can be treated as defining an undirected linear graph  independently of the relation of the activities in conflict to additional  constraints of time and space.  Each connected component of such a graph, which can be found by an algorithm described by Gotlieb and Corneil,  corresponds to a set of events that must be scheduled at different times.	1970	Grimes	allocation, conflict matrix, connected component, scheduling, spanning tree, undirected linear graph
2053	On the Conversion of Decision Tables to Computer Programs	The use of execution time diagnostics in pinpointing ambiguities in decision tables is discussed.  It is pointed out that any attempt at resolving ambiguities at compile time will, in general, be impossible.  It is shown that, as a consequence, tree methods of converting decision tables to programs are inadequate in regard to ambiguity detection. Two algorithms for programming decision tables whose merits are simplicity of implementation and detection of ambiguities at execution time are presented. The first algorithm is for limited entry decision tables and clarifies the importance of proper coding of the information in the decision table.  The second algorithm programs a mixed entry decision table directly without going through the intermediate step of conversion to a limited entry form, thereby resulting in storage economy.  A comparison of the algorithms and others  proposed in the literature is made.  Some features of a decision table to Fortran IV translator for the IBM 7044 developed by the authors are given.	1970	Muthukrishnan, Rajaraman	decision tables, diagnostic aids, system analysis, business applications
2054	On the Feasibility of Voice Input to an On-line Computer Processing System	An on-line digital computer processing system is considered in which an  ordinary telephone is the complete terminal device, input to the computer  being provided as a sequence of spoken words, and output to the user being  audio responses from the machine.  The feasibility of implementing such a  system with a FORTRAN-like algebraic compiler as the object processor is  considered.  Details of a specific word recognition program are given.  This  technique depends on three simplifying restrictions, namely, a "small" vocabulary set, "known" speakers, and a "moment of silence" between each input word.  Experimental results are presented giving error rates for different experimental conditions as well as the machine  resources required to accommodate several users at a time. The results show  that at this time it is both economically and logically feasible to handle at  least 40 users at a time with an IBM 360/65 computer.	1970	Elder	speech recognition, word recognition, pattern-matching, pattern recognition, time-sharing, remote access, voice input, speech input, telephone input/output, acoustic signal, spoken-word input, talking to  computers, man-machine interaction
2055	Subroutine to Perform In-Situ Transposition of a Rectangular Matrix (Algorithm 380)		1970	Lachenmaier	rectangular matrix, transpose
2056	Gomory (Algorithm 263A $H))		1970	Proll	linear programming, integer variables, dual method
2057	Random Vectors Uniform in Solid Angle (Algorithm 381 $G5))		1970	Knop	random number, random vector, random number generator, probability distribution, frequency distribution, simulation, Monte Carlo
2058	In-Situ Transposition of a Rectangular Matrix (Algorithm 380 $F1))		1970	Laflin, Brebner	rectangular matrix, transpose
2059	A Language for Treating Graphs	A language for the representation of graph is described, and the formulation of  graph operations such as node and/or link deletion or insertion, union,  intersection, comparison, and traversal of graphs is given.  Graphs are represented by linked lists.  The language is syntactically defined as an extension to ALGOL 60, and it is translated into ALGOL by means of a syntax-driven compiler.  Application areas for this language are operation research, network problems, control theory, traffic problems, etc.	1970	Crespi-Reghizzi, Morpurgo	graphs, oriented, nonoriented, multiple, colored graph, language extended ALGOL, operator-precedence, syntax-driven compiler, operation research, network, traffic
2060	GEDANKEN-A Simple Typeless Language Based on the Principle of Completeness and the Reference Concept	GEDANKEN is an experimental programming language with the following  characteristics.  (1) Any value which is permitted in some context of the  language is permissible in any other meaningful context.  In particular, functions and labels are permissible  results of functions and values of variables.  (2) Assignment and indirect addressing are formalized by introducing values, called reference, which in turn possess other values.  The assignment operation always affects the relation between some reference and its value,  (3) All compound data structures are treated as functions.  (4) Type declarations are not permitted.  The functional approach to data structures and the use of references insure that any process which accepts some data structure will accept any logically equivalent structure, regardless of its internal representation.  More generally, any data structure may be implicit; i.e. it may be specified by giving an arbitrary algorithm for computing or accessing its components.  The existence of  label variables permits the construction of coroutines, quasi-parallel processes, and other unorthodox control mechanisms. A variety of programming examples illustrates the generality of the language. Limitations and possible extensions are discussed briefly.	1970	Reynolds	programming language, data structure, reference, assignment, coroutine, quasi-parallel process, typeless language, applicative language, lambda calculus, list processing, nondeterministic algorithm
2061	An Algorithm for the Construction Of Bounded-Context Parsers	An algorithm is described which accepts an arbitrary context-free grammar and constructs a bounded-context parser for it whenever such a parser exists.  In the first part of the paper the definition of a context-free grammar and the working of a bounded-context parser are recalled.  The notion of reduction class for a context-free grammar is then introduced and its connection with the structure of a bounded-context parser is indicated.  Next, pushdown automata which generate the different reduction classes of a context-free grammar are defined.  Finally, the algorithm is described; it essentially carries out an exhaustive study of all possible runs of the pushdown automata generating the reduction classes. In the second part, the utility of the algorithm is discuss ed in the light of the experience gained from its use in compiler design. The algorithm is claimed to be particularly useful in the simultaneous design of a language and a compiler for it.	1970	Loeckx	bounded-context parsing, bounded-context syntactic analysis, parser  construction, syntactical analyzer construction, generators, compiler  compilers, compiler writing systems, translator writing systems metacompilers, context-free grammars, formal languages, pushdown automata
2062	The Application of Sequential Sampling to Simulation: An Example Inventory Model	Four different sequential sampling procedures are applied to the analysis of data generated by a computer simulation experiment with a multi-item inventory model.  For each procedure the cost of computer time required to achieve given levels of statistical precision is calculated.  Also the cost of computer time using comparable fixed sample size methods is calculated.  The computer costs of fixed sample size procedures versus sequential sampling procedures are compared.	1970	Sasser, Burdick, Graham, Naylor	simulation, inventory models, sequential sampling, models, experimental design
2063	Translation Equations (Errata)		1970	Vere	
2064	Operations on Generalized Arrays with the Genie Compiler	Operations on vectors, matrices, and higher dimensional storage arrays are  standard features of most compilers today.  The elements of such structures are usually restricted to be scalars.  For many sophisticated applications this restriction can impose cumbersome data representations. An efficient system has been devised and implemented which allows the elements of multidimensional arrays to themselves be multidimensional arrays.  This system was developed from a storage structure in which the location, length, and content of each array is described by a codeword which can be interpreted by the system.  Code words may describe  arrays containing more codewords, thus providing all needed descriptive information for hyperstructures of any form.	1970	Sitton	multidimensional arrays, matrix operations, storage control, subscripting, compilers
2065	A Programming System for the On-line Analysis of Biomedical Images	A preliminary description of the software for a computer-display system is given with special emphasis on the  man-machine interaction. This  system is intended for a wide variety of biomedical applications. As an example, the methods are applied to the karyotyping of chromosomes.  The  system is separated into four programming tasks: picture transformations, file  maintenance, picture structuring, and display management.  Picture structuring is considered as the vehicle for man-machine communication. A prototype data format for pictures, called a picture-form, is developed. Structure operators are defined which manipulate picture-forms to produce  new pictures-forms.  Many of the ideas are taken from the symbolic mathematical  laboratory at MIT conceived by Marvin Minsky.	1970	Hodes	image processing, biomedical image processing, on-line image processing, semiautomatic image processing, data structure, structure operators, picture processing, biomedical picture processing, on-line picture processing, semiautomatic picture processing, semiautomatic karyotyping, karyotyping, list processing picture processing
2066	An Algol Construction for Procedures as Parameters of Procedures		1970	Knight	Algol, procedures, parameters, side effects
2067	Comment on Lawler's Multilevel Boolean Minimization		1970	DeVries	multilevel logic design, generalized prime implicants, minimal forms, minimization, incompletely specified functions
2068	Comment on Multiprogramming Under a Page on Demand Strategy		1970	Smith	multiprogramming, paging, modeling
2069	Comments on a Paper by Wallace and Mason		1970	Heess Jr.	page-on-demand, demand paging, time-sharing multiprogramming, Markovian computer models, scheduling strategies, operating systems, memory management
2070	A Formal System for Information Retrieval from Files		1970	Hsiao, Harary	
2071	Filon Quadrature (Algorithm 353 $D1))		1970		Fosdick, L. D. Einarsson, Bo
2072	Modified Romberg Quadrature (Algorithm 351 $D1))		1970	Cook	numerical integration, Romberg quadrature, trapezoid values, rectangle values, error bound
2073	Solution of Linear Programs in 0-1 Variables by Implicit Enumeration (Algorithm 341 $H))		1970	Guignard	linear programming, zero-one variables, partial enumeration
2074	Sqank (Algorithm 379 $D1))		1970	Lyness	numerical integration, integration rule, adaptive integration, automatic integration, Simpson's rule, numerical quadrature, quadrature, quadrature rule, adaptive quadrature, automatic quadrature, round-off error control
2075	Discretized Newton-Like Method for Solving a System of Simultaneous Nonlinear Equations (Algorithm 378 $C5))		1970	Pankiewicz	Newton's method, nonlinear equations, interpolating polynomials
2076	Cubic Splines on Uniform Meshes	A very simple procedure is presented for constructing cubic splines, periodic or nonperiodic, on uniform meshes.  Arcs of two cubics suffice to construct a basis of cardinal splines.  An algorithm is given which  requires only minimal storage and computation and permits easy trade-off  of one against the other.	1970	Nilson	simple spline representation, cardinal splines, uniform mesh splines
2077	The Cyclical Majority Problem	The problem of the cyclical majority is presented and some new, simulated results for 3, 4, 5, ..., 40 issues ad 3, 5, 7, ..., 37 judges are reported.	1970	Pomeranz, Weil Jr.	Arrow's paradox, cyclical majority, simulation, voter's paradox, voting paradox
2078	Representations for Space Planning	Problems involving the arrangement of objects in two-  or three-space where the objective function primarily consists of derivatives of the distance between objects or their arrangement are called space planning problems.  The representational requirements for this problem area are defined and compared with current computer graphic languages.  Four alternative data structures that allow automated space planning are described and compared.	1970	Eastman	automated design, data structures, computer graphics, computer-aided design, engineering design, architectural design, robots
2079	On Multiprogramming, Machine Coding, and Computer Organization		1970	Wirth	
2080	The Nucleus of a Multiprogramming System	This paper describes the philosophy and structure of a multiprogramming system  that can be extended with a hierarchy of operating systems to suit diverse  requirements of program scheduling and resource allocation.  The system nucleus simulates an environment in which program execution and input/output are handled uniformly as parallel, cooperating process es.  A fundamental set of primitives allows the dynamic creation and control of a hierarchy of processes as well as the communication among them.	1970	Hansen	multiprogramming, operating systems, parallel processes, process concept, process communication, message buffering, process hierarchy, process creation, process removal
2081	Some Complete Calculi for Matrices	A matrix calculus is introduced with the intention of developing data structures suitable for a high level algorithmic language for mathematical programming.   The paper investigates how the special structure of matrices can be described and utilized for efficient computing by saving memory space and superfluous operations.  Sequences of Matrices (and sequences of sequences of matrices) are considered, and matrix operators areext ended to sequence operators and cumulative operators.  Algorithms are given which use symbol manipulation of matrix expressions so as to find the forms best suited for computation.  These forms are called normal forms.  Several completeness results are obtained in the sense that for each expression an equivalent expression in normal form can be found within a specified calculus.	1970	Bayer, Witzgall	complete calculus, data structures, linear programming, matrix, matrix concatenation, matrix sequences, programming languages, sequence operations, symbol manipulation
2082	Syntax-Directed Documentation For PL 360	The language PL 360, together with its phrase structure grammar, is used as a  concrete basis for illustrating an idea called syntax-directed documentation.  This idea is (1) to use the phrase structure of a program to define the  structure of a formal documentation for that program; (2) to use the syntactic  types and identifiers in the resulting structure to trigger the automatic formation of questions to the programmer, whose answers will become part of that documentation; and (3) to provide automatic storage and retrieval facilities so that other programmers who want to understand or modify the program can access the resulting documentation, which is cross-indexed in various ways by syntactic types and objects.  A small PL 360 program, already found in the literature, is worked out as an example.	1970	Mills	documentation, syntax analysis, PL 360, enforced documentation, indexed documentation, automatic interrogation, phase structured grammar, syntax-directed documentation, syntax processing
2083	Creation and Control of Internal Data Bases Under a Fortran Programming Environment	A method is described for the definition of a user's COMMON structure and the automatic generation of the necessary COMMON, DIMENSION, EQUIVALENCE, and type declarations for each of the user's routines.  The definition for the COMMON is contained in an easy to modify form, thus allowing the control of general communications of data between routines. The described system has been implemented on the IBM 7094, CDC 6000 series, and the IBM 360.  The method has proved to be invaluable for the definition and control of COMMON in many large-scale programs.	1970	DeSalvio, Purdy, Rau	data base, Fortran, common, common equivalencing, subroutine communication, data communication
2084	A Note on the Complement of Inherently Ambiguous Context-Free Languages		1970	Maurer	ambiguity, inherent ambiguity, complement, context-free language, Chomsky-language, phrase structure language, production system, type 2 language, bounded language
2085	Comment on a Paging Anomaly		1970	Dempster	paging machines, demand paging, replacement algorithm
2086	Another Method of Converting from Hexadecimal to Decimal		1970	Kailas	binary-decimal conversion, computer arithmetic categories
2087	A Number System for the Permutations		1970	Pager	permutation, ordering, number, number system, p-number, combinatorial
2088	Netflow (ALgorithm 336 $H))		1970	Bray, Witzgall	capacitated network, linear programming, minimum-cost flow, network flow, out-of-kilter
2089	Prime Number (Algorithm 310 $A1))		1970	Rapp, Scott	prime numbers, generator
2090	Symbolic Expansion of Algebraic Expressions (Algorithm 377 $R2))		1970	Levine	algebra, symbolic algebra, symbolic multiplication, algebraic distribution, algebraic multiplication, distribution algorithm, multiplication algorithm, product  algorithm, polynomial distribution, polynomial expansion
2091	PDEL-A Language for Partial Differential Equations	Conventional computer methods available to solve continuous system problems characterized by partial differential equations are very time-consuming and cumbersome.  A convenient, easy to learn and to use, high level problem oriented language to solve and study partial differential equation problems has been designed; a practical translator for the language has also been designed, and a working version of it has been constructed for a significant portion of the language.  This Partial Differential Equation Language, PDEL, is outlined, and the highlights of the translator are briefly summarized.	1970	Cardenas, Karplus	problem oriented or digital simulation language, partial differential equations, translator, PL/1, preprocessor PL/1, finite difference algorithms
2092	A Deductive Question-Answer for Natural Language Inference	The question-answering aspects of the Protosynthex III pro totype language processing system are described and exemplified in detail.  The system is written in LISP 1.5 and operates on the Q-32 time-sharing system.  The system's data structures and their semantic organization, the deductive question-answering formalism of relational properties and complex-relation-forming operators, and the question-answering procedures which employ these features in their operation are all described and illustrated.  Examples of the system's performance and of the limitations of its question-answering  capability are presented and discussed.  It is shown that the use of semantic information in deductive question answering greatly  facilitates the process, and that a top-down procedure which works from question to answer enables effective use to be made of this information.  It is concluded that the development of Protosynthex III into a practically useful system to work with large data bases is possible but will require changes in both the data structures and the algorithms used for question answering.	1970	Schwarcz, Burger, Simmons	question answering, natural language, Protosynthex III, LISP, semantics, artificial intelligence, computational linguistics, language processing, fact retrieval
2093	A Comparison of Error Improvement Estimates for Adaptive Trapezoid Integration	Various simple choices of error improvement estimates for the trapezoid rule are studied to demonstrate a comparison procedure which is relatively independent of the profusion of adaptive search and stopping strategies.  Comparisons are based on x^r, `; the inclusion of the noninteger powers makes this more realistic than the usual polynomial based comparison.  Behavior near the singularity was found to be the dominant factor, and a new estimate, based on a constant curvature assumption and parametric differences, was considered slightly better than the other choices considered.	1970	Schweikert	adaptive integration, error improvement estimate, trapezoid rule, nonpolynomial error criteria
2094	On an Algorithm for Nonlinear Minimax Approximation	Certain nonlinear minimax approximation problems are characterize d by properties which permit the application of special algorithms, mainly based on the exchange algorithms of Remes (1934, 1935), for their solution.  In this paper the application to problems of this type of a general nonlinear algorithm due to Osborne and Watson (1969) is considered.  Examples are given to illustrate that this algorithm can give satisfactory results and, in particular, can successfully solve problems which lead to difficulties with the more conventional specialist method.	1970	Watson	minimax approximation, nonlinear approximation, linear programming
2095	Measurements of Segment Size	Distributions of segment sizes measured under routine operating con ditions on a computer system which utilizes variable sized segments (the Burroughs B5500) are discussed.  The most striking feature of the measurements is the large number of small segments-about 60 percent of the segments in use contain less than 40 words.  Although the results are certainly not installation independent, and although they  are particularly influenced by features of the B5500 ALGOL system, they  should be relevant to the design of new computer systems, especially with respect to the organization of paging schemes.	1970	Batson, Ju, Wood	storage allocation, segmentation, segment sizes, page sizes, paging, resource allocation, memory allocation, core utilization
2096	Experiments with the M & N Tree-Searching Program	The M & N procedure is an improvement to the mini-max backing-up procedure widely used in computer program for game-playing and other purposes.  It is based on the principle that it is desirable to have many options when making decisions in the face of uncertainty.  The mini-max procedure assigns to a MAX (MIN) node the value of the highest (lowest) valued successor to that node.  The M & N procedure assigns to a MAX (MIN) node some function of the M (N) highest (lowest) valued successors.  An M & N procedure was written in LISP to play the game of kalah, and it was demonstrated that  the M & N procedure is significantly superior to the mini-max procedure.  The statistical significance of important conclusions is given.  Since information on statistical significance has often been lacking in papers  on computer experiments in the artificial intelligence field, these experiments  can perhaps serve as a model for future work.	1970	Slagle, Dixon	artificial intelligence, heuristic program, tree searching, LISP, kalah, game playing, decision theory, mini-max backing-up procedure, backing-up procedures
2097	A Program to Teach Programming	The TEACH system was developed at MIT to ease the cost and improve the results  of elementary instruction in programming.  To the student, TEACH offers loosely  guided experience with a  conversational language which was designed with teaching in mind.  Faculty involvement is minimal.  A term of experience with TEACH is discussed.  Pedagogically, the system appears to be successful;  straightforward reimplementation will make it economically successful as well.  Similar programs of profound tutorial skill will appear only as the results of extended research.  The outlines of his research are beginning to become clear.	1970	Fenichel, Weizenbaum, Yochelson	elementary programming, computer-assisted learning, UNCL, TEACH
2098	t-Test Probabilities (Algorithm 321); Student's t-Distribution (Algorithm 344)		1970	Hill, Loughhead	t-test, Student's t-statistic, distribution function, approximation
2099	Eigenvalues and Eigen vectors of a Real General Matrix (Algorithm 343 $F))		1970	Knoble	norm, characteristic equation, degenerate eigen-system, diagonalizable matrix, defective matrix
2100	Ortho (Algorithm 127 $F5))		1970	Barrodale	orthogonalization, approximation
2101	Least Squares Fit By f(x) = Acos(Bx+C) (Algorithm 376 $E2))		1970	Spath	nonlinear least squares fit
2102	Fitting Data To One Exponential (Algorithm 375 $E2))		1970	Spath	nonlinear least squares fit
2103	Restricted Partition Generator (Algorithm 374 $A1))		1970	White	partitions, restricted partitions, sums of integers, restricted sums
2104	Number of Doubly Restricted Partitions (Algorithm 373 $A1))		1970	White	partitions, restricted partitions, sums of integers, restricted sums
2105	An Interactive Computer System Using Graphical Flowchart Input	An interactive computer system operational on a graphical computer terminal is  described.  This system was designed to demonstrate a method of programming by  computer interpretation of a flowchart.  The user draws a description of a sampled-data system and specifies description is transmitted to a large scale computer.  The design is simulated, and a graphic representation of the  processed signal is returned to the scope.  A successful design may require  numerous modifications of the original design.  A graphical interactive system  provides an environment to perform this iterative process efficiently and  effectively.	1970	Robins, Beyer	simulation program, graphical input-output sampled data systems
2106	Computer Education in a Graduate School of Management	Several years of experience have led to the belief that the creative design and evaluation of management information systems requires a thorough understanding of the related computer technology.  Concepts such as paging and priority interrupt systems can best be explained at the  machine language level.  Any machine used for exposition should fulfill several criteria.  It should: (1) raise as few spurious issues as possible; (2) allow, without undue effort, the solution of interesting problems; (3) be capable of exposing all outstanding issues of significance, capable of exposing all outstanding issues of significance, within the chosen machine; (4) be seful for pursuing issues in great depth when appropriate; (5) not be committed to the equipment provided by any manufacturer; (6) be able to  provide the student with diagnostic aids to a great depth; (7) allow the student ready access to the machine; (8) be capable of extension to expose new issues as they come along.  We have constructed a simulated machine and its associated software which meets these criteria.  This system, called the PRISM system, is documented by a primer and a reference manual.	1970	Ness, Green, Martin, Moulton	education, simulation, machine language, management information systems, interpreters
2107	The Quadratic Quotient Method: A Hash Code Eliminating Secondary Clustering	Secondary clustering as a cause of hash code inefficiency is discussed, and a  new hashing method based on its elimination is presented.  Comparisons with  previous methods are made both analytically and empirically.	1970	Bell	hashing, hash code, scatter storage, calculated address, clustering, search, symbol table, collisions, keys, table look-up
2108	A Variation on Sorting by Address Calculation	The principles of address calculation and merging are combined to yield an efficient sorting technique. Detailed flowcharts of the most important program steps are included. The characteristics of the proposed sort are discussed.	1970	Jones	sorting, address calculation, merging, order, sequence creation
2109	The Use of Quadratic Residue Research	A quadratic residue search method has previously been suggested to avoid the clustering usually encountered when hash address collisions occur and linear search methods are used.  The search size, because of the property of quadratic residues, is limited to one half of the storage table.  It is shown that for some classes of prime numbers the complement of the set of quadratic residues can easily be determined and  hence the entire table of size p, where p is that prime number, can be searched.	1970	Radke	quadratic residue, search method, hash addressing, address clustering, scatter storage, file searching, file addressing, hash coding, quadratic search, random search, storage layout, searching
2110	An Efficient Context-free Parsing Algorithm	A parsing algorithm which seems to be the most efficient general context-free  algorithm known is described.  It is similar to both Knuth's LR(k) algorithm  and the familiar top-down algorithm.  It has a time bound proportional to  n^3 (where n is the length of the string being parsed) in general; it has a  n^2 bound for unambiguous grammars; and it runs in linear time on a large  class of grammars, which seems to include most practical context-free programming language grammars.  In an empirical comparison it appears to be superior to the top-down and bottom-up algorithms studied by Griffiths  and Petrick.	1970	Earley	syntax analysis, parsing, context-free grammar, compilers, computational complexity
2111	Spelling Correction in Systems Programs	Several specialized techniques are shown for efficiently incorporating spelling correction algorithms in to compilers and operating systems.  These include the use of syntax and semantics information, the organization of restricted keyword and symbol tables, and the consideration of a limited class of spelling errors.  Sample 360 coding for performing spelling correction is presented.  By using systems which perform spelling correction, the number of debugging runs per program has been decreased, saving both programmer and machine time.	1970	Morgan	spelling correction, error correction, debugging, compilers, operating systems, diagnostics, error detection, misspelling, lexical analysis systems programming
2112	Translation Equations	Input limited transduction expressions, or translation equations, are used to describe the syntax and left-context sensitive semantics for  context-free languages.  A formal procedure is given for deriving from a set of translation equations the specifications for a pushdown translator. The translator consists of Mealy form finite-state automata interacting by means of a pushdown stack.  Within the framework described string  recognition and parsing may be treated as special cases of the translation  problem.	1970	Vere	automata, Turing machines, regular expression, transduction expression, context-free languages, translation, recognizers, parsing, meta-compilers, pushdown transducer, syntax directed compilers, finite state automata
2113	The Multistore Parser for Hierarchical Syntactic Structures	A syntactic parser is described for hierarchical concatenation patterns that are presented to the analyzer in the form of linear strings.  Particular emphasis is given to the system of "significant addresses" by means of which processing times for large-scale matching procedures can be substantially reduced.  The description makes frequent use of examples taken from the fully operational implementation of the parser in an experimental English sentence analyzer.  By structuring an area of the computer's central core storage in such a way that the individual locations of bytes and bits come to represent the data involved in the matching procedure, the shifting of information is reduced to a minimum, and the searching of lists is eliminated altogether.  The matches are traced by means of binary masks and the state of single bits determines the operational flow of the procedure.  The method could be implemented with any interpretive grammar, provided it can be expressed by the functional classification of the items composing the input hierarchical structures.	1970	von Glasersfeld	parsing, syntactic analysis, natural-language analysis, linguistic data processing, computational linguistics, correlational grammar, structure recognition, pattern recognition, matching procedures, tree-structure  interpretation, machine translation, automatic abstracting
2114	A Formal System for Information Retrieval from Files	A generalized file structure is provided by which the concepts of keyword, index, record, file, directory, file structure, directory decoding, and record retrieval are defined and from which some of the frequently used file structures such as inverted files, index-sequential files, and multilist files are derived.  Two algorithms which retrieve records from the generalized file  structure are presented.	1970	Hsiao	attribute-value pair, index, keyword, record, record address, K-pointer, K-list, file, directory, generalized file structure, inverted file, index-sequential-file, multilist file, description, file search, directory search, serial processing of lists, prime keyword, parallel processing of lists
2115	Fortran Tausworthe Pseudorandom Number Generator		1970	Payne	random numbers, pseudorandom numbers, shift register sequences
2116	Interchange Rolls of Perforated Tape for Information Interchange* (Proposed American National Standard)		1970		interchange, rolls, perforated tape, tape, information interchange, directional markers, leaders, trailers, roll-up tape, 9-track paper tape, dimensions
2117	Representation for Calen dar Date for Machine-to-Machine Data Interchange* (Proposed American National Standard)		1970		calendar date, machine-to-machine interchange, month, year, day, representation coded
2118	An Efficient Algorithm for Sorting with Minimal Storage (Algorithm 347 $M1))		1970	Griffin, Redish	sorting, minimal storage sorting, digital computer sorting
2119	Derivatives (Algorithm 282 $S22))		1970	Gautschi, Klein	recursive computation, successive derivatives, error control
2120	An Algorithm to Produce Complex Primes, Csieve (Algorithm 372 $A1))		1970	Dunham	primes, complex numbers
2121	Partitions in Natural Order (Algorithm 371 $A1))		1970	McKay	partitions, number theory
2122	General Random Number Generator (Algorithm 370 $G5))		1970	Butler	random number generator, probability density function, transformation, cumulative density function
2123	Generator of Random Numbers Satisfying the Poisson Distribution (Algorithm 369 $G5))		1970	Schaffer	Poisson distribution, random number generator
2124	Numerical Inversion of Laplace Transforms (Algorithm 368 $D5))		1970	Stehfest	Laplace transform inversion, integral transformations, integral equations
2125	A Note on Minimal Length Polygonal Approximation to a Digitized Contour	A method for extracting a smooth polygonal contour from a digitized image is illustrated. The ordered sequence of contour points and the connection graph of the image are first obtained by a modified Ledley algorithm in one image scan.  A minimal perimeter polygon subjected to specified constraints is then chosen as the approximating contour.  The determination of the minimal  polygon can be reduced to a nonlinear programming problem, solved by an algorithm which takes into account the weak bonds between variables.  Some examples are presented, and the corresponding computing times are listed.	1970	Montanari	digitized image, connection tree, minimal polygon, optimal approximation, nonlinear programming
2126	Experience with an Extensible Language	An operational extensible language system is described. The system and its base language are appraised with  respect to efficiency, flexibility, and utility for different categories of users.	1970	Irons	programming languages, extensible, compiler, bootstrapping, ambiguity
2127	Natural Language Question-Answering Systems: 1969	Recent experiments in programming natural language question-answering systems are reviewed to summarize the methods that have been developed for syntactic, semantic, and logical analysis of English strings.  It is concluded that at least minimally effective techniques have been devised for answering questions from natural language subsets in small scale experimental systems and that a useful paradigm has evolved to guide research efforts in the field.  Current approaches to semantic analysis and logical inference are seen to be effective beginnings but of questionable generality with respect either to subtle aspects of  meaning or to applications over large subsets of English.  Generalizing from current small-scale experiments to language-processing  systems based on dictionaries with thousands of entries-with correspondingly large grammars and semantic systems-may entail a new order of complexity and require the invention and development of entirely different approaches to semantic analysis and questions answering.	1970	Simmons	question-answering, natural language, artificial intelligence, language processing, fact retrieval, semantics
2128	A Processor Allocation Method for Time-Sharing	A scheduling algorithm is proposed which is intended to minimize changes of  tasks on processors and thereby reduce over-head.  The algorithm also has application to more general resource allocation problems.  It is implemented  by means of a method for efficiently handling dynamically changing segmented  lists.	1970	Mullery, Driscoll	time sharing, resource allocation, scheduling algorithms, monitors, dynamic allocation, processor allocation, multiprogramming, multiprocessing, time slicing, scheduling, conversational systems, interactive systems
2129	Recursive Computation of Certain Derivatives-A Study of Error Propagation	A brief study is made of the propagation of errors in linear first-order difference equations.  The recursive computation of successive derivatives of (e^x)/x and (cos x)/x is considered as an illustration.	1970	Gautschi	recursive computation, successive derivatives, error propagation
2130	Automatic Segmentation of Cyclic Program Structures Based on Connectivity and Processor Timing	Time-shared, multiprogrammed, and overlayed batch systems frequently require segmentation of computer programs into discrete portions.   These program portions are transferred between executable and peripheral storage whenever necessary; segmentation of program s in a manner that  reduces the frequency of such transfers is the subject of this paper.  Segmentation techniques proposed by C. V. Ramamoorthy are subject to limitations that arise when the preferred segment size is not compatible with the physical restrictions imposed by the available computing equipment.  A generalization of Ramamoorthy's suggestions is made in order to allow their application  when circumstances are other than ideal.	1970	Lowe	automatic segmentation, cyclic program structures, loops, paging, multiprogramming, loaders, assemblers, compilers, time-sharing, program  connectivity
2131	Rapid Computation of Weights of Interpolatory Quadrature Rules [D1] (Algorithm 417)		1971	Gustafson	divided differences
2132	Rapid Computation of Coefficients of Interpolation Formulas [E1] (Algorithm 416)		1971	Gustafson	divided differences, Newton's interpolation formula
2133	Algorithm for the Assignment Problem (Rectangular Matrices) [H] (Algorithm 415)		1971	Bourgeois, Lassalle	operations research, optimization theory, assignment problem, rectangular matrices
2134	An Extension of the Munkres Algorithm for the Assignment Problem to Rectangular Matrices	The assignment problem, together with Munkres proposed algorithm for its solution in square  matrices, is presented first.  Then the authors develop an extension of this algorithm which permits  a solution for rectangular matrices.  Timing results obtained by using an adapted version of Silver's  Algol procedure are discussed, and a relation between solution time and problem size is given.	1971	Bourgeois, Lassalle	operations research, optimization theory, assignment problem, rectangular matrices, algorithm
2135	Rapid Computation of General Interpolation Formulas and Mechanical Quadrature Rules	Let f have n continuous on a closed interval [a,b] and let L be a linear functional.  The attempt  is made to approximate L (f) with L (Q) where Q is a polynomial, approximating f.  Algorithms are developed  for rapid computation of L (Q) for a wide class of selections of Q which includes the Lagrangian and  Hermitian rules as special cases.	1971	Gustafson	linear functionals, divided differences, Newton's interpolation formula
2136	A Note on "A Modification of Nordsieck's Method Using an 'Off-Step' Point"		1971	Blumberg, Foulk	ordinary differential equations, multi-step methods, predictor, corrector, round-off error, Nordsieck's  method, Gragg-Stetter modification
2137	New LISP Techniques for a Paging Environment	The system described herein employs the block concept, and that of global and local variables,  in addition to the methods applied in most LISP systems.  Also, a new means of list representation is  used: "local sequential" for lists created during compilation, and "block level sequential" for those  created dynamically.  A new garbage collection algorithm has been introduced to make lists as compact  as possible; partial garbage collection is performed after each block exit instead of total garbage collection  when storage is exhausted.  The algorithm does not use the customary flagging procedure.  This combination  of features has eliminated the need for a free list, and effectively minimizes the number of pages used  at any moment.	1971	Rochfeld	LISP, list processing, paging, virtual memory, garbage collection, core fragmentation, compact  list structures, block, segment
2138	BLISS: A Language for Systems Programming	A language, BLISS, is described.  This language is designed so as to be especially suitable  for use in writing production software systems for a specific machine (the PDP-10): compilers, operating  systems, etc.  Prime design goals of the design are the ability to produce highly efficient object code,  to allow access to all relevant hardware features of the host machine, and to provide a rational means  by which to cope with the evolutionary nature of systems programs.  A major feature which contributes  to the realization of these goals is a mechanism permitting the definition of the representation of all  data structures in terms of the access algorithm for elements of the structure.	1971	Wulf, Russell	programming languages, implementation language, systems programming, data structures
2139	Implementation of the Substring Test by Hashing	A technique is described for implementing the test which determines if one string is a substring  of another.  When there is low probability that the test will be satisfied, it is shown how the operation  can be speeded up considerably if it is preceded by a test on appropriately chosen hash codes of the  strings.	1971	Harrison	substring, hashing, subset, signature, information compression, information retrieval, searching
2140	Retrieval-Update Speed Tradeoffs Using Combined Indices	In a paper in the November 1970 Communications of the ACM, V. Y. Lum introduced a technique  of file indexing named combined indices.  This technique permitted decreased retrieval time at the cost  of increased storage space.  This paper examines combined indices under conditions of file usage with  different fractions of retrieval and update.  Tradeoff curves are developed to show minimal cost of file  usage by grouping various partially combined indices.	1971	Mullin	file organization, combined index files, inverted files, information retrieval, query, multi-attribute  retrieval, file update
2141	Algorithmic Selection of the Best Method for Compressing Map Data Strings	The best of a dozen different methods for compressing map data is illustrated.  The choices  are generated by encoding data strings-sequence of like codes-by three methods and in four directions.   Relationships are developed between compression alternatives to avoid comparing all of them.  The technique  has been used to compress data from forest resource maps, but is widely applicable to map and photographic  data reduction.	1971	Amidon, Akin	data compression, map storage, information retrieval, input/output, run coding, data reduction
2142	Reconstruction of Pictures from Their Projections	There are situations in the natural sciences and medicine (e.g. in electron microscopy and  X-ray photography) in which it is desirable to estimate the gray levels of a digital picture at the individual  points from the sums of the gray levels along straight lines (projections) at a few angles.  Usually,  in such situations, the picture is far from determined and the problem is to find the "most representative"  picture.  Three algorithms are described (all using Monte Carlo methods) which were designed to solve  this problem.  The algorithms are applicable in a large and varied number of fields.  The most important  uses may be the reconstruction of possibly asymmetric particles from electron micrographs and three-dimensional  X-ray analysis.	1971	Gordon, Herman	approximation, biomedical image processing, efficient encoding, image processing, linear programming, mathematical programming, Monte Carlo techniques, optimization, picture compression, picture description, picture processing, stereology, X-ray analysis
2143	Chebyshev Approximation of Continuous Functions by a Chebyshev System of Functions [E2] (Algorithm 414)		1971	Bolub, Smith	approximation, Chebyshev approximation, Remex algorithm
2144	On Accurate Floating-Point Summation	The accumulation of floating-point sums is considered on a computer which performs t-digit  base B floating-point addition with exponents in the range -m to M.  An algorithm is given for accurately  summing N t-digit floating-point numbers.  Each of these N numbers is split into q parts, forming qN  t-digit floating-point numbers.  Each of these is then added to the appropriate one of n auxiliary t-digit  accumulators.  Finally, the accumulators are added together to yield the computed sum.  In all, qN+n-1  t-digit floating-point additions are performed.  Under usual conditions, the relative error in the computed  sum is at most [(t+1)/v]B^(1-t) for some v.  Further, with an additional q+n-1 t-digit additions, the  computed sum can be corrected to full t-digit accuracy.  For example, for the IBM/360 (B=16, t=14, M=63,  m=64), typical values for q and n are q=2 and n=32.  In this case, (*) becomes N <= 32,768, and we have  [(t+1)/v]B^(1-t) = 4x16^-13.	1971	Malcolm	floating-point summation, error analysis
2145	Automation of Etching-Pattern Layout	HELP (Heuristic Etching-Pattern Layout Program) is an application program developed to computerize  the tedious and error-prone although vitally important wiring design of printed circuit boards.  HELP  helps automate a design stage one step closer to production than logical design.  It can be used to design  wiring patterns of two-layer circuit boards on which ICs in dual-in-line packages as well as discrete  components such as transistors and resistors have been placed.  HELP employs two methods of wiring.   One is the heuristic method, which simulates human approaches to wiring design, and the other is the  theoretically interesting but time-consuming method of maze-running, based on the Lee's algorithm.  HELP  performs more than 90 percent of required wiring by the heuristic path with respect to a performance  function for each point-to-point, and point-to-line connection.  It can bring the number of successful  wiring connections very close to 100 percent.	1971	Aramaki, Kawabata, Kazuhiko	heuristic etching-pattern layout, wiring design of printed circuit board, maze-running, Lee's algorithm
2146	Optimizing the Polyphase Sort	Various dispersion algorithms for the polyphase sorting procedure are examinedhe optimum  algorithm based on minimizing the total number of unit strings read is displayed.  The logic of this  algorithm is rather complicated; hence, several other new dispersion algorithms with more straightforward  logic are presented.  Of the simple dispersion algorithms discussed, the  Horizontal is best.  It does  approximately one-fourth to one and one-half percent less reading and writing than most algorithms in  use today.  An additional two and one-fourth to three percent improvement can be achieved by utilizing  the Modified Optimum Algorithm.  This algorithm is relatively straightforward, but it requires a fairly  close estimate of the total number of unit strings before the dispersion begins.	1971	Shell	sorting, polyphase sorting, dispersion algorithms, optimum dispersion algorithm, repetition operator
2147	Using Computers in Higher Education: Past Recommendations, Status, and Needs	Data from a survey conducted with National Science foundation support, which was published  in December 1970, is reviewed, and it is pointed out that, with regard to computers in higher education,  national goals stated in the Rosser and Pierce Reports have not been attained.  Quality was lacking in  hardware or courses in nearly half of the associate and bachelor's degree programs in data processing,  computer science, etc., offered in 1966-67.  A plea is made for continuing studies on status and goals  for computing in higher education, improvement of degree programs, and a national testing laboratory  for educational technology.	1971	Hamblen	higher education, computers, degree programs, national goals, testing laboratory, educational technology
2148	The Composition of Semantics in Algol 68	The main features of Algol 68 are explained from a semantic point of view.  It is shown how  the language permits the composition of values and actions, i.e. ultimately programs, from a minimum  set of primitives with a few fundamental recursive rules of composition.  The associated syntax is briefly  reviewed.  An attempt has been made to obtain a structured and simple introduction to both Algol 68 and  its orthogonal design.	1971	Branquart, Lewi, Sintzoff, Wodon	programming primitives, programming languages, Algol, semantics, recursive composition, design  of programming languages, data structures
2149	ENTCAF and ENTCRE: Evaluation of Normalized Taylor Coefficients of an Analytic Function [C5]  (Algorithm 413)		1971	Lyness	Taylor coefficients, Taylor series, Cauchy integral, numerical integration, numerical differentiation, interpolation, complex variable, complex arithmetic, fast Fourier transform
2150	Concurrent Control with "Readers" and "Writers"	The problem of the mutual exclusion of several independent processes from simultaneous access  to a "critical section" is discussed for the case where there are two distinct classes of processes known  as "readers" and "writers."  The "readers" may share the section with each other, but the "writers" must  have exclusive access.  Two solutions are presented: one of the case where we wish minimum delay for  the readers; the other for the case where we wish writing to take place as early as possible.	1971	Courois, Heymans, Parnas	mutual exclusion, critical section, shared access to resources
2151	User Program Measurement in a Time-Shared Environment	A general discussion of the measurement of software systems is followed by a description of  a hardware and software scheme for measuring user programs in a time-shared environment.  The TX-2 computer  at MIT Lincoln Laboratory was used for the implementation of such a system and the characteristics of  this implementation are reported.  A scenario showing the system in use is presented.  Finally, it is  shown how other time-sharing systems may provide similar measuring facilities.	1971	Nemeth, Rovner	operating systems, multiprogramming systems, time-sharing systems, software measurement, user program  measurement, measurement technology, TX-2 computer, virtual computers, performance improvement
2152	Display Procedures	Although the use of structured display files is widespread in interactive computer graphics,  these structures present a number of problems which tend to restrict their generality and usefulness.   This paper discusses some of these problems, and suggests an alternative approach to display system  design which avoids the use of structured display files.  This technique employs display procedures to  generate information for display.  By including transformations within calls to these procedures it is  possible both to simplify the specification of pictures and to speed up their generation.  Display procedures  permit picture elements to be defined conditionally and also facilitate the processing of inputs from  pointing devices.  The paper is illustrated by examples from aversion of the EULER language in which  display procedures were implemented.	1971	Newman	computer graphics, programming languages, display files
2153	Experiments with an Automated Instructional System for Numerical Methods	A computer system was developed at Purdue University to teach portions of an undergraduate  course in numerical methods.  Each instructional unit or lesson is divided into three modes of instruction  which allow the student to press from a computer-controlled presentation to a student-controlled investigation.  The system is designed as a classroom-independent course of study, and has been used for two semesters  by students in lieu of conventional classroom instruction.  Initial measures of effectiveness, student  acceptance, and operational cost are the result of testing the system independent of instructor intervention.   The system is operational on a CDC 6500 with teletype terminals.	1971	Oldehoeft, Conte	computer-assisted instruction, numerical methods, CAI, instructional systems
2154	Clarification of Fortran Standards-Second Report	In 1966, after four years of effort, Fortran became the first programming language standardized  in the United States.  Since that initial achievement study and application of the standard specifications  have revealed the need for maintenance of the standards.  As the result of work initiated in 1967, an  initial set of clarifying interpretations was prepared and this clarification was published in Communications  of the ACM in May 1969.  That work has continued and has resulted in the preparation of this second set  of clarifying interpretations.  The nature of the maintenance and the new set of corrections to and interpretations  of the standard specifications are reported.	1971		ANSI Subcommittee X3J3
2155	Toward an Understanding of Data Structures	This paper presents a notation and formalism for describing the semantics of data structures.   This is based on directed graphs with named edges and transformations on these graphs.  In addition,  an implementation facility is described which could be part of a programming language, which allows a  programmer who has expressed the semantics of an algorithm in terms of the graphs to then specify the  implementation of some of his data structures in order to gain efficiency.	1971	Earley	data structures, graph, implementation, semantic formalism, programming language
2156	Comment on Cheney's List-Compaction Algorithm		1971	Fenichel	LISP, garbage collector, virtual memory, list processing, storage allocation
2157	Average Binary Search Length for Dense Ordered Lists		1971	Flores, Madpis	searching, binary searching, record retrieval
2158	A Stopping Criterion for the Newton-Raphson Method in Implicit Multistep Integration Algorithms  for Nonlinear Systems of Ordinary Differential Equations		1971	Liniger	ordinary differential equations, linear multistep formulas, Newton-Raphson method, stopping criterion
2159	A Note on Best One-Sided Approximations		1971	Phillips	best approximation, one-sided approximation, logarithmic, error, relative error
2160	Canonical Structure in Attribute Based File Organization	A new file structure for attribute based retrieval is proposed in this paper.  It allows queries  involving arbitrary Boolean functions of the attribute-value pairs to be processed without taking intersections  of lists.  The structure is highly dependent on the way in which the file is to be used and is uniquely  determined by the specification of the allowed queries.  Thus, for example, the structure for retrieval  on the basis of ranges of values of a given attribute would be very different from one where only retrieval  on the basis of a single value is permitted.  The file organization being proposed is based on the atoms  of a Boolean algebra generated by the queries.  The desirable properties claimed for this structure are  proved, and file maintenance questions are discussed.	1971	Wong, Chiang	address calculation, atoms of Boolean algebra, attributes, Boolean functions, Boolean queries, file organization, information retrieval, inverted file, key words, multilist, queries, searches
2161	An Algorithm for the Blocks and Cutnodes of a Graph (Corrigendum)		1971	Paton	
2162	An Efficient Bit Table Technique for Dynamic Storage Allocation of 2^n-word Blocks	An efficient bit table technique for dynamic storage allocation of 2^n-word blocks, which requires  a minimized amount of memory for bookkeeping purposes, is described. The technique has been tested in  an implementation of the list processing language L^6.  A number of ideas incorporated in the processor  are also described.	1971	Isoda, Goto	bit table, dynamic storage allocation, buddy system, L^6, list processing, free storage
2163	Education Related to the Use of Computers in Organizations	The ACM Curriculum Committee on Computer Education for Management has been carrying out a study  on "Curriculum Development in Management Information Systems Education in Colleges and Universities"  under a grant from the National Science Foundation.  This position paper provides a framework for the  study.  Preliminary conclusions are presented on the need for education in administrative information  systems, and appropriate college curricula and courses are suggested.  Also, the role of professional  societies and organizations using computers is discussed, and the plans of the Committee are outlined.   The initial approach of the Committee has been to describe the education necessary for the effective  use of computers in organizations, to classify the positions for which education is required, and to  survey educational programs now available.	1971	Teichroew	education, information analysis, systems design, business data processing
2164	Symbolic Integration: The Stormy Decade	Three approaches to symbolic integration in the 1960's are described.  The first, from artificial  intelligence, led to Slagle's SAINT and to a large degree to Moses' SIN.  The second, from algebraic  manipulation, led to Manove's implementation and to Horowitz' and Tobey's reexamination of the Hermite  algorithm for integrating rational functions.  The third, from mathematics, led to Richardson's proof  of the unsolvability of the problem for a class of functions and for Risch's decision procedure for the  elementary functionseneralizations of Risch's algorithm to a class of special functions and programs  for solving differential equations and for finding the definite integral are also described.	1971	Moses	integration, symbolic integration, definite integrals, rational functions
2165	General Relativity and the Application of Algebraic Manipulative Systems	The paper describes some applications of symbolic algebra systems to problems of general relativity  including the derivation of the field equations, the Petrov classification of a metric, and the solution  of the field equations in the presence of matter in a simple case.  Attention is drawn to the strictly  algebraic difficulties encountered in this work.	1971	Barton, Fitch	symbolic mathematics, nonnumerical mathematics, general relativity, algebraic manipulation, equation  manipulation
2166	Automated Algebraic Manipulation in Celestial Mechanics	In this paper we consider some of the applications of automated algebraic manipulation which  have been made in celestial mechanics.  Particular attention is paid to the use of Poisson series, and  a typical problem in perturbation theory is described.  The requirements of processors for use in celestial  mechanics are considered and compared with those for general manipulation packages.  Some future directions  for research using these systems are briefly outlined.  To illustrate the relative simplicity of the  algorithm required in celestial mechanics, a typical integration problem is considered in an appendix.	1971	Jefferys	series manipulation, automated algebra, celestial mechanics
2167	Algebraic Simplification: A Guide for the Perplexed	Algebraic simplification is examined first from the point of view of a user who needs to comprehend  a large expression, and second from the point of view of a designer who wants to construct a useful and  efficient system.  First we describe various techniques akin to substitution.  These techniques can be  used to decrease thesize of an expression and make it more intelligible to a user.  Then we delineate  the spectrum of approaches to the design of automatic simplification capabilities in an algebraic manipulation  system.  Systems are divided into five types.  Each type provides different facilities for the manipulation  and simplification of expressions. Finally we discuss some of the theoretical results related to algebraic  simplification.  We describe several positive results about the existence of powerful simplification  algorithms and the number-theoretic conjectures on which they rely.  Results about the nonexistence of  algorithms for certain classes of expressions are included.	1971	Moses	algebraic manipulation, algebraic simplification, canonical simplification
2168	List Tracing in Systems Allowing Multiple Cell-Types	List-processing systems have each allowed the use of only a single size and configuration of  list cell.  In this paper a system is described which allows the use of arbitrarily many different sizes  and configurations of list cells, possibly not specified until run time.	1971	Fenichel	list-processing, storage allocation, LISP, SLIP, based storage, pointers
2169	The Altran System for Rational Function Manipulation-A Survey	Altran is a complete system for symbolic computation with rational functions in several variables  with integer coefficients.  It has been designed and implemented to handle large problems with ease and  efficiency.  Considerable effort has been spent to ensure a minimum amount of machine dependence in the  implementation, thus permitting the system to be installed quickly and easily on a variety of computing  machines.  In this paper a brief description of the language, run time data structures, and implementation  is given.	1971	Hall Jr.	symbolic algebra, rational function manipulation, polynomial manipulation, interpreters, translators
2170	Applications of Symbol Manipulation in Theoretical Physics	This paper surveys the applications of symbolic computation techniques to problems in theoretical  physics.  Particular emphasis is placed on applications in quantum electrodynamics where the most activity  has occurred.	1971	Hearn	symbol manipulation, algebraic simplification, computational physics, quantum electrodynamics
2171	Solution of Simultaneous Nonlinear Equations		1971	Raduchel	nonlinear equations
2172	Graph Plotter [J6] (Algorithm 412)		1971	Cermak	plot, graph, line printer plot
2173	Three Procedures for the Stable Marriage Problem [H] (Algorithm 411)		1971	McVitie, Wilson	assignment problems, assignment procedures, combinatorics, discrete mathematics, operations research, stable marriage problem, university entrance
2174	The Stable Marriage Problem	The original work of Gale and Shapley on an assignment method using the stable marriage criterion  has been extended to find all the stable marriage assignments.  The algorithm derived for finding all  the stable marriage assignments is proved to satisfy all the conditions of the problem.  Algorithm 411  applies to this paper.	1971	McVitie, Wilson	assignment problems, assignment procedures, combinatorics, discrete mathematics, operational research, stable marriage problem, university entrance
2175	Subexpression Ordering in the Execution of Arithmetic Expressions	An arithmetic expression can often be broken down into its component subexpressions.  Depending  on the hardware environment in which the expression is to be executed, these subexpressions can be evaluated  in serials, in parallel, or in a combination of these modes.  This paper shows that expression execution  time can be minimized only if consideration is given to the ordering of the subexpressions.  In particular,  subexpressions should be executed in order of decreasing memory and processor time requirements.  This  observation is valid for configurations ranging from a uniprocessor with an unbuffered main memory to  multiprocessor with a "cache" buffer memory.  If the number of subexpressions which can be executed in  parallel exceeds the number of available processors, then execution of some of these subexpressions must  be postponed.  A procedure is given which combines this requirement with the earlier ordering considerations  to provide an optimal execution sequence.	1971	Ramamoorthy, Gonzalez	parallel processing, cache, arithmetic expressions, subexpression ordering, computational trees, compilers
2176	Buffer Allocation in Merge-Sorting	A fixed buffer allocation for merge-sorting is presented here which minimizes the number of  input-output operations for a given order of merge.  When sorting on movable arm disks, the number of  seeks is equal to the number of input-output operations, and the seek time usually controls the sort  time.  First some standard terminology is introduced.  Then the input buffer allocation method is described,  followed by an analysis of the improvement to be expected over more conventional allocation.  This analysis  makes use of a particular distribution function.  An analysis of a completely different distribution  is given which yields similar results.  This suggests that the results do not depend on a particular  distribution function.  An optimum output buffer size is also determined.  It is concluded that this  buffering allocation can significantly reduce the time of merge sorting on movable arm disks when the  input data are not random, and that this output buffer allocation should be used whether the data is  random or not.	1971	Ferguson	file, item, string, merge sort, seek time, gamma distribution function
2177	An Algorithm for the Blocks and Cutnodes of a Graph	An efficient method is presented for finding blocks and cutnodes of an arbitrary undirected  graph.  The graph may be represented either (i) as an ordered list of edges or (ii) as a packed adjacency  matrix.  If w denotes the word length of the machine employed, the storage (in machine words) required  for a graph with n nodes and m edges increases essentially as 2(m+n) in case (i), or (n^2)/win case  (ii).  A spanning tree with labeled edges is grown, two edges finally bearing different labels if and  only if they belong to different blocks.  For both representations the time required to analyze a graph  on n nodes increases as n^G where G depends on the type of graph, 1 <= G <= 2, and both bounds are attained.   Values of G are derived for each of several suitable families of test graphs, generated by an extension  of the web grammar approach.  The algorithm is compared in detail with that proposed by Read for which  1 <= G <= 3.	1971	Paton	algorithm, block, block-cutpoint-tree, cutnode, fundamental cycle set, graph, lobe, lobe decomposition  graph, separable, spanning tree, web grammar
2178	A Language Extension for Graph Processing and Its Formal Semantics	A simple programming language "extension," Graspe, for processing directed graphs is defined.   Graspe consists of a type of directed graph data structure and a set of primitive operations for manipulating  these structures.  Graspe may be most easily implemented by embedding it in a host language.  Emphasis  is placed both on Graspe itself and on its method of definition.  Commonly, the definition of a language  involves definition of the syntactic elements and explanation of the meaning to be assigned them (the  semantics).  The definition of Graspe here is solely in terms of its semantics; that is, the data structures  and operations are defined precisely but without assignment of a particular syntactic representation.   Only when the language is implemented is assignment of an explicit syntax necessary.  An example of  an implementation of Graspe embedded in Lisp is given as an illustration.  The advantages and disadvantages  of the definition of a language in terms of its semantics are discussed.	1971	Pratt, Friedman	graph processing, programming language, formal semantics, directed graph, Lisp, network, data structure, flowchart, syntax, language definition
2179	Simple LR(k) Grammars	A class of context-free grammars, called the "Simple LR(k)" or SLR(k) grammars is defined.   This class has been shown to include weak precedence and simple precedence grammars as proper subsets.   How to construct parsers for the SLR(k) grammars is also shown.  These parser-construction techniques  are extendible to cover all of the LR(k) grammars of Knuth; they have been implemented and by direct  comparison proved to be superior to precedence techniques, not only in the range of grammars covered,  but also in the speed of parser construction and in the size and speed of the resulting parsers.	1971	DeRemer	context-free grammar, LR(k) grammar, precedence grammar, syntactic analysis, parsing algorithm, parser, finite-state machine, deterministic pushdown automaton
2180	A Programmer Training Project	A project is described whose purpose is to train selected black residents of the Albany-Schenectady  area in computer programming and arrange for jobs for them in the computer field. Both the organization  and curriculum of the course are discussed.	1971	Bernstein	programmer training, job opportunities, Fortran
2181	The State of Computer Oriented Curricula in Business Schools 1970	The ACM Committee on Computer Education for Management, supported by a National Science Foundation  Grant, is established to appraise the state of the art and to develop a series of recommendations for  improving computer education for management.  To provide the Committee with material for its study of  curricular needs, five regional meetings in the United States were held in 1970, at each of which a broad  cross section of invited academicians and practitioners considered the state of curricula in business  schools.  Three topics were covered: curricula for the general manager; computer-related material in  required and functional courses; and curricula for students concentrating on computer-based information  systems.  An analysis of the minutes of the meetings revealed a common set of experiences which raised  similar pedagogic and economic issues.  This presentation gives a summary of the discussions; a condensation of the pedagogic and substantive concerns raised; and consideration of the resource allocation issues  involved.  Preliminary to the Committee's recommendations for improving computer education for management,  this report has been prepared to provide the participants and the administrators of their institutions  with background information for the ongoing task of course development.  Chairman of the ten-man Committee  is Daniel Teichroew (The University of Michigan).	1971	McKenney, Tonge	university programs, management education, curriculum design, business administration curricula, graduate business school resource planning
2182	Interrupt Driven Programming		1971	Zelkowitz	interrupts, supervisors, monitors, debugging, parallel processing, associative memories, microprogramming
2183	Binary Summation		1971	Walker	summation, binary summation, floating-point addition, round-off errors
2184	On the Meaning of Names in Programming Systems	It is assumed that there is a similarity of function between the data names of a programming  language and the file names of an operating system.  The two functions are discussed in terms of the  same basic concepts in order to identify the extent to which they overlap. It is suggested that there  is some similarity between the idea of a file directory and a storable object of type context.  Manipulations  with contexts are then discussed at length.  It is noted that there is a simple extension of Church's  Lambda notation that deals nicely with these ideas of context manipulation.  Whereas a function can be  regarded as the abstraction based upon the first two terms of the expression Lambda(name list)(expression)(value list),  it is found that a context can be viewed as an abstraction based upon the first two terms in the equivalent  expression Mu(name list)(value list)(expression).	1971	Fraser	file, operating system, programming language, functions, names, context, file directory, file dictionary, lambda calculus, theory of programming
2185	A Note on Compiling Fixed Point Binary Multiplications	An algorithm is developed for compiling, as a sequence of shifts, additions,and subtractions,  many fixed point binary multiplications involving a constant.  The most significant characteristics of  the algorithm are the simplicity of the test which determines if the algorithm should be applied and  the degree to which it "suggests" efficient object code.	1971	Glaswin	compiling multiplications, fixed point arithmetic
2186	Numerical Properties of the Ritz-Trefftz Algorithm for Optimal Control	In this paper the Ritz-Trefftz algorithm is applied to the computer solution of the state regulator  problem.  The algorithm represents a modification of the Ritz direct method and is designed to improve  the speed of solution and the storage requirements to the point where real-time implementation becomes  feasible.  The modification is shown to be more stable computationally than the traditional Ritz approach.   The first concern of the paper is to describe the algorithm and establish its properties as a valid  and useful numerical technique.  In particular such useful properties as definiteness and reasonableness  of condition are established for the method.  The second part of the paper is devoted to a comparison  of the new techniques with the standard procedure of numerically integrating a matrix Riccati equation  to determine a feedback matrix.  The new technique is shown to be significantly faster for comparable  accuracy.	1971	Bosarge Jr., Johnson	splines, regulator problem, control theory, numerical analysis
2187	Computer Science: A Conceptual Framework for Curriculum Planning	Two views of computer science are considered: a global view which attempts to capture broad  characteristics of the field and its relationships to other fields, and a local view which focuses on  the inner structure of the field.  This structure is presented in terms of the kinds of knowledge, problems,  and activities that exist within the discipline, as well as the relations between them.  An approach  to curriculum planning in computer science is presented which is guided by the structure of the field,  by the fact that change is an important feature of the situation, and by the expectation that computer science will continue to increase its working contacts with other disciplines.	1971	Amarel	computer science, curriculum planning, education
2188	An Approach to the Optimum Design of Computer Graphics Systems	Display system designers are faced with the difficult task of selecting major subsystems in  an intelligent way.  Each subsystem is chosen from large numbers of alternatives; the selection is based  on considerations such as system response time, system cost, and the distribution of data storage and  processing between the graphics processor and its supporting data processing system.  The work reported  here develops an objective, quantitative design procedure and helps give a better understanding of now  to  configure display systems.  This is accomplished by means of a mathematical model of a computer driven  graphics system.  The parameters of the model are functions of the capabilities of the graphics hardware  and of the computational requirements of the graphics application.  The model can be analyzed using numerical  queueing analysis or simulation to obtain an average response time prediction.  By combining the model  with an optimization, the best graphics system configuration, subject to a cost constraint, is found  for several applications.  The optimum configurations are in turn used to find general display system  design guidelines.	1971	Foley	design guidelines, graphic display systems, mathematical model, optimum system design, queueing  model
2189	Generation of Rosary Permutations Expressed in Hamiltonian Circuits	Systematic generation of a specific class of permutations fundamental to scheduling problems  is described.  In a nonoriented complete graph with n vertices, Hamitonian circuits equivalent to .5(n  - 1)! specific permutations of n elements, termed rosary permutations, can be defined.  Each of them  corresponds to two circular permutations which mirror-image each other, and is generated successively  by a number system covering 3*4*...*(n-1) sets of edges.  Every set of edges {E[k]}, 1 <= E[k] <= k,  3 <= k <= (n-1) is determined recursively by constructing a Hamiltonian circuit with k vertices from  a Hamiltonian circuit with k-1 vertices, starting with the Hamiltonian circuit of 3 vertices.  The basic  operation consists of transposition of a pair of adjacent vertices where the position of the pair in  the permutation is determined by {E[k]}.  Two algorithms treating the same example for five vertices  are presented.  It is very easy to derive all possible n! permutations  from the .5(n - 1 )! rosary permutations  be cycling the permutations and by taking them in the reverse order-procedures which can be performed  fairly efficiently by computer.	1971	Harada	permutation, graph theory, scheduling, combinatorial algebra
2190	Function Minimization		1971	House	
2191	ALGORITHM 410 Partial Sorting [M1]		1971	Chambers	sorting, partial sorting order statistics
2192	Another Recursion Induction Principle	An inductive method for proving things about recursively defined functions is described.  It  is shown to be useful for proving partial functions equivalent and thus applicable in proofs about interpreters  for programming languages.	1971	Morris Jr.	recursion, induction, correctness, proofs, compiler correctness
2193	On Implementation of Label Variables	Variables of label mode are conventionally implemented with a technique which fails to trap  certain programming errors.  Fine-grained calendar clocks have recently become available; these allow  implementation of label variables via a new technique which traps all programming errors of this variety.	1971	Fenichel	labels, compiler, interpreter, go to, transfer
2194	How To Keep the Addresses Short	An algorithm is presented for minimizing the sum of the lengths of the blocks of coding produced  by an assembler or compiler when (1) the length of each computer instruction is assumed to be either  "long" or "short" ("long," if the memory location addressed is more than a predetermined distance from  the current location; "short," otherwise), and (2) there are blocks of instructions whose beginnings  (origins) are separated by prespecified amounts. For example, some computers permit either 8-bit addressing  (interpreted relative to the location counter) or full 16-bit addressing of all of memory.  When assembling  or compiling two or more blocks of instructions which have many mutual references in such a computer,  there is no simple iterative procedure for keeping as many of the addresses short as possible.  This  paper demonstrates that a wide class of problems of this type can be formulated as covering problems  solvable by means of elementary arithmetic operations on the column vectors of a ternary matrix.	1971	Richards	addressing, assembler, covering problem, integer programming, variable-length addressing
2195	On the Optimal Detection of Curves in Noisy Pictures	A technique for recognizing systems of lines is presented.  In this technique the heuristic  of the problem is not embedded in the recognition algorithm but is expressed in a figure of merit.    A multistage decision process is then able to recognize in the input picture the optimal system of lines  according to the given figure of merit.  Due to the global approach, greater flexibility and adequacy  in the particular problem is achieved.  The relation between the structure of the figure of merit and  the complexity of the optimization process is then discussed.  The method described is suitable for parallel  processing because the operations relative to each state can be computed in parallel, and the number of stages is equal to the length N of the curves (or to log2 N if the approximate method is used).	1971	Montanari	picture processing, picture recognition, picture description, curve detection, line detection, edge detection, optimal detection, heuristic methods, global recognition, parallel processing, dynamic  programming, interaction graph, secondary optimization problem
2196	A Man-Machine Approach Toward Solving the Traveling Salesman Problem	The traveling salesman problem belongs to an important class of scheduling and routing problems.   It is also a subproblem in solving others, such as the warehouse distribution problem.  It has been  attacked by many mathematical methods with but meager success.  Only for special forms of the problem  or for problems with a moderate number of points can it be solved exactly, even if very large amounts  of computer time are used.  Heuristic procedures have been proposed and tested with only slightly better  results.  This paper describes a computer aided heuristic technique which uses only a modest amount of  computer time in real-time to solve large (100-200) point problems.  This technique takes advantage of  both the computer's and the human's problem-solving abilities.  The computer is not asked to solve the  problem in a brute force way as in many of today's heuristics, but it is asked to organize the data for  the human so that the human can solve the problem easily.  The technique used in this paper seems to  point to new directions in the field of man-machine interaction and in the field of artificial intelligence.	1971	Krolak, Felts, Marble	heuristic procedures, computer-aided heuristic technique, man-machine interaction, artificial intelligence, assignment problem, mask of the assignment, rubber band tour generator, interaction process, traveling  salesman problem
2197	The Merit of Regional Computing Networks	One of the suggested means for stimulating the spread of computing capabilities in institutions  of higher learning is through the construction of regional computing networks.  One such network has  been constructed in the San Francisco Bay Area by Stanford University.  This paper reports upon the lessons  learned from the operation of the network over the past two years.  A major impact of the network was  not so much the computer power delivered to the schools as the awakening of computing awareness and the  fostering of capability development at these schools. The expertise and assistance from the central facility as well as the sharing of ideas among the participants were other important benefits.  Both the quality  and variety of services provided by the central facility were found to play a key role in the effectiveness  of the network.  A regional network brings many benefits and should not be judged as a purveyor of raw  computer power alone.	1971	Nielsen	computer sharing, computer utility, cooperative networks, curriculum development, educational computing, network computing, regional computing networks, remote computing, shared computing
2198	Introduction to "Feature Analysis of Generalized Data Base Management Systems"	This paper is a separately published introduction to a main report which analyzes the features  of generalized data base management systems.  This introduction gives a review of the current state of  the art in these systems and discusses the differences and similarities between capabilities found in  host language systems and those found in self-contained systems.  After some discussion of the problems  of data independence and binding,the four user levels are identified and described.  Technical problems  facing future designers are described.  The first of these is that of handling existing stored data and  the next is that of providing more complex data structures than those already available in conventional  programming languages.  The problem of high level interrogation and update functions acting on network  structures is mentioned, followed by a discussion of the problem of catering to a high volume of transactions  initiated from terminals by parametric users-the lowest level of user.  The use of Cobol as a basis for  further development work is considered at some length with respect to data structures, host language  capabilities, and self-contained capabilities.  This section also assesses the effect of the Data Base  Task Group proposals.  The final section outlines the ten major topics in the main body of the full report.	1971		Codasyl Systems Committee
2199	A Sparse Matrix Package (Part I) [F4] (Algorithm 408)		1971	McNamee	matrix, sparse matrix, matrix manipulation
2200	On Complement Division	The division algorithm theorem is expressed in a form that permits it to serve as the basis  for devising division operations that produce both quotient and remainder in complement form.  Algorithms  for division yielding complement results are derived for numbers represented in any base greater than  one.  Both radix and radix-less-one complementation schemes are considered.  The binary form of the algorithms  thus includes both two's and one's complement implementation.  The problem of quotient overflow for complement  results is dealt with as is that of selecting an appropriate form of the remainder condition for complement  division.	1971	Stein, Munro	division algorithm, complement arithmetic, complement division, one's complement arithmetic, two's  complement arithmetic
2201	Animator: An On-Line Two-dimensional Film Animation System	Animator is a computer animation system which was designed to overcome some of the inherent  disadvantages associated with conventional computer animation techniqueshe DEC-338 serves as an input  terminal for movie making, allowing the trial and error design of picture sequences in a conversational  mode.  During all stages on the system input elements (light pen, pushbuttons, and teletype) is maintained.   At the user's request, this record is sent to the IBM 360/75 where the S-D 4020 instructions necessary  to produce the same sequence of pictures can be generated.  It is anticipated that one of the primary  contributions of Animator will be the provision of a facility which will allow any professor to produce  his own expository film strips.	1971	Talbot, Carr III, Coulter Jr., Hwang	computer graphics, computer animation, on-line systems, two-dimensional languages, CRT, microfilm  recorder
2202	Dynamic Microprogramming: Processor Organization and Programming	A dynamically microprogrammed processor is characterized by a small (4^k 64-bit word) read-write  "micro" storage.  The access time of this storage is similar to the cycle time of the machine (50-100  nsec).  This microstorage is used to contain both data and subroutines.  The (micro) instructions in  such a processor differ from the conventional in that they perform only purely combinatorial operations;  sequencing is under the control of the microinstruction. The presence of the read-write microstorage  permits a more flexible assignment of resources than the read-only storage.  In particular, the processor  developed in this paper stresses the simultaneous operation (within the microinstruction) of the adder,  shifter, masker, and testing facilities of the processor.  A microassembly language is developed and  the overhead involved in subroutine linkages is analyzed.  The efficiency of a flexible software linkage  scheme is examined as to its overhead for various subroutine characteristics.  Finally, three examples  of problem-oriented programming are considered and the resulting coding is compared against a System/360  assembly language version, with the technology normalized.	1971	Tucker, Flynn	microprogramming, read-write microstorage, subroutine linkage, execution speed
2203	Key-to-Address Transform Techniques: A Fundamental Performance Study on Large Existing Formatted  Files	The results of a study of eight different key-to-address transformation methods applied to  a set of existing files are presented.  As each method is applied to a particular file, load factor and  bucket size are varied over a wide range.  In addition, appropriate variables pertinent only to a specific  method take on different values.  The performance of each method is summarized in terms of the number  of accesses required to get to a record and the number of overflow records created by a transformation.   Peculiarities of each method are discussed.  Practical guidelines obtained from the results are stated.   Finally, a proposal for further quantitative fundamental study is outlined.	1971	Lum, Yuen, Dodd	hashing, hashing techniques, hashing methods, hash coding, keys, key transformation, key-to-address  transformation, direct addressing, direct access method, randomizing, random access, file addressing, file organizations, file structures, scatter storage, search, collisions, clusters, information retrieval
2204	Program Development by Stepwise Refinement	The creative activity of programming-to be distinguished from coding-is usually taught by examples  serving to exhibit certain techniques.  It is here considered as a sequence of design decisions concerning  the decomposition of tasks into subtasks and of data into data structures.  The process of successive  refinement of specifications is illustrated by a short but nontrivial example, from which a number of  conclusions are drawn regarding the art and the instruction of programming.	1971	Wirth	education in programming, programming techniques, stepwise program construction
2205	DIFSUB for Solution of Ordinary Differential Equations [D2] (Algorithm 407)		1971	Gear	differential equations, stiff differential equations
2206	Exact Solution of Linear Equations Using Residue Arithmetic [F4] (Algorithm 406)		1971	Howell	residue arithmetic, symmetric residue, modulus, mixed-radix representation, symmetric mixed-radix  representation, mixed-radix conversion, prime number, linear equations, Gaussian elimination, matrix  inversion, determinant, adjoint matrix, ill-condition
2207	The Automatic Integration of Ordinary Differential Equations	An integration technique for the automatic solution of an initial value problem for a set of  ordinary differential equations is described.  A criterion for the selection of the order of approximation  is proposed.  The objective of the criterion is to increase the step size so as to reduce solution time.  An option permits the solution of "stiff" differential equations.  A program embodying the techniques  discussed appears in Algorithm 407.	1971	Gear	differential equations, stiff equations, integration, step control, order control
2218	An Analysis of Some Time-Sharing Techniques	The effectiveness of certain time-sharing techniques such as program, relocation, disk rotational  delay minimization, and swap volume minimization is investigated.  Summary data is presented, and the  findings are discussed.  The vehicle for this investigation was a SIMULA based simulation model reflecting  an early framework for a planned Burroughs B6500 time-sharing system.  Inasmuch as the B6500 system is  based upon the use of variable sized segments and a dynamic overlay procedure, data is also presented  which provides some indication of the effectiveness of this type of organization in a time-sharing environment.   The design characteristics and operational capabilities of the simulation model are also described.	1971	Nielsen	B6500, bulk core usage, operating system model, relocation, rotational delay minimization, simulation, swap volume minimization, system simulation, time-sharing
2246	Levels of Language for Portable Software	An increasing amount of software is being implemented in a portable form.  A popular way of  accomplishing this is to encode the software in a specially designed machine-independent language and  then to map this language, often using a macro processor, into the assembly language of each desired  object machine.  The design of the machine-independent language is the key factor in this operation.   This paper discusses the relative merits of pitching this language at a high level or a low level, and  presents some comparative results.	1972	Brown	portable software, level of language, machine independent, macro processor, efficiency
2251	Weighted Increment Linear Search for Scatter Tables	A new linear search for hash tables whose increment step is a function of the key being addressed  is presented.  Comparisons with known methods are given, in terms of efficiency and computation complexity.   In particular, the new method applies to tables of size n = 2^r.  It allows full table searching, and  practically eliminates primary clustering at a very low cost.	1972	Luccio	linear search, weighted increment search, scatter storage, hash table, key, hash address, clustering, search length
2256	Further Comments on Dijkstra's Concurrent Programming Control Problem		1972	Eisenberg, McGuire	critical section, concurrent programming control, multiprocessing
2266	A Highly Parallel Algorithm for Approximating All Zeros of a Polynomial with Only Real Zeros	An algorithm is described based on Newton's method which simultaneously approximates all zeros  of a polynomial with only real zeros.  The algorithm, which is conceptually suitable for parallel computation,  determines its own starting values so that convergence to the zeros is guaranteed.  Multiple zeros and  their multiplicity are readily determined.  At no point in the method is polynomial deflation used.	1972	Patrick	parallel numerical algorithms, real polynomials, real zeros, Newton's method, starting values, guaranteed convergence
2274	Generating English Discourse from Semantic Networks	A system is described for generating English sentences from a form of semantic nets in which  the nodes are word-sense meanings and the paths are primarily deep case relations.  The grammar used  by the system is in the form of a network that imposes an ordering on a set of syntactic transformations  that are expressed as LISP functions.  The generation algorithm uses the information in the semantic  network to select appropriate generation paths through the grammar.  The system is designed for use as  a computational tool that allows a linguist to develop and study methods for generating surface strings  from an underlying semantic structure.  Initial finding with regard to form determiners such as voice,  form, tense, and mood, some rules for embedding sentences, and some attention to pronominal substitution  are reported.  The system is programmed in LISP 1.5 and is available from the authors.	1972	Simmons, Slocum	semantic nets, grammars, deep case relations, semantic generation, discourse generation
2277	Demand Paging Through Utilization of Working Sets on the MANIAC II	A hardware implementation on the Maniac II computer of the working set model for demand paging,  as introduced by Denning, is discussed.  Characteristics of the Maniac II are given, along with a description  of the basic demand paging scheme and the associate memory which has been added to the Maniac II hardware.   Finally, a description of the hardware design for implementation of the working set model is discussed  and a specification of the actions taken under various conditions which may arise during the operation  of the full working set model, demand paging system is given.	1972	Morris	demand paging, dynamic storage allocation, Maniac II, memory allocation, one-level store, paging, paging associative memory, storage allocation, thrashing, virtual memory, working set model
2289	Cellular Arrays for the Solution of Graph Problems	A cellular array is a two-dimensional, checkerboard type interconnection of identical modules  (or cells), where each cell contains a few bits of memory and a small amount of combinational logic,  and communicates mainly with its immediate neighbors in the array.  The chief computational advantage  offered by cellular arrays is the improvement in speed achieved by virtue of the possibilities for parallel  processing.  In this paper it is shown that cellular arrays are inherently well suited for the solution  of many graph problems.  For example, the adjacency matrix of a graph is easily mapped onto an array;  each matrix element is stored in one cell of the array, and typical row and column operations are readily  implemented by simple cell logic.  A major challenge in the effective use of cellular arrays for the  solution of graph problems is the determination of algorithms that exploit the possibilities for parallelism,  especially for problems whose solutions appear to be inherently serial.  In particular, several parallelized  algorithms are presented for the solution of certain spanning tree, distance, and path problems, with  direct applications to wire routing, PERT chart analysis, and the analysis of many types of networks.   These algorithms exhibit a computation time that in many cases grows at a rate not exceeding log2 n,  where n is the number of nodes in the graph.  Straightforward cellular implementations of the well-known  serial algorithms for these problems require about n steps, and noncellular implementations require from  n^2 to n^3 steps.	1972	Levitt, Kautz	graph theory, cellular logic-in-memory arrays, parallel processing, special purpose computers, algorithms for distance and spanning tree problems
2290	Immediate Predominators in a Directed Graph [H] (Algorithm A430)		1972	Purdom Jr., Moore	predominator, immediate predominator, graph theory, directed graph, shortest path, articulation, connectivity, program optimization, optimizing compiler
2297	A Model of Memory Contention in a Paging Machine	This paper is concerned with certain aspects of contention for main memory resources in a multiprogrammed  computer system operating under demand paging.  In the model presented, the number of page-frames of  main memory allocated to a problem program varies in time.  These changes in memory configuration are  represented explicitly in the model, CPU requirements and page exception characteristics of program material  being described statistically.  Expressions for the distribution of the number of page-frames allocated  to an executing program, the long run expected fraction of a program's execution time in a given number  of page-frames, and the average execution interval of the multiprogrammed load are obtained.  It is pointed  out heuristically and demonstrated numerically that an increase is obtain able in the average execution  interval of the multiprogrammed load over that resulting from equal fixed partitioning of main memory.	1972	Oden, Shedler	paging machines, demand paging, operating systems studies, queuing analysis, memory contention, memory management
2307	Dynamic Document Processing	The current role of computers in automatic document processing is briefly outlined, and some  reasons are given why the early promise of library automation and of the mechanization of documentation  processes has not been fulfilled.  A new dynamic document environment is then outlined in which clustered files are searched and information is retrieved following an interactive user-controlled search process.   Methods are described for an automatic query modification based on user needs, and for a continuous  reorganization of the stored information as a function of earlier file processing and of normal collection  growth.  The proposed procedures provide powerful tools for information retrieval and for the control  of dynamic library collections in which new items are continually added and old ones are retired.	1972	Salton	automatic indexing, automatic search and retrieval, iterative searching, mechanized library processing, collection growth, collection retirement, feedback search
2316	Programming Languages: History and Future	This paper discusses both the history and future of programming languages (= higher level languages).   Some of the difficulties in writing such a history are indicated.  A key part of the paper is a tree  showing the chronological development of languages and their interrelationships.  Reasons for the proliferation  of languages are given.  The major languages are listed with the reasons for their importance.  A section  on chronology indicates the happenings of the significant previous time periods and the major topics  of 1972.  Key concepts other than specific languages are discussed.	1972	Sammet	programming languages, higher level languages, languages, history, future directions, language  interrelationships, programming language tree, programming language history, programming language future
2318	The Role of Computer System Models in Performance Evaluation	Models constitute a useful means of investigating computer system performance.  This paper  examines the interrelationships between models and other methods for evaluating the performance of computer  systems and establishes circumstances under which the use of a model is appropriate.	1972	Kimbleton	modeling, evaluation, performance, analytic-models, simulation-models, system-models
2319	Operating System Performance	An overview of the current and future positions with respect to operating system performance  is given.  While a great deal of information and a large number of models for subsystems have been developed,  gaps still exist in out knowledge.  Because of the severe interactions between the various subsystems  of an operating system, an overall model of the total system must be developed to be able to analyze  and design the performance aspects of an operating system although such total system designs are exceptional  today, it is projected that they will become increasingly more common and necessary in the near future.   Such a design philosophy will clearly have a severe impact on the way we go about modularizing operating  and computer systems.	1972	Lynch	computer system, operating system, performance evaluation, performance measurement, measurement, techniques, modularity, layering, structured programming, paging, virtual memory, input/output, disk  storage facility, drum storage facility, sector queueing
2340	A Boolean Matrix Method for the Computation of Linear Precedence Functions	A modified version of Bell's Boolean matrix method for the computation of linear precedence  functions associated with a conflict-free matrix of precedence relations is given.  This algorithm not  only detects when the precedence functions do not  exist, but also provides an indication of why they  do not exist, so that corrective action can be taken if possible.  Necessary and sufficient conditions  for the existence of precedence functions are given.  The use of Boolean matrices to prove the existence  of precedence functions associated with classes of conflict-free grammars is illustrated through an example.	1972	Martin	precedence grammars, context-free parsing
2342	Interference Between Communicating Parallel Processes	Various kinds of interference between communicating parallel processes have been examined by  Dijkstra, Knuth, and others.  Solutions have been given for the mutual exclusion problem and associated  subproblems, in the form of parallel programs, and informal proofs of correctness have been given for  these solutions.  In this paper a system of parallel processes is regarded as a machine which proceeds  from one state S (i.e. a collection of pertinent data values and process configurations) to a next state  S' in accordance with a transition rule S --> S'.  A set of such rules yields sequences of states, which  dictate the system's behavior.  The mutual exclusion problem and the associated subproblems are formulated  as questions of inclusion between sets of states, or of the existence of certain sequences.  A mechanical  proof procedure is shown, which will either verify (prove the correctness of ) or discredit (prove the  incorrectness of) an attempted solution, with respect to any of the interference properties.  It is shown  how to calculate transition rules from the "partial rules" by which the individual processes operate.   The formation of partial rules and the calculation of transition rules are both applicable to hardware  processes as well as to software processes, and symmetry between processes is not required.	1972	Gilbert, Chandler	concurrent programming control, cooperating processes, formal programs, interference, mutual exclusion, operating systems, parallel processes
2358	The Multics Virtual Memory: Concepts and Design	As experience with use of on-line operating systems has grown, the need to share information  among system users has become increasingly apparent.  Many contemporary systems permit some degree of  sharing.  Usually, sharing is accomplished by allowing several users to share data via input and output  of information stored in files kept in secondary storage.  Through the use of segmentation, however,  Multics provides direct hardware addressing by user and system programs of all information, independent  of its physical storage location.  Information is stored in segments each of which is potentially sharable  and carries its own independent attributes of size and access privilege.  Here, the design and implementation  considerations of segmentation and sharing in Multics are first discussed under the assumption that all  information resides in large, segmented main memory.  Since the size of main memory on contemporary systems  is rather limited, it is then shown how the Multics software achieves the effect of a large segmented  main memory through the use of the Honeywell 645 segmentation and paging hardware.	1972	Bensoussan, Clingen, Daley	operating system, Multics, virtual memory, segmentation, information sharing, paging, memory management, memory hierarchy
2359	An Improved Index Sequential Access Method Using Hashed Overflow	The Index Sequential Access Method (ISAM) is one of the most important file management systems  used with moveable head disk devices.  This study investigates the use of an unconventional method of  treating overflow records.  The method is to use hashing techniques to allocate space for such records.   If certain conditions are satisfied, this is superior to the conventional ISAM method of chaining the  overflow records via linked list techniques.  These conditions are: long overflow chains with significant  overflow; lack of tight disk space constraints; record keys which are small compared to the total record  size; and significant use of the file in the index as opposed to the sequential mode.  Using hashed overflow,  the time to locate a record is dependent not on the total volume of overflow records as in conventional  ISAM, but on the percentage use of space dedicated to overflow records.	1972	Mullin	ISAM, index sequential, hashing, scatter storage, disk, cylinder, overflow
2363	Minimal Spanning Tree [H] (Algorithm A422)		1972	Whitney	spanning tree, minimal spanning tree, maximal spanning tree
2373	Properties of the Working-Set Model	A program's working set W(t,T) at time t is the set of distinct pages among the T most recently  referenced pages.  Relations between the average working-set size, the missing-page rate, and the interreference-interval  distribution may be derived both from time-average definitions and from ensemble-average (statistical)  definitions. An efficient algorithm for estimating these quantities is given.  The relation to LRU (least  recently used) paging is characterized.  The independent-reference model, in which page references are  statistically independent, is used to assess the effects to interpage dependencies on working-set size  observations. Under general assumptions, working-set size is shown to be normally distributed.	1972	Denning, Schwartz	working-set model, paging, paging algorithms, program behavior, program modeling
2376	Synchronization of Communicating Processes	Formalization of a well-defined synchronization mechanism can be used to prove that concurrently  running processes of a system communicate correctly.  This is demonstrated for a system consisting of  many sending processes which deposit messages in a buffer and many receiving processes which remove messages  from that buffer.  The formal description of the synchronization mechanism makes it very easy to prove  that the buffer will neither overflow nor underflow, that senders and receivers will never operate on  the same message frame in the buffer nor will they run into a deadlock.	1972	Habermann	parallel programming, multiprogramming, program correctness, process communication, process scheduling
2382	Reconstruction of Pictures from Their Projections (Corrigendum)		1972	Gordon, Herman	
2390	A Proposal for a Computer-Based Interactive Scientific Community	Because of the problems created by the explosion of papers in the mathematical sciences and  the drawbacks that this places on research, it is suggested that a tree of all mathematical results and  terminology be maintained in a multiterminal computer system.  Users of the system can store in the computer  an updated file of their current knowledge, and on selecting a paper to read, they can obtain from the  computer the minimum subtree of theorems required to bring them from what they already know to the background  knowledge which the paper assumes.  Under certain conditions, means are also provided for the contribution  of useful comments by the readers of a work and for interaction between commentators and with the author.   This paper describes how the system can be organized and the role required of readers, writers, and  commentators.	1972	Pager	interactive system, organization of scientific community, readers, writers, commentators, computer  utility, information retrieval, trees, data structures
2412	Comment on Brent's Scatter Storage Algorithm		1973	Feldman, Low	Hashing, information storage and retrieval, scatter storage, searching, symbol table
2423	A Parser-Generating System for Constructing Compressed Compilers	This paper describes a parser-generating system (PGS) currently in use on the CDC-6500 computer  at Purdue University.  The PGS is a Fortran-coded compiler.  In the input translation grammar, each BNF  syntactic rule corresponds to a (possibly empty) "code generator" realizable as an assembly language,  Fortran or Algol, subroutine that is called whenever that syntactic rule is applied in the parse of a  program.  Typical one-pass compilers constructed by the PGS translate source programs at speeds approaching  14,000 cards per minute.  For an XPL compiler, the parser program and its tables currently occupy 288  words of 60-bit core memory of which 140 words are parsing table entries and 82 words are links to code  generators.	1973	Mickunas, Schneider	parser generators, translator writing systems, syntactic analysis, normal-form grammars, pushdown  automata, translation grammars, translator optimization, compression algorithm
2433	Control Structures in Illiac IV Fortran	As part of an effort to design and implement a Fortran compiler on the ILLIAC IV, an extended  Fortran, called IVTRAN, has been developed.  This language provides a means of expressing data and control  structures suitable for exploiting ILLIAC IV parallelism.  This paper reviews the hardware characteristics  of the ILLIAC and singles out unconventional features which could be expected to influence language (and  compiler) design.  The implications of these features for data layout and algorithm structure are discussed,  and the conclusion is drawn that data allocation rather than code structuring is the crucial ILLIAC optimization  problem.  A satisfactory method of data allocation is then presented.  Language structures to utilize  this storage method and express parallel algorithms are described.	1973	Millstein	array processing, parallelism detection, explicit parallelism, array allocation, parallel control  structures, ILLIAC IV Fortran
2434	Using Page Residency To Select the Working Set Parameter	Denning's method for selecting the working set parameter, which uses interreference intervals,  is examined.  Several omissions in his model are noted, and new assumptions are introduced to overcome  these omissions.  Using this modified model, Dening's results on page residency are rederived and reconsidered  for selecting the working set parameter.	1973	Prieve	working set model, paging, program behavior, program modeling
2451	Design of Tree Structures for Efficient Querying	A standard information retrieval operation is to determine which records in a data collection  satisfy a given query expressed in terms of data values.  The process of locating the desired responses  can be represented by a tree search model.  This paper poses an optimization problem in the design of  such trees to serve a well-specified application. The problem is academic in the sense that ordinarily  the optimal tree cannot be implemented by means of practical techniques.  On the other hand, it is potentially  useful for the comparison it affords between observed performance and that of an intuitively attractive  ideal search procedure.  As a practical application of such a model this paper considers the design of  a novel tree search scheme based on a bit vector representation of data and shows that essentially the  same algorithm can be used to design either an ideal search tree or a bit-vector tree.  An experimental  study of a small formatted file illustrates the concepts.	1973	Casey	tree file, information storage and retrieval, clustering, search, data structure, data management, query answering
2452	Evaluation and Selection of File Organization-A Model and System	This work first discusses the factors that affect file (data base) organization performance,  an elusive subject, and then presents a methodology, a model and a programmed system to estimate primarily  total storage costs and average access time of several file organizations, given a specific data base,  query characterization and device-related specifications.  Based on these estimates, an appropriate file  structure may be selected for the specific situation.  The system is a convenient tool to study file  structures and to facilitate as much as possible the process of data base structure design and evaluation.	1973	Cardenas	file organization, file structures, file management, file organization performance, file organization  model, file structure design, secondary index organization, simulation, data base, access time, storage  requirement, data base analysis, data management
2495	Adapting Optimal Code Generation for Arithmetic Expressions to the Instruction Sets Available  on Present-Day Computers		1973	Stockhausen	arithmetic expressions, code generation, compilers, object-code optimization, register assignment, trees
2514	An Array Grammar Programming System	A package of Fortran programs has been developed that permits a user to interactively design  and test array grammars.  The user can control the rule selection procedure in a derivation or parse,  using weighted programming matrices; he also has a choice of instance selection schemes (raster,random,  parallel).  Examples are given involving array languages consisting of simple geometrical patterns, as  well as a language of "neuron pictures."	1973	Mercer, Rosenfeld	picture grammars, array grammars
2523	A Region Coloring Technique for Scene Analysis	A method of converting a picture into a "cartoon" or "map" whose regions correspond to differently  textured regions is described.  Texture edges in the picture are detected, and solid regions surrounded  by these (usually broken) edges are "colored in" using a propagation process.  The resulting map is cleaned  by comparing the region colors with the textures of the corresponding regions in the picture, and also  by merging some regions with others according to criteria based on topology and size.  The method has  been applied to the construction of cloud cover maps from cloud cover pictures obtained by satellites.	1973	Strong III, Rosenfeld	picture processing, scene analysis, edge detection
2524	Some Approaches to Best-Match File Searching	The problem of searching the set of keys in a file to find a key which is closest to a given  query key is discussed.  After "closest," in terms of a metric on the the key space, is suitably defined,  three file structures are presented together with their corresponding search algorithms, which are intended  to reduce the number of comparisons required to achieve the desired result. These methods are derived  using certain inequalities satisfied by metrics and by graph-theoretic concepts.  Some empirical results  are presented which compare the efficiency of the methods.	1973	Burkhard, Keller	matching, file structuring, file searching, heuristics, best match
2527	Implementation of High Level Language Machine	Computing machines which directly execute the statements of a high level language have been  proposed in the past.  This report describes the actual implementation of such a machine: it is a computer  whose "machine language" is APL.  The machine is fully operational and correctly executes almost all  of the APL operations on scalars, vectors, and arrays.  The machine automatically allocates memory, executes  statements, calls functions, converts numbers from one type to another, checks subscripts, and automatically  detects many types of programmer errors.	1973	Hassitt, Lageschulte, Lyon	computer architecture, high level language machine, emulators, microprogramming, interpreters, execution speed, APL
2530	An Algorithm for Extracting Phrases in a Space-Optimal Fashion [Z] (Algorithm A444)		1973	Wagner	information retrieval, coding, text compression
2532	On Harrison's Substring Testing Technique		1973	Bookstein	string, substring, hashing, information storage and retrieval
2535	The Effects of Multiplexing on a Computer-Communications System	A study is made of the way in which asynchronous time division multiplexing changes the stochastic  nature of the arrival process from a user to the computer and, consequently, affects the performance  of a time-shared computer-communications system.  It is concluded that while, for certain values of system  parameters, there is noticeable improvement in the performance of the computer (model), in the sense  that time-shared scheduling delays are reduced, these improvements are offset by the transmission delays  imposed by multiplexing so that there may be little or no change in the computer-communications system  performance.  Analytical and simulation results are based on the model of the computer-communications  system being an M/D/1 queue (the multiplexor) in tandem with a single exponential server (the computer).   Analytical results include a general description of the output process of an M/D/1 queue and the conditions  under which this output process is approximately Poisson.	1973	Pack	computer communications, time-sharing, multiplexing, scheduling algorithms, operating systems
2537	Common Phrases and Minimum-Space Text Storage	A method for saving storage space for text strings, such as compiler diagnostic messages, is  described.  The method relies on hand selection of a set of text strings which are common to one or more  messages.  These phrases are then stored only once.  The storage technique gives rise to a mathematical  optimization problem: determine how each message should use the available phrases to minimize its storage  requirement.  This problem is nontrivial when phrases which overlap exist.  However, a dynamic programming  algorithm is presented which solves the problem in time which grows linearly with the number of characters  in the text.  Algorithm 444 applies to this paper.	1973	Wagner	diagnostic messages, error messages, common phrases, minimum space, text storage, optimization, dynamic programming
2543	Reducing the Retrieval Time of Scatter Storage Techniques	A new method for entering and retrieving information in a hash table is described.  The method  is intended to be efficient if most entries are looked up several times.  The expected number of probes  to look up an entry, predicted theoretically and verified by Monte Carlo experiments, is considerably  less than for other comparable methods if the table is nearly full.  An example of a possible Fortran  implementation is given.	1973	Brent	address calculation, content addressing, file searching, hash addressing, hash code, linear probing, linear quotient method, scatter storage, searching, symbol table
2547	Representation of Contours ad Regions for Efficient Computer Search	A novel computer-searchable representation for the three basic pictorial features, contour  maps, region coverage, and line structures, is described.  The representation, which has practical storage  requirements, provides a rapid mean of searching large files for data associated with geometric position  as well as with attribute value.  An application of this representation to handling terrain information  illustrates its utility.  The algebraic properties of the data structure make it computationally easy  to determine whether a point lies within a closed boundary; compute the area contained by a closed boundary;  generate the closed boundary representing the union or intersection of two closed boundaries; and determine  the neighboring boundaries to a point and the minimum distances between them and the point.	1973	Merrill	contour map representation, region boundary representation, computer-search-able structure, graphic  data retrieval, graphic language, two-dimensional patterns, computer graphics, graphic display
2552	A Note on When To Chain Overflow Items Within a Direct-Access Table		1973	Bays	hash code, open hash, chaining, information retrieval, collision
2557	On the Time Required for a Sequence of Matrix Products	This paper discusses the multiplication of conformable sequences of row vectors, column vectors,  and square matrices.  The minimum time required to evaluate such products on ordinary serial computers  as well as parallel computers is discussed.  Algorithms are presented which properly parse such matrix  sequences subject to the constraints of the machine organization.	1973	Muraoka, Kuck	matrix expressions, matrix multiplication, operation minimization, parallel machine, time minimization
2559	The Reallocation of Hash-Coded Tables	When the space allocation for a hash-coded table is altered, the table entries must be rescattered  over the new space.  A technique for accomplishing this rescattering is presented.  The technique is  independent of both the length of the table and the hashing function used, and can be utilized in conjunction  with a linear reallocation of the table being rescattered.  Moreover, it can be used to eliminate previously  flagged deletions from any hash-coded table, or to change from one hashing method to another.  The efficiency  of the technique is discussed and theoretical statistics are given.	1973	Bays	reallocation, dynamic storage, hash code, scatter storage, deletions
2561	A Heuristic Approach to Inductive Inference in Fact Retrieval Systems	Heuristic procedures are presented which have been developed to perform inferences by generalizing  from available information.  The procedures make use of a similarity structure which is imposed on the  data base using nonnumerical clustering algorithms.  They are implemented in a model fact retrieval system  which uses a formal query language and a property-list data structure.  A program of experiments is described  wherein the procedures are used with test data bases which are altered by deleting part of the data and  by purposely introducing false data.  It is found that the system can infer the correct response under  a variety of conditions involving incomplete and inconsistent data.	1974	Skinner	inference, inductive inference, clustering, fact retrieval, heuristics
2575	The Best-Match Problem in Document Retrieval		1974	Van Rijsbergen	document retrieval, best match, clustering, file searching, matching, dissimilarity, hierarchy, classification
2579	Register Allocation Via Usage Counts	This paper introduces the notion of usage counts, shows how usage counts can be developed by  algorithms that eliminate redundant computations, and describes how usage counts can provide the basis  for register allocation.  The paper compares register allocation based on usage counts to other commonly  used register allocation techniques, and presents evidence which shows that the usage count technique  is significantly better than these other techniques.	1974	Freiburghouse	optimization, redundant computations, common subexpressions, register allocation, compilers, programming  languages, virtual memory, demand paging
2582	Improving Locality by Critical Working Sets	A new approach to program locality improvement via restructuring is described.  The method  is particularly suited to those systems where primary memory is managed according to a working set strategy.   It is based on the concept of critical working set, a working set which does not contain the next memory  reference.  The data the method operates upon are extracted from a trace of the program to be restructured.   It is shown that, except in some special cases, the method is not optimum.  However, the experimental  results obtained by using the method to restructure an interactive text editor and the file system module  of an operating system have shown its substantial superiority over the other methods proposed in the  literature.	1974	Ferrari	program restructuring, program segmentation, locality improvement, memory hierarchies, virtual  memory, multiprogramming, restructuring techniques, static restructuring, dynamic restructuring, working  set strategy, critical working set
2586	Adapting Optimal Code Generation for Arithmetic Expressions to the Instruction Sets Available  on Present-Day Computers (Errata)		1974	Stockhausen	
2618	A New Solution of Dijkstra's Concurrent Programming Problem	A simple solution to the mutual exclusion problem is presented which allows the system to continue  to operate despite the failure of any individual component.	1974	Lamport	critical section, concurrent programming, multiprocessing, semaphores
2629	The UNIX Time-Sharing system	UNIX is a general-purpose, multi-user, interactive operating system for the Digital Equipment  Corporation PDP-11/40 and 11/45 computers.  It offers a number of features seldom found even in larger  operating systems, including: (1) a hierarchical file system incorporating demountable volumes; (2) compatible  file, device, and inter-process I/O; (3) the ability to initiate asynchronous processes; (4) system command  language selectable on a per-user basis; and (5) over 100 subsystems including a dozen languageshis  paper discusses the nature and implementation of the file system and of the user command interface.	1974	Ritchie, Thompson	time-sharing, operating system, file system, command language, PDP-11
2631	An Information-Theoretic Approach to Text Searching in Direct Access Systems	Using direct access computer files of bibliographic information, an attempt is made to overcome  one of the problems often associated with information retrieval, namely, the maintenance and use of large  dictionaries, the greater part of which is used only infrequently.  A novel method is presented, which  maps the hyperbolic frequency distribution.  This is more suited to implementation on storage devices.   This method treats text as a string of characters rather than words bounded by spaces, and chooses subsets  of strings such that their frequencies of occurrence are more even than those of word types.  The members  of this subset are then used as index keys for retrieval. The rectangular distribution of key frequencies  results in a much simplified file organization and promises considerable cost advantages.	1974	Barton, Creasey, Lynch, Snell	text searching, information theory, file organization, direct access, information retrieval, character  string, bit vector
2641	A Minimal Spanning Tree clustering Method [Z] (Algorithm A479)		1974	Page	clustering, pattern recognition, feature selection, minimal spanning trees
2664	Parallelism in Tape-Sorting	Two methods for employing parallelism in tape-sorting are presented.  Method A is the natural  way to use parallelism. Method B is new.  Both approximately achieve the goal of reducing the processing  time by a divisor which is the number of processors.	1974	Even	tape sorting, parallelism, external sorting, queues, stacks
2667	Execution Characteristics of Programs in a Page-on-Demand System	Data are presented which show the execution characteristics of two types of commonly used programs  in a large-scale, time-shared computer system.  A software monitoring facility built into the supervisor  was used for data collection during normal system operation.  These data were analyzed, and results of  this analysis are presented for a Fortran compiler and an interactive line file editor.  Probability  distribution functions and other data are given for such things as CPU intervals, I/O intervals, and  the number of such intervals during execution.  Empirical distributions are compared with simple theoretical  distributions (exponential, hyperexponential, and geometric).  Other data show paging characteristics  of tasks as a function of the number of pages those tasks have in core.	1974	Boyse	program behavior, virtual memory, paging, demand paging, software monitor, program execution characteristics, compiler execution behavior, editor execution behavior
2668	Computation of Page Fault Probability from Program Transition Diagram	An algorithm is given for calculating page fault probability in a virtual memory system operating  under demand paging with various memory sizes and replacement rules.  A first order Markov model of program  behavior is assumed, and a representation of the system based on memory states, control states, and memory  substates is presented.  The algorithm is general in the sense that the page fault probabilities can  be calculated for nonpredictive replacement rules applied to any program represented by a one-step Markov  chain.  A detailed example is given to illustrate the algorithm for Random and Least Recently Used (LRU)  replacement rules.	1974	Franklin, Gupta	virtual memory, demand paging, replacement rule, program model, program behavior, Markov model, page fault, page fault probability
2669	A Simple Linear Model of Demand Paging Performance	Predicting the performance of a proposed automatically managed multilevel memory system requires  a model of the patterns by which programs refer to the information stored in the memory.  Some recent  experimental measurements on the Multics virtual memory suggest that, for rough approximations, a remarkably  simple program reference model will suffice.  The simple model combines the effect of the information  reference pattern with the effect of the automatic management algorithm to produce a single, composite  statement: the mean number of memory references between paging exceptions increases linearly with the  size of the paging memory.  The resulting model is easy to manipulate, and is applicable to such diverse  problems as choosing an optimum size for a paging memory, arranging for reproducible memory usage charges,  and estimating the amount of core memory sharing.	1974	Saltzer	paging, demand paging, memory models, program models, performance measurement, multilevel memory  systems, virtual memory, associative memory, memory usage accounting, Multics
2673	Quadratic Search for Hash Tables of Size p^n		1974	Ackerman	hashing, quadratic search
2681	Dynamic Memory Repacking	A probabilistic model of a multiprogramming system is exercised in order to determine the conditions  under which the dynamic repacking of main memory is beneficial.  An expression is derived for the maximum  interference that a repacking process may introduce before the original performance of the system is  degraded.  Alternative approaches to repacking are discussed, and the operating conditions that lead  to improved system throughput through repacking are delineated.	1974	Balkovich, Chiu, Presser, Wood	dynamic memory repacking, memory compaction, storage fragmentation, multiprogramming system model, probabilistic model, central processor productivity, resource utilization, system throughput
2685	The Parallel Execution of DO Loops	Methods are developed for the parallel execution of different iterations of a DO loop.  Both  asynchronous multiprocessor computers and array computers are considered.  Practical application to the  design of compilers for such computers is discussed.	1974	Lamport	parallel computing, multiprocessor computers, array computers, vector computers, loops
2687	A Cell Organized Raster Display for Line Drawings	Raster scan computer graphics displays with "real time" character generators have previously  been limited to alphanumeric characters.  A display is described which extends the capabilities of this  organization to include general graphics.  The feasibility of such a display is shown by deriving the  minimum number of patterns required in the read only memory of the character generator to synthesize  an arbitrary line.  The synthesis process does not compromise picture quality since the resulting dot  patterns are identical with those of a conventional raster display.  Furthermore, the time constraints  of a raster display are shown to be satisfied for a typical design for very complex line drawings.	1974	Jordan, Barrett	graphics, raster display, line drawing, discrete image, dot generation, matrix displays
2695	Tridiagonalization by Permutations	Tridiagonalizing a matrix by similarity transformations is an important computational tool  in numerical linear algebra. Consider the class of sparse matrices which can be tridiagonalized using  only row and corresponding column permutations.  The advantages of using such a transformation include  the absence of round-off errors and improved computation time when compared with standard transformations.   A graph theoretic algorithm which examines an arbitrary n x n matrix and determines whether or not it  can be permuted into tridiagonal form is given.  The algorithm requires no arithmetic while the number  of comparisons, the number of assignments, and the number of increments are linear in n.  This compares  very favorably with standard transformation methods.  If the matrix is permutable into tridiagonal form,  the algorithm gives the explicit tridiagonal form.  Otherwise, early rejection will occur.	1974	Gibbs, Poole	tridiagonal matrix, permutation, algorithm, eigenvalues, graph, bandwidth, sparse matrix
2699	Automatic Data Structure Choice in a Language of Very High Level	SETL is a set-theoretically oriented language of very high level whose repertoire of semantic  objects includes finite sets, ordered n-tuples, and sets of ordered n-tuples usable as mappings.  This  paper describes the structure of an optimizer for this language.  Among other methods of interest, the  optimizer uses techniques which allow relations of inclusion and membership to be established, the domains  and ranges of (tabulated) mappings to be estimated from above and below, and the single-valuedness of  (tabulated) mappings to be proved.  Once facts of this kind have been established, automatic choice of  data structures becomes possible. The methods employed are based upon, and extend, known techniques of  data flow analysis.	1975	Schwartz	program optimization, automatic programming, high-level languages, set-theoretic languages, data  structure choice
2700	Reduction: A Method of Proving Properties of Parallel Programs	When proving that a parallel program has a given property it is often convenient to assume  that a statement is indivisible, i.e. that the statement cannot be interleaved with the rest of the program.   Here sufficient conditions are obtained to show that the assumption that a statement is indivisible  can be relaxed and still preserve properties such as halting.  Thus correctness proofs of a parallel  system can often be greatly simplified.	1975	Lipton	deadlock free, reduction, interruptible, indivisible, parallel program, semaphore, verification  method, process, computation sequence
2702	On the Complexity of LR(k) Testing	The problem of determining whether an arbitrary context-free grammar is a member of some easily  parsed subclass of grammars such as the LR(k) grammars is considered.  The time complexity of this problem  is analyzed both when k is considered to be a fixed integer and when k is considered to be a parameter  of the test.  In the first case, it is shown that for every k there exists an O(n(k+2)) algorithm for  testing the LR(k) property, where n is the size of the grammar in question.  On the other hand, if both  k and the subject grammar are problem parameters, then the complexity of the problem depends very strongly  on the representation chosen for k.  More specifically, it is shown that this problem is NP-complete  when k is expressed in unary.  When k is expressed in binary the problem is complete for nondeterministic  exponential time.  These results carry over to many other parameterized classes of grammars, such as  the LL(k), strong LL(k), SLR(k), LC(k), and strong LC(k) grammars.	1975	Hunt, III Szymanski, Ullman	computational complexity, context-free grammars, parsing, LR(k) grammars, NP-complete problems
2708	Practical Syntactic Error Recovery	This paper describes a recovery scheme for syntax errors which provides automatically-generated  high quality recovery with good diagnostic information at relatively low cost. Previous recovery techniques  are summarized and empirical comparisons are made.  Suggestions for further research on this topic conclude  the paper.	1975	Graham, Rhodes	syntax errors, error recovery, error correction, parsing, simple precedence, compilers, debugging
2710	Specifying Queries as Relational Expressions: The SQUARE Data Sublanguage	This paper presents a data sublanguage called SQUARE, intended for use in ad hoc, interactive  problem solving by non-computer specialists. SQUARE is based on the relational model of data, and is  shown to be relationally complete; however, it avoids the quantifiers and bound variables required by  languages based on the relational calculus.  Facilities for query, insertion, deletion, and update on  tabular data bases are described.  A syntax is given, and suggestions are made for alternative syntaxes,  including a syntax based on English key words for users with limited mathematical background.	1975	Boyce, Chamberlin, King	database, data sublanguages, relations, query languages, casual user, relational data model, tabular  data, interactive problem solving, nonprocedural languages, relational completeness
2711	A Vector Space Model for Automatic Indexing	In a document retrieval, or other pattern matching environment where stored entities (documents)  are compared with each other or with incoming patterns (search requests), it appears that the best indexing  (property) space is one where each entity lies as far away from the others as possible; in these circumstances  the value of an indexing system may be expressible as a function of the density of the object space;  in particular, retrieval performance may correlate inversely with space density.  An approach based on  space density computations is used to choose an optimum indexing vocabulary for a collection of documents.   Typical evaluation results are shown, demonstrating the usefulness of the model.	1975	Salton, Wong, Yang	automatic information retrieval, automatic indexing, content analysis, document space
2714	Merging with Parallel Processors	Consider two linearly ordered sets A, B, |A|=m, |B|=n, m<=n, and p, p<=m, parallel processors  working synchronously.  The paper presents an algorithm for merging A and B with the p parallel processors,  which requires at most 2[log2 (2m+1)]+[3m/p] + [m/p][log2 (n/m)] steps.  If n = (2^B)m (B an integer),  the algorithm requires at most 2[log2 (m+1)] + [m/p](2+B) steps.  In the case where m and n are of the  same order of magnitude, i.e. n=km with k being a constant, the algorithm requires 2[log2 (m+1)] + [m/p](3+k)  steps.  These performances compare very favorably with the previous best parallel merging algorithm,  Batcher's algorithm, which requires n/p + ((m+n)/2p)log2 m steps in the general case and km/p + ((k+1)/2)(m/p)log2  m in the special case where n=km.	1975	Gavril	parallel processing, parallel merging, parallel binary insertion
2715	Implementation of a Structured English Query Language	The relational model of data, the XRM Relational Memory System, and the SEQUEL language have  been covered in previous papers and are reviewed.  SEQUEL is a relational data sublanguages intended  for the ad hoc interactive problem solving by non-computer specialists.  A version of SEQUEL that has  been implemented in a prototype interpreter is described.  The interpreter is designed to minimize the  data accessing operations required to respond to an arbitrary query.  The optimization algorithms designed  for this purpose are described.	1975	Astrahan, Chamberlin	relational model, query language, nonprocedural language, database, data structure, data organization
2716	Optimizing the Performance of a Relational Algebra Database Interface	An approach for implementing a "smart" interface to support a relational view of data is proposed.   The basic idea is to employ automatic programming techniques so that the interface analyzes and efficiently  refines the high level query specification supplied by the user.  A relational algebra interface, called  SQUIRAL, which was designed using this approach, is described in detail. SQUIRAL seeks to minimize query  response time and space utilization by: (1) performing global query optimization, (2) exploiting disjoint  and pipelined concurrency, (3) coordinating sort orders in temporary relations, (4) employing directory  analysis, and (5) maintaining locality in page references.  Algorithms for implementing the operators  of E. F. Codd's relational algebra are presented, and a methodology for composing them to optimize the  performance of a particular user query is described.	1975	Smith, Yen-Tang Chang	relational database, database optimization, inverted file, automatic programming, query language, data manipulation language, very high level language
2733	Deterministic Parsing of Ambiguous Grammars	Methods of describing the syntax of programming languages in ways that are more flexible and  natural than conventional BNF descriptions are considered.  These methods involve the use of ambiguous  context-free grammars together with rules to resolve syntactic ambiguities.  It is shown how efficient  LR and LL parsers can be constructed directly from certain classes of these specifications.	1975	Aho, Johnson, Ullman	programming language specification, parser generation, translator writing systems, syntax analysis, LR parsing, LL parsing, ambiguous grammars
2741	Decomposability, Instabilities, and Saturation in Multiprogramming Systems	A step-by-step approach to model the dynamic behavior and evaluate the performance of computing  systems is proposed.  It is based on a technique of variable aggregation and the concept of nearly decomposable  system, both borrowed from Econometrics.  This approach is taken in order to identify in multiprogramming  paging systems (i) unstable regimes of operations and (ii) critical computing loads which bring the system  into states of saturation.  This analysis leads to a more complete definition of the circumstances in  which "thrashing" can set in.	1975	Courtois	multiprogramming, paging, performance evaluation, saturation, instabilities, thrashing, aggregation, system levels, hierarchy, networks of queues
2751	Illumination for Computer Generated Pictures	The quality of computer generated images of three-dimensional scenes depends on the shading  technique used to paint the objects on the cathode-ray tube screen.  The shading algorithm itself depends  in part on the method for modeling the object, which also determines the hidden surface algorithm.  The  various methods of object modeling, shading, and hidden surface removal are thus strongly interconnected.   Several shading techniques corresponding to different methods of object modeling and the related hidden  surface algorithms are presented here.  Human visual perception and the fundamental laws of optics are  considered in the development of a shading rule that provides better quality and increased realism in  generated images.	1975	Phong	computer graphics, graphic display, hidden surface removal.
2765	Analysis and performance of Inverted Data Base Structures	The need to envision and architecture data base systems in a hierarchical level by level framework  is stressed. The inverted data base (file) organization is then analyzed, considering implementation  oriented aspects.  The inverted directory is viewed realistically as another large data base which itself  is subjected to inversion.  Formulations are derived to estimate average access time (read only) and  storage requirements, formalizing the interaction of data base content characteristics, logical complexity  of queries, and machine timing and blocking specifications identified as having a first-order effect  on performance.  The formulations presented are necessary to be used in conjunction with any index selection  criteria to determine the optimum set of index keys.	1975	Cardenas	data base architecture, inverted file organization, data base performance and measurement, secondary  index organization, information storage and retrieval, query answering
2771	The Synthesis of Solids Bounded by Many Faces	A technique is presented which allows a class of solid objects to be synthesized and stored  using a computer.  Synthesis begins with primitive solids like a cube, wedge, or cylinder.  Any solid  can be moved, scaled, or rotated.  Solids may also be added together or subtracted.  Two algorithms to  perform addition are described.  For practical designers, the technique has the advantage that operations  are concise, readily composed, and are given in terms of easily imagined solidsuite short sequences  of operations suffice to build up complex solids bounded by many faces.	1975	Braid	computational geometry, computer-aided design, graphics, machined components, polyhedra, shape  synthesis, three-dimensional modeling
2777	On a Solution to the Cigarette Smoker's Problem (Without Conditional Statements)	This report discusses a problem first introduced by Patil, who has claimed that the cigarette  smoker's problem cannot be solved using the P and V operations introduced by Dijkstra unless conditional  statements are used.  An examination of Patil's proof shows that he has established this claim only under  strong restrictions on the use of P and V.  These restrictions eliminate programming techniques used  by Dijkstra and others since the first introduction of the semaphore concept.  This paper contains a  solution to the problem.  It also discusses the need for the generalized operators suggested by Patil.	1975	Parnas	operating systems, co-operating processes, process synchronization primitives
2785	Glypnir-A Programming Language for Illiac IV	GLYPNIR is one of the earliest existing languages designed for programming the Illiac IV computer.  The syntax of the language is based on ALGOL 60, but has been extended to allow the programmer explicitly  to specify the parallelism of his algorithm in terms of 64-word vectors.  This paper describes the characteristics,  goals and philosophy of the language, and discusses some of the problems associated with parallel computer  architectures.	1975	Lawrie, Layman, Baer, Randal	GLYPNIR, Illiac IV, Programming language, parallel computation, parallel architecture
2795	Sentence Paraphrasing from a Conceptual Base	A model of natural language based on an underlying language-free representation of meaning  is described.  A program based on this model is able to produce sentence paraphrases which demonstrate  understanding with respect to a given context.  This generator operates in conjunction with a natural  language analyzer and a combined memory and inference model.  In generating sentences from meaning structures,  the program employs both the information retrieval and deduction capabilities of the memory model.  The  model encompasses several diverse classes of linguistic knowledge, which include: (1) executable tests  of conceptual properties stored in discrimination nets; (2) information relating conceptual to syntactic  roles, stored in a word-sense dictionary, and (3) surface grammatical knowledge, stored in a formal grammar.	1975	Goldman	artificial intelligence, natural language processing, language generation, models of cognitive  processes, semantic representation
2798	Analysis of Interleaved Memory Systems Using Blockage Buffers	A model of interleaved memory systems is presented, and the analysis of the model by Monte  Carlo simulation is discussed.  The simulations investigate the performance of various system structures,  i.e. schemes for sending instruction and data requests to the memory system.  Performance is measured  by determining the distribution of the number of memory modules in operation during a memory cycle.   An important observation from these investigations is that separately grouping instruction and data requests  for memory can substantially increase the average number of memory modules in operation during a memory  cycle.  Results of the simulations and an analytical study are displayed for various system structures.	1975	Burnett, Coffman	interleaved memory systems, modular memory systems, memory performance analysis, blockage buffer, conflict buffer, simulation, Monte Carolo simulation
2818	Interference in Multiprocessor Computer Systems with Interleaved Memory (Corrigendum)		1976	Baskett, Smith	
2824	An Improvement to Martin's Algorithm for Computation of Linear Precedence Functions		1976	Duong-Kien, Hoffmann, Muth	syntax analysis, precedence functions, Boolean matrices
2827	A Parametric Algorithm for Drawing Pictures of Solid Objects Composed of Quadric Surfaces	An algorithm for drawing pictures of three-dimensional objects, with surfaces made up of patches  of quadric surfaces, is described.  The emphasis of this algorithm is on calculating the intersections  of quadric surfaces. A parameterization scheme is used.  Each quadric surface intersection curve (QSIC)  is represented as a set of coefficients and parameter limits.  Each value of the parameter represents  at most two points, and these may easily be distinguished.  This scheme can find the coordinates of points  of even quartic (fourth-order) intersection curves, using equations of no more than second order.  Methods  of parameterization for each type of OSIC are discussed, as well as surface bounding and hidden surface  removal.	1976	Levin	computer graphics, hidden surface removal, quadric surface intersection curves
2828	Hierarchical Geometric Models for Visible Surface Algorithms	The geometric structure inherent in the definition of the shapes of three-dimensional objects  and environments is used not just to define their relative motion and placement, but also to assist in  solving many other problems of systems for producing pictures by computer.  By using an extension of  traditional structure information, or a geometric hierarchy, five significant improvements to current  techniques are possible.  First, the range of complexity of an environment is greatly increased while  the visible complexity of any given scene is kept within a fixed upper limit.  Second, a meaningful way  is provided to vary the amount of detail presented in a scene.  Third, "clipping" becomes a very fast  logarithmic search for the resolvable parts of the environment within the field of view.  Fourth, frame  to frame coherence and clipping define a graphical "working set," or fraction of the total structure  that should be present in primary store for immediate access by the visible surface algorithm.  Finally,  the geometric structure suggests a recursive descent, visible surface algorithm in which the computation  time potentially grows linearly with the visible complexity of the scene.	1976	Clark	visible surface algorithms, hidden surface algorithms, hierarchical data structures, geometric  models
2829	Texture and Reflection in Computer Generated Images	In 1974 Catmull developed a new algorithm for rendering images of bivariate surface patches.   This paper describes extensions of this algorithm in the areas of texture simulation and lighting models.   The parameterization of a patch defines a coordinate system which is used as a key for mapping patterns  onto the surface.  The intensity of the pattern at each picture element is computed as a weighted average  of regions of the pattern definition function. The shape and size of this weighting function are chosen  using digital signal processing theory.  The patch rendering algorithm allows accurate computation of  the surface normal to the patch at each picture element, permitting the simulation of the mirror reflections.   The amount of light coming from a given direction is modeled in a similar manner to the texture mapping  and then added to the intensity obtained from the texture mapping.  Several examples of images synthesized  using these new techniques are included.	1976	Blinn, Newell	computer graphics, graphic display, shading, hidden surface removal
2831	Analysis of the PFF Replacement Algorithm via a Semi-Markov Model (Corrigendum)		1976	Chu, Opderbeck	
2832	Faster Retrieval from Context Trees (Corrigendum)	Context trees provide a convenient way of storing data which is to be viewed as a hierarchy  of contexts.  This note presents an algorithm which improves on previous context tree retrieval algorithms.  It is based on the observation that in typical uses context changes are infrequent relative to retrievals,  so that data can be cached to speed up retrieval.  A retrieval is started from the position of the previous  retrieval and auxiliary structures are built up to make the search rapid.  Algorithms for addition and  deletion of data and for garbage collection are outlined.	1976	Wegbreit	context trees, frame problem, variable bindings, data structures
2836	Weighted Derivation Trees	The nodes of a weighted derivation tree are associated with weighting functions over the vocabulary  of a context-free grammar.  An algorithm is presented for constructing the optimal derivation tree having  the same structure as a given weighted derivation tree.  In addition, the correctness of the algorithm  is established.  The method may be applied to problems involving probabilistic parsing or combinatorial  optimization.	1976	Loui	derivation tree, parse tree, probabilistic grammar, structural ambiguity, combinatorial optimization
2851	Formal Verification of Parallel Programs	Two formal models for parallel computation are presented: an abstract conceptual model and  a parallel-program model.  The former model does not distinguish between control and data states.  The  latter model includes the capability for the representation of an infinite set of control states by allowing  there to be arbitrarily many instruction pointers (or processes) executing the program.  An induction  principle is presented which treats the control and data state sets on the same ground.  Through the  use of "place variables," it is observed that certain correctness conditions can be expressed without  enumeration of the set of all possible control states.  Examples are presented in which the induction  principle is used to demonstrate proofs of mutual exclusion.  It is shown that assertions-oriented proof  methods are special cases of the induction principle. A special case of the assertions method, which  is called parallel place assertions, is shown to be incomplete.  A formalization of "deadlock" is then  presented. The concept of a "norm" is introduced, which yields an extension, to the deadlock problem,  of Floyd's technique for proving termination.  Also discussed is an extension of the program model which  allows each process to have its own local variables and permits shared global variables.  Correctness  of certain forms of implementation is also discussed.  An Appendix is included which relates this work  to previous work on the satisfiability of certain logical formulas.	1976	Keller	parallel program, correctness, verification, assertions, deadlock, mutual exclusion, Petrinet
2859	Interference in Multiprocessor Computer Systems with Interleaved Memory	This paper analyzes the memory interference caused by several processors simultaneously using  several memory modules.  Exect results are computed for a simple model of such a system.   The limiting  value is derived for the relative degree of memory interference as the system size increases.  The model  of the limiting behavior of the system yields approximate results for the simple model and also suggests  that the results are valid for a much larger class of models, including those more nearly like real systems  that the simple model are tested against some measurements of program behavior and simulations of systems  using memory references from real programs.  The model results provide a good indication of the performance  that should be expected from real system of this type.	1976	Baskett, Smith	memory, memory interference, multiprocessing, interleaved memory, trace driven simulation
2862	Analysis of the PFF Replacement Algorithm via a Semi-Markov Model	An analytical model is presented to estimate the performance of the Page Fault Frequency (PFF)  replacement algorithm.  In this model, program behavior is represented by the LRU stack distance model  and the PFF replacement algorithm is represented by a semi-Markov model.  Using these models, such parameters  as the inter-page-fault interval distribution, the probability of the number of distinct pages being  referenced during an inter-page-fault interval, etc. are able to be analytically determined.  Using these  models to evaluate these parameter values permits study of the performance of the replacement algorithm  by simulating the page fault events rather than every page reference event.  This significantly reduces  the required computation time in estimating the performance of the PFF algorithm.	1976	Chu, Opderbeck	PFF replacement algorithm, semi-Markov model, simulation of replacement algorithm
2863	VMIN-An Optimal Variable-Space Page Replacement Algorithm	A criterion for comparing variable space page replacement algorithms is presented.  An optimum  page replacement algorithm, called VMIN, is described and shown to be optimum with respect to this criterion.  The results of simulating VMIN, Denning's working set, and the page partitioning replacement algorithms  on five virtual memory programs are presented to demonstrate the improvement possible over the known  realizable variable space algorithms.	1976	Prieve, Fabry	demand paging, performance measurement, multilevel memory systems, virtual memory, working set, page replacement algorithms, optimal page replacement
2881	A Counterintuitive Example of Computer Paging	A counterexample is exhibited to a natural conjecture concerning the optimal way to group records  into pages in the independent reference model of computer paging (an organization is said to be optimal  if the "least recently used" miss ratio is minimized).	1976	Fagin	least recently used, most likely together, independent reference model, storage organization, record  allocation
2895	A Language for Formal Problem Specification	A language for specifying the in tended behavior of communicating parallel processes is described.   The specifications are constrain ts on the order in which events of a computation can occur.  The language  is used to write specifications of the readers/writers problem and the writer priority of the second  readers/writers problem.	1977	Greif	formal specifications, program correctness, parallel processing, synchronization, readers/writers  problem
2896	An Exercise in Proving Parallel Programs Correct	A parallel program, Dijkstra's on-the-fly garbage collector, is proved correct using a proof  method developed by Owicki.  The fine degree of in terleaving in this program makes it especially difficult  to understand, and complicates the proof greatly.  Difficulties with proving such parallel programs correct  are discussed.	1977	Gries	garbage collection, multiprocessing, program correctness for multiprocessing tasks
2897	A Case Study of a New Code Generation Technique for Compilers	Recent developments in optimizing techniques have allowed a new design for compilers to emerge.   Such a compiler translates the parsed source code into lower level code by a sequence of steps.  Each  step expands higher level statements into blocks of lower level code and then performs optimizations  on the result.  Each statement has only one possible expansion-the task of tailoring this code to take  advantage of any special cases is done by the optimizations.  This paper provides evidence that this  strategy can indeed result in good object code.  The traditionally difficult PL/I concatenate statement  was investigated as a detailed example.  A set of fairly simple optimizations was identified which allow  the compiler to produce good code. More elaborate optimizations can further improve the object code.   For most contexts of the concatenate statement, the code produced by a compiler using the expansion-optimization  strategy described above compares favorably with the code produced by a conventional PL/I optimizing  compiler.	1977	Carter	compiler structure, optimizing compiler, code generation, PL/I compiler, concatenation, program  optimization, optimization techniques, data flow analysis
2905	Perfect Hashing Functions: A Single Probe Retrieving Method for Static Sets	A refinement of hashing which allows retrieval of an item in a static table with a single probe  is considered.  Given a set I of identifiers, two methods are presented for building, in a mechanical  way, perfect hashing functions, i.e. functions transforming the elements of I into unique addresses.   The first method, the "quotient reduction" method, is shown to be complete in the sense that for every  set I the smallest table in which the elements of I can be stored and from which they can be retrieved  by using a perfect hashing function constructed by this method can be found.  However, for nonuniformly  distributed sets, this method can give rather sparse tables.  The second method, the "remainder reduction"  method, is not complete in the above sense, but it seems to give minimal (or almost minimal) tables for  every kind of set.  The two techniques are applicable directly to small sets.  Some methods to extend  these results to larger sets are also presented.  A rough comparison with ordinary hashing is given which  shows that this method can be used conveniently in several practical applications.	1977	Sprugnoli	hashing, hashing methods, hash coding, direct addressing, identifier-to-address transformations, perfect hashing functions, perfect hash coding, reduction, scatter storage
2906	A Very High Level Programming Language for Data Processing Applications	Application development today is too labor-in tensive.  In recent years, very high-level languages  have been increasingly explored as a solution to this problem.  The Business Definition Language (BDL)  is such a language, one aimed at business data processing problems.  The concepts in BDL mimic those  which have evolved through the years in businesses using manual methods.  This results in three different  sublanguages or components: one for defining the business forms, one for describing the business organization,  and one for writing calculations.	1977	Hammer, owe, Kruskal, Wladawsky	very high level language, nonprocedural language, data flow language, business application, business  data processing, structured programming, modular programming, format specification, and design methodology
2910	Equivalence of Hough Curve Detection to Template Matching		1977	Stockman	picture processing, pattern recognition, curve detection, Hough transformation, template matching
2912	Concurrent Reading and Writing	The problem of sharing data among asynchronous process is considered.  It is assumed that only  one process at a time can modify the data, but concurrent reading and writing is permitted.  Two general  theorems are proved, and some algorithms are presented to illustrate their use.  These include a solution  to the general problem in which a read is repeated if it might have obtained an incorrect result, and  two techniques for transmitting messages between processes.  These solutions do not assume any synchronizing  mechanism other than data which can be written by one process and read by other processes.	1977	Lamport	asynchronous multiprocessing, multiprocess synchronization, readers/writers problem, shared data
2913	The Aliasing Problem in Computer-Generated Shaded Images	Certain defects, such as jagged edges and disappearing detail, have long been an annoyance  in digitally generated shaded images.  Although increasing the resolution or defocusing the display can  attenuate them, an understanding of these defects leads to more effective methods.  This paper explains  the observed defects in terms of the aliasing phenomenon inherent in sampled signals and discusses prefiltering  as a recognized cure.  A method for evaluating filters is presented, the application of prefiltering  to hidden-surface algorithms is discussed, and an implementation of a filtering tiler is shown accompanied  by examples of its effectiveness.	1977	Crow	aliasing, computer graphics, convolutional filtering, hidden-surface removal, sampling
2915	Considerations for Future Programming Language Standards Activities	This paper reviews the current state of programming language standards activities with respect  to the anomalies which exist between the various published and proposed standards for Fortran, Cobol,  PL/I, and Basic.  Proposals are made for the inclusion of formalisms within future standards and the  extension of the standards to include additional items such as error conditions and documentation.	1977	Lee	programming languages, standards, formalisms, formal descriptions, Fortran, Cobol, PL/I, Basic, Vienna Definition Language (VDL)
2916	A Fast String Searching Algorithm	An algorithm is presented that searches for the location, "i," of the first occurrence of a  character string, "pat," in another string, "string."  During the search operation, the characters of  pat are matched starting with the last character of pat.  The information gained by starting the match  at the end of the pattern often allows the algorithm to proceed in large jumps through the text being  searched.  Thus the algorithm has the unusual property that, in most cases, not all of the first i characters  of string are inspected.  The number of characters actually inspected (on the average) decreases as a  function of the length of pat.  For a random English pattern of length 5, the algorithm will typically  inspect i/4 characters of string before finding a match at i.  Furthermore, the algorithm has been implemented  so that (on the average) fewer than i+patlen machine instructions are executed.  These conclusions are  supported with empirical evidence and a theoretical analysis of the average behavior of the algorithm.   The worst case behavior of the algorithm is linear in i+patlen, assuming the availability of array space  for tables linear in patlen plus the size of the alphabet.	1977	Boyer	bibliographic search, computational complexity, information retrieval, linear time bound, pattern  matching, text editing
2917	SITAR: An Interactive Text Processing System for Small Computers (Corrigendum)		1977	Schneider, Watts	
2918	Multiprocessor Memory Organization and Memory Interference	The structure of shared memory in a multiprocessor computer system is examined with particular  attention to nonin terleaved memory.  Alternative memory organizations are compared and it is shown that  a home memory organization, in which each processor is associated with one or more memories in which  its address space is concentrated, is quite effective in reducing memory in terference.  Home memory organization  is shown to be particularly suited to certain specialized computation problems as well as to possess  advantages in terms of in terference and reliability for general purpose computation.  Results for in terleaved  memory are drawn from previous work and are used for comparison.  Trace-driven simulations are used to  verify the conclusions of the analysis.	1977	Smith	memory in terference, interleaving, multiprocessing, trace-driven simulation, queueing theory, shared  memory
2919	The Programmer's Workbench-A Machine for Software Development	On almost all software development projects the assumption is made that the program development  function will be done on the same machine on which the eventual system will run.  It is only when this  production machine is unavailable or when its programming environment is totally inadequate that alternatives  are considered.  In this paper it is suggested that there are many other situations where it would be  advantageous to separate the program development and main tenance function onto a specialized computer  which is dedicated to that purpose.  Such a computer is here called a Programmer's Workbench.  The four  basic sections of the paper in troduce the subject,outline the general concept, discuss areas where such  an approach may prove beneficial, and describe an operational system utilizing this concept.	1977	Ivie	computer configurations, computer networks, software development, software engineering, software  main tenance, UNIX
2920	Game Interpretation of the Deadlock Avoidance Problem	The deadlock avoidance problem may be defined informally as the determination, from some a  priori information about the processes, resources, operating system, etc., of the "safe situations" which  may be realized without endangering the smooth running of the system.  When each process specifies its  future needs by a flowchart of need-defined steps, a global approach to the phenomenon and its in terpretation  as a game between the operating system and the processes allows formalization of risk and safety concepts.   The bipartite graph representation of this game may then be used to construct explicitly the set of safe  states and to study their properties.	1977	Devillers	operating system, multiprogramming, time-sharing, resource allocation, deadlock, in terlock, deadly  embrace, deadlock avoidance, flowchart
2921	Regular Right Part Grammars and Their Parsers	This paper in troduces an alternative to context-free grammars called regular right part (RRP)  grammars, which resemble PASCAL syntax diagrams.  Formally, RRP grammars have production right parts,  which are nondeterministic finite state machines (FSMs), and, as a special case, regular expressions,  since these can be converted to FSMs.  RRP grammars describe the syntax of programming languages more  concisely and more understandably than is possible with CF grammars.  Also in troduced is a class of parsers,  RRP LR(m, k) parsers, which includes the CF LR(k) parsers and provides the same advantages.  Informally,  an RRP LR(m, k) parser can determine the right end of each handle by considering at most k symbols to  the right of the handle and the left end, after the right end has been found, by considering at most  m symbols to the left of the handle.  A mechanism for determining the left end is required because there  is no bound on the length of the handle.	1977	LaLonde	finite state machines (automata), regular expressions, syntax diagrams, LR(k) grammars, parser construction, parsing, programming languages, language generation, formal definition, compilers, translators, scanners
2922	Two-Level Control Structure for Nondeterministic Programming	The basic ideas of nondeterministic programming are critically reconsidered to single out a  proper attitude and programming style for language allowing direct control of nondeterministic features.   The proposed attitude aims at retaining the purity of the nondeterministic formulation of search processes  on one level (the attempt level), deferring the coordination of problem solving efforts to another (the  choice level).  The feasibility of recognizing these two levels is discussed, stressing that the structure  to be managed at the choice level is a free of contexts.  The leaves are computational environments,  each holding an alternative under inspection, while the other nodes are associated with choice poin ts.   According to the proposed programming style, a generative function is associated with each choice poin t,  which expresses the desired choice strategy. The main advantage on this approach is the localization  of the search strategies: Each nonterminal node of the tree keeps track of the state of the computation  as it was when the choice poin t was last interrogated, holding at the same time the strategy to coordinate  the available alternatives.  Examples are given in term of ND-Lisp, an extension of Lisp designed and  implemented according to these guidelines.	1977	Montangero, Pacini, Turini	nondeterministic programming, artificial in telligence, control structures, backtracking, search  strategy planning, context tree
2923	High-Level Data Flow Analysis	In contrast to the predominant use of low-level in termediate text, high-level data flow analysis  deals with programs essentially at source level and exploits the control flow information implicit in  the parse tree.  The need for high-level flow analysis arises from several aspects of recent work on  advanced methods of program certification and optimization.  This paper proposes a simple general method  of high-level data flow analysis that allows free use of escape and jump statements, avoids large graphs  when compiling large programs, facilitates updating of data flow information to reflect program changes,  and derives new global information helpful in solving many familiar global flow analysis problems.  An  illustrative application to live variable analysis is presented.  Many of the graphs involved are constructed  and analyzed before any programs are compiled, thus avoiding certain costs that low-level methods incur  repeatedly at compile time.	1977	Rosen	data flow analysis, high-level language, control flow graph, structured programming, escapes, exits, jumps, goto statements
2924	An Interactive Computer Graphics Approach to Surface Representation	An in teractive computer graphics method has been developed for the rapid generation of arbitrary  shaped three-dimensional surfaces.  The method is a synthesis of spline theory and algorithms, an in teractive  means for man-machine communication, and software for static or dynamic graphics display.  The basic  technique employed is a modified lofting method on which sectional curves are represented by uniform  B-splines and the surface is in terpolated between sections by Cardinal splines.  Among the features of  this method are algorithms which enable in teractive modification of the B-spline representation of the  sectional curves.  At all stages of the process, the spatial information is graphically displayed to  the user.  Complex surfaces can be created by the combination of a number of shapes that have been separately  generated and automatically joined.  The system has been successfully in terfaced to a variety of analytical  routines for structural, medical and graphical applications.	1977		Wu, Sheng-Chuan Abel, J. F. Greenberg, D. P.
2925	Optimal Surface Reconstruction from Planar Contours	In many scientific and technical endeavors, a three-dimensional solid must be reconstructed  from serial sections, either to aid in the comprehension of the object's structure or to facilitate its  automatic manipulation and analysis.  This paper presents a general solution to the problem of constructing  a surface over a set of cross-sectional contours.  This surface, to be composed of triangular tiles,  is constructed by separately determining an optimal surface between each pair of consecutive contours.  Determining such a surface is reduced to the problem of finding certain minimum cost cycles in a directed  toroidal graph.  A new fast algorithm for finding such cycles is utilized.  Also developed is a closed-form  expression, in term of the number of contour poin ts, for an upper bound on the number of operations required  to execute the algorithm.  An illustrated example which involves the construction of a minimum area surface  describing a human head is included.	1977	Fuchs, Kedem, Uselton	surface reconstruction, contour data, serial sections, three-dimensional computer graphics, minimum  cost paths, continuous tone displays
2926	Pagination of B*-Trees with Variable-Length Records	A strategy is presented for pagination of B*-trees with variable-length records.  If records  of each length are uniformly distributed within the file, and if a wide distribution of record lengths  exists within the file, then this strategy results in shallow trees with fast access times.  The performance  of this strategy in an application is presented, compared with that of another strategy, and analyzed.	1977	McCreight	B-tree, index, database, tree storage structure, searching
2927	Some New Upper Bounds on the Generation of Prime Numbers	Given an integer N, what is the computational complexity of finding all the primes less than  N?  A modified sieve of Eratosthenes using doubly linked lists yields an algorithm of O(N) arithmetic  complexity.  This upper bound is shown to be equivalent to the theoretical lower bound for sieve methods  without preprocessing.  Use of preprocessing techniques involving space-time and additive-multiplicative  tradeoffs reduces this upper bound to O(N/log logN) and the bit complexity to O(N logN log log logN).   A storage requirement is described using O(N logN/log logN) bits as well.	1977	Mairson	computational complexity, sieve, prime number generation, number theory, linked list, preprocessing, balancing
2928	Hardware Estimation of a Process' Primary Memory Requirements	A minor hardware extension to the Honeywell 6180 processor is demonstrated to allow the primary  memory requirements of a process in Multics to be approximated.  The additional hardware required for  this estimate to be computed consists of a program accessible register containing the miss rate of the  associative memory used for page table words.  This primary memory requirement estimate was employed  in an experimental version of Multics to control the level of multiprogramming in the system and to bill  for memory usage.  The resulting system's tuning parameters display configuration insensitivity, and it is conjectured that the system would also track shifts in the referencing characteristics of its workload  and keep the system in tune.	1977	Giffor	primary memory requirement, virtual memory, level of multiprogramming, associative memory, working  set, resource allocation, LRU stack model, referencing characteristics
2929	An Analysis of Inline Substitution for a Structured Programming Language	An optimization technique known as inline substitution is analyzed.  The optimization consists  of replacing a procedure invocation by a modified copy of the procedure body.  The general problem of  using inline substitution to minimize execution time subject to size constrain ts is formulated, and an  approximate algorithmic solution is proposed.  The algorithm depends on run-time statistics about the  program to be optimized.  Preliminary results for the CLU structured programming language indicate that,  in programs with a low degree of recursion, over 90 percent of all procedure calls can be eliminated,  with little increase in the size of compiled code and a small savings in execution time.  Other conclusions  based on these results are also presented.	1977	Scheifler	inline substitution, open coding, open compilation, program optimization, compilers, structured  programming languages, run-time statistics
2930	The GRE Advanced Test in Computer Science	This report describes the Advanced Test in Computer Science which was recently in troduced in  the Graduate Record Examination Program.  The GRE program is described in general, and, the events leading  to the establishment of the Advanced Computer Science Test are discussed.  Content specifications and  their rationale are given.  A set of sample questions is included.	1977	Austing	education, computer science, graduate school admissions, test development examinations
2931	Logic and Programming Languages	Logic has been long in terested in whether answers to certain questions are computable in principle,  since the outcome puts bounds on the possibilities of formalization.  More recently, precise comparisons  in the efficiency of decision methods have become available through the developments in complexity theory.   These, however, are applications to logic, and a big question is whether methods of logic have significance  in the other direction for the more applied parts of computability theory.  Programming languages offer  an obvious opportunity as their syntactic formalization is well advanced; however, the semantical theory  can hardly be said to be complete.  Though we have many examples, we have still to give wide-ranging  mathematical answers to these queries:  What is a machine?  What is a computable process?  How (or how  well) does a machine simulate a process?  Programs naturally enter in giving descriptions of processes.   The definition of the precise meaning of a program then requires us to explain what are the objects  of computation (in a way, the statics of the problem) and how they are to be transformed (the dynamics).   So far the theories of automata and of nets, though most in teresting for dynamics, have formalized only  a portion of the field, and there has been perhaps too much concentration on the finite-state and algebraic  aspects.  It would seem that the understanding of higher-level program features involves us with infinite  objects and forces us to pass through several levels of explanation to go from the conceptual ideas to  the final simulation on a real machine.  These levels can be made mathematically exact if we can find  the right abstractions to represent the necessary structures.  The experience of many independent workers  with the method of data types as lattices (or partial orderings) under an information content ordering,  and with their continuous mappings, has demonstrated the flexibility of this approach in providing definitions  and proofs, which are clean and without undue dependence on implementations.  Nevertheless much remains  to be done in showing how abstract conceptualizations can (or cannot) be actualized before we can say  we have a unified theory.	1977	Scott	logic, programming languages, automata, denotational semantics, a-calculus models, computability, partial functions, approximation, function spaces
2932	Complexity of Computations	The framework for research in the theory of complexity of computations is described, emphasizing  the in terrelation between seemingly diverse problems and methods.  Illustrative examples of practical  and theoretical significance are given.  Directions for new research are discussed.	1977	Rabin	complexity of computations, algebraic complexity, in tractable problems, probabilistic algorithms
2933	Another Advantage of Keyword Notation for Parameter Communication with Subprograms		1977	Francez	Keyword notation, positional notation, parameters, transmission, subprograms, readability, call  by value, call by reference, call by name, compile-time errors
2934	Comment on Computing the k Shortest Paths in a Graph		1977	Lawler	graph, network, shortest path, algorithm, ranking
2935	Production and Employment of Ph.'s in Computer Science-1976 (Corrigendum)		1977	Taulbee, Conte	
2936	An Efficient Data Structure for the Simulation Event Set	Recently algorithms have been presented for the realization of event scheduling routines suitable  for general purpose discrete event simulation systems.  Several exhibited a performance superior to that  of commonly used simple linked list algorithms.  In this paper a new event scheduling algorithm is presented  which improves on two aspects of the best of the previously published algorithms.  First, the new algorithm's  performance is quite insensitive to skewed distributions, and second, its worst-case complexity is O(  n), where n is the number of events in the set.  Furthermore, tests conducted to estimate the average  complexity showed it to be nearly independent of n.	1977	Franta, Maly	simulation, time flow mechanisms, event scanning mechanisms, multilinked lists
2937	An Experimental Evaluation of Data Type Conventions	The language in which programs are written can have a substantial effect on the reliability  of the resulting programs.  This paper discusses an experiment that compares the programming reliability  of subjects using a statically typed language and a "typeless" language.  Analysis of the number of errors  and the number of runs containing errors shows that, at least in one environment, the use of a statically  typed language can increase programming reliability.  Detailed analysis of the errors made by the subjects  in programming solutions to reasonably small problems shows that the subjects had difficulty manipulating  the representation of data.	1977	Gannon	data types, experimentation, language design, redundancy, reliable software
2938	Toward a Discipline of Real-Time Programming	Programming is divided into three major categories with increasing complexity of reasoning  in program validation: sequential programming, multiprogramming, and real-time programming.  By adhering  to a strict programming discipline and by using a suitable high-level language molded after this discipline,  the complexity of reasoning about concurrency and execution time constrain ts may be drastically reduced.   This may be the only practical way to make real-time systems analytically verifiable and ultimately  reliable.  A possible discipline is outlined and expressed in terms of the language Modula.	1977	Wirth	multiprogramming, real-time programming, process synchronization, processor sharing, program validation, Modula
2939	Abstraction Mechanisms in CLU	CLU is a new programming language designed to support the use of abstractions in program construction.   Work in programming methodology has led to the realization that three kinds of abstractions-procedural,  control, and especially data abstractions-are useful in the programming process.  Of these, only the  procedural abstraction is supported well by conventional languages, through the procedure or subroutine.   CLU provides, in addition to procedures, novel linguistic mechanisms that support the use of data and  control abstractions.  This paper provides an in troduction to the abstraction mechanisms in CLU.  By  means of programming examples, the utility of the three kinds of abstractions in program construction  is illustrated, and it is shown how CLU programs may be written to use and implement abstractions.  The  CLU library, which permits incremental program development with complete type checking performed at compile  time, is also discussed.	1977	Liskov, Snyder, Atkinson, Schaffert	programming languages, data types, data abstractions, control abstractions, programming methodology, separate compilation
2940	Abstraction and Verification in Alphard: Defining and Specifying Iteration and Generators	The Alphard "form" provides the programmer with a great deal of control over the implementation  of abstract data types.  In this paper the abstraction techniques are extended from simple data representation  and function definition to the iteration statement, the most important poin t of interaction between data  and the control structure of the language itself.  A means of specializing Alphard's loops to operate  on abstract entities without explicit dependence on the representation of those entities is in troduced.   Specification and verification techniques that allow the properties of the generators for such iterations  to be expressed in the form of proof rules are developed.  Results are obtained that for common special  cases of these loops are essentially identical to the corresponding constructs in other languages.  A  means of showing that a generator will terminate is also provided.	1977	Shaw, Wulf	abstraction and representation, abstract data types, assertions, control specialization, correctness, generators, invariants, iteration statements, modular decomposition, program specifications, programming  languages, programming methodology, proofs of correctness, types, verification
2941	Early Experience with Mesa	The experiences of Mesa's first users-primarily its implementers-are discussed, and some implications  for Mesa and similar programming languages are suggested. The specific topics addressed are: module structure  and its use in defining abstractions, data-structuring facilities in Mesa, an equivalence algorithm for  types and type coercions, the benefits of the type system and why it is breached occasionally, and the  difficulty of making the treatment of variant records safe.	1977	Geschke, Morris, Satterthwaite	programming languages, types, modules, data structures, systems programming
2942	An Algol-Based Implementation of SNOBOL 4 Patterns		1977	Brownlee	patterns SNOBOL 4, pattern matching, string processing, pattern implementation, algorithms in Pascal
2943	Lucid, a Nonprocedural Language with Iteration	Lucid is a formal system in which programs can be written and proofs of programs carried out.   The proofs are particularly easy to follow and straightforward to produce because the statements in  a Lucid program are simply axioms from which the proof proceeds by (almost) conventional logical reasoning,  with the help of a few axioms and rules of inference for the special Lucid functions.  As a programming  language, Lucid is unconventional because, among other things, the order of statements is irrelevant  and assignment statements are equations.  Nevertheless, Lucid programs need not look much different than  iterative programs in a conventional structured programming language using assignment and conditional  statements and loops.	1977	Ashcrof, Wadge	program proving, formal systems, semantics, iteration, structured programming
2944	Shifting Garbage Collection Overhead to Compile Time	This paper discusses techniques which enable automatic storage reclamation overhead to be partially  shifted to compile time.  The paper assumes a transaction oriented collection scheme, as proposed by  Deutsch and Bobrow, the necessary features of which are summarized.  Implementing the described optimizations  requires global flow analysis to be performed on the source program.  It is shown that at compile time  certain program actions that affect the reference counts of cells can be deduced.  This information is  used to find actions that cancel when the code is executed and those that can be grouped to achieve improved  efficiency.	1977	Barth	garbage collection, global flow analysis, list processing, optimization, reference counts, storage  management
2945	Certification of Programs for Secure Information Flow	This paper presents a certification mechanism for verifying the secure flow of information  through a program.  Because it exploits the properties of a lattice structure among security classes,  the procedure is sufficiently simple that it can easily be included in the analysis phase of most existing  compilers.  Appropriate semantics are presented and proved correct.  An important application is the  confinement problem: The mechanism can prove that a program cannot cause supposedly nonconfidential results  to depend on confidential input data.	1977	Denning, Denning	protection, security, information flow, program certification, lattice, confinement, security classes
2946	An Alternative to Event Queues for Synchronization in Monitors	In the monitor concept, as proposed by Brinch Hansen and Hoare, event are used for synchronization.   This paper describes another synchronizing primitive which is nearly as expressive as the conditional  wait, but can be implemented more efficiently.  An implementation of this primitive in terms of P and  V operations is given together with a correctness proof.  Two examples are presented: the readers and  writers problem and the problem of information streams sharing a finite buffer pool.	1977	Kessels	monitor, operating system, mutual exclusion, synchronization, conditional critical region, structuring  concept
2947	SITAR: An Interactive Text Processing System for Small Computers	SITAR, a low-cost in teractive text handling and text analysis system for nontechnical users,  is in many ways comparable to in teractive bibliographical search and retrieval systems, but has several  additional features. It is implemented on a PDP/11 time-sharing computer invoked by a CRT with microprogrammed  editing functions.  It uses a simple command language designating a function, a file, and a search template  consisting of the textual string desired and strings delimiting the context in which the hit is to be  delivered.  Extensive experience with SITAR shows that the combined powers of simple commands, string  orientation, circular file structure, a CRT with local memory, and conversational computing produce a  system much more powerful than the sum of its parts.	1977	Schneider, Watts	information retrieval, text editing, minicomputers, CRTs, time sharing, bibliographic search and  retrieval, literary analysis, linguistic analysis, command languages
2948	A Terminal-Oriented Communication System	This paper describes a system for full-duplex communication between a time-shared computer  and its terminals.  The system consists of a communications computer directly connected to the time-shared  system, a number of small remote computers to which the terminals are attached, and connecting medium  speed telephone lines.  It can service a large number of terminals of various types.  The overall system  design is presented along with the algorithms used to solve three specific problems: local echoing, error  detection and correction on the telephone lines, and multiplexing of character output.	1977	Heckel, Lampson	terminal system, error correction, multiplexing, local echoing, communication system, network
2949	A Correctness Proof of a Topology Information Main tenance Protocol for a Distributed Computer  Network	In order for the nodes of a distributed computer network to communicate, each node must have  information about the network's topology.  Since nodes and links sometimes crash, a scheme is needed  to update this information.  One of the major constrain ts on such a topology information scheme is that  it may not involve a central controller.  The Topology Information Protocol that was implemented on the  MERIT Computer Network is presented and explained; this protocol is quite general and could be implemented  on any computer network.  It is based on Baran's "Hot Potato Heuristic Routing Doctrine."  A correctness  proof of this Topology Information Protocol is also presented.	1977	Tajibnapis	distributed computer network, correctness proofs, computer networks, distributed control, network  topology, routing problem in networks, distributed operating system, store and forward packet switching, store and forward message switching, traffic control
2950	A Unifying Approach to Scheduling	This paper presents a scheme for classifying scheduling algorithms based on an abstract model  of a scheduling system which formalizes the notion of priority.  Various classes of scheduling algorithms are defined and related to existing algorithms.  A criterion for the implementation efficiency of an  algorithm is developed and results in the definition of time-invariant algorithms, which include most  of the commonly implemented ones.  For time-invariant algorithms, the dependence of processing rates  on priorities is derived.  The abstract model provides a framework for implementing flexible schedulers  in real operating systems.  The policy-driven scheduler of Bernstein and Sharp is discussed as an example  of such an implementation	1977	Ruschitzka, Fabry	scheduling algorithms, scheduling models, priority, operating systems, processor sharing, implementation  efficiency
2951	Dynamic Response Time Prediction for Computer Networks	If the ultimate aim of a computing network is resource sharing, then the human component as  well as the technical component of networking must be fully investigated to achieve this goal.  This  research is a first step toward assisting the user in participating in the vast store of resources available  on a network. Analytical, simulation, and statistical performance evaluation tools are employed to investigate  the feasibility of a dynamic response time monitor that is capable of providing comparative response  time information for users wishing to process various computing applications at some network computing  node.  The research clearly reveals that sufficient system data are currently obtainable, at least for  the five diverse ARPA network systems studied in detail, to describe and predict the response time for  network time-sharing systems as it depends on some measure of system activity or load level.	1977	Mamrak	response time monitor, computer networks, time-sharing systems, comparative response time, ARPA  network, anlytic modeling, simulation, benchmark jobs, system measurement
2952	Functions Realizable with Word-Parallel Logical and Two's-Complement Addition Instructions		1977	Warren	Boolean functions, two's-complement, sign propagation
2953	Notes on Recursion Elimination	Various methods of recursion elimination are applied to the schematic recursive procedure:  proc S(x); px then N(x); S(fx); S(gx); M(x) fi.  Procedures with this general form arise in connection  with tree traversal and sorting algorithms.  Each method of recursion removal involves the use of one  or more stacks, and the solutions are compared on the basis of their running time.	1977	Bird	recursion elimination, optimization of programs, stacks, trees, sorting algorithms, computational  induction
2954	A Bounded Storage Algorithm for Copying Cyclic Structures	A new algorithm is presented which copies cyclic list structures using bounded workspace and  linear time. Unlike a previous similar algorithm, this one makes no assumptions about the storage allocation  system in use and uses only operations likely to be available in a high-level language.  The distinctive  feature of this algorithm is a technique for traversing the structure twice, using the same spanning  tree in each case, first from left to right and then from right to left.	1977	Robson J. M.	copying, shared subtrees, cyclic structures
2955	Buddy Systems	Two algorithms are presented for implementing any of a class of buddy systems for dynamic storage  allocation.  Each buddy system corresponds to a set of recurrence relations which relate the block sizes  provided to each other. Analyses of the in ternal fragmentation of the binary buddy system, the Fibonacci  buddy system, and the weighted buddy system are given. Comparative simulation results are also presented  for in ternal, external, and total fragmentation.	1977	Peterson, Norman	dynamic storage allocation, buddy system, fragmentation, Fibonacci buddy system, weighted buddy  system
2956	Some Ideas on Data Types in High-Level Languages	A number of issues are explored concerning the notion that a data type is a set of values together  with a set of primitive operations on those values.  Among these are the need for a notation for iterating  over the elements of any finite set (instead of the more narrow for i:= 1 to n notation), the use of  the domain of an array as a data type, the need for a simple notation for allowing types of parameters  to be themselves parameters (but in a restrictive fashion), and resulting problems with conversion of  values from one type to another.	1977	Gries, Gehani	data types, generic procedures, programming languages
2957	Database Abstractions: Aggregation	Aggregation is in troduced as an abstraction which is important in conceptualizing the real  world.  Aggregation transforms a relationship between objects into a higher-level object.  A new data  type, called aggregation, is developed which, under certain criteria of "well-definedness," specifies  aggregation abstractions.  Relational databases defined as collections of aggregates are structured as  a hierarchy on n-ary relations.  To main tain well-definedness, update operations on such databases must  preserve two invariants.  Well-defined relations are distinct from relations in third normal form.  It  is shown that these notions are complementary and both are important in database design.  A top-down  methodology for database design is described which separates decisions concerning aggregate structure  from decisions concerning key identification.  It is suggested that aggregate types, and other types  which support real-world abstractions without in troducing implementation detail, should be incorporated  into programming languages.	1977	Smith, Smith	data abstraction, relational database, data type, aggregation, database design, data structure, knowledge representation, data definition language
2958	Abstract Data Types and the Development of Data Structures	Abstract data types can play a significant role in the development of software that is reliable,  efficient, and flexible.  This paper presents and discusses the application of an algebraic technique  for the specification of abstract data types.  Among the examples presented is a top-down development  of a symbol table for a block structured language; a discussion of the proof of its correctness is given.   The paper also contains a brief discussion of the problems involved in constructing algebraic specifications  that are both consistent and complete.	1977	Guttag	abstract data type, correctness proof, data type, data structure, specification, software specification
2959	The System for Business Automation (SBA): Programming Language	The system for business automation (SBA) is a system within which application experts-nonprogrammers-can  describe and execute their applications on a computer.  The user of SBA views his application as manipulation  of information in two-dimensional pictures of tables, business forms, and reports on a display terminal.   He can gradually automate this application by giving "examples" to the system of how he manually manipulates  the information.  The Query-by-Example database language is a subset of the SBA programming language.	1977	Zloof, de Jong	programming language, graphics, user in terface, data flow, forms flow, data abstraction, database, query, data processing, business system specification, application programming
2960	Two Views of Data Abstraction		1977	Ledgard, Taylor	
2961	Experimental Investigations of the Utility of Detailed Flowcharts in Programming	This paper describes previous research on flowcharts and a series of controlled experiments  to test the utility of detailed flowcharts as an aid to program composition, comprehension, debugging,  and modification.  No statistically significant difference between flowchart and nonflowchart groups  has been shown, thereby calling into question the utility of detailed flowcharting.  A program of further  research is suggested.	1977	Shneiderman, Mayer, McKay, Heller	flowcharts, program composition, program comprehension, debugging, modification, experimental testing, human factors
2962	Production and Employment of Ph.'s in Computer Science-1976	Statistics are presented on the production and employment of Ph.'s in computer science for  the calendar year 1975-76.  Data include profiles of graduate students and of faculty at 60 Ph.-producing  departments as well as a breakdown of degrees granted by specialty areas.  Significant trends are noted  and comparisons with comparable data gathered for the 1974-75 calendar year are made.	1977	Taulbee, Conte	computer science, production of Ph.'s, employment, students
2963	A Fast Algorithm for Computing Longest Common Subsequences	Previously published algorithms for finding the longest common subsequence of two sequences  of length n have had a best-case running time of O(n^2).  An algorithm for this problem is presented  which has a running time of O((r + n)log n), where r is the total number of ordered pairs of positions  at which the two sequences match.  Thus in the worst case the algorithm has a running time of O(n^2 log  n).  However, for those applications where most positions of one sequence match relatively few positions  in the other sequence, a running time of O(n log n) can be expected.	1977	Hunt, Szymanski	Longest common subsequence, efficient algorithms
2964	An Approach to Optimal Design of Storage Parameters in Databases		1977	Milman	database organization, storage parameter optimization, resident, overflow storage
2965	An Optimal Evaluation of Boolean Expressions in an Online Query System		1977	Hanani	query, Boolean expression, information retrieval, file organization
2966	The Choice of Reference Poin ts in Best-Match File Searching	Improvements to the exhaustive search method of best-match file searching have previously been  achieved by doing a preprocessing step involving the calculation of distances from a reference poin t.    This paper discusses the proper choice of reference poin ts and extends the previous algorithm to use  more than one reference poin t.  It is shown that reference poin ts should be located outside of data clusters.   The results of computer simulations are presented which show that large improvements can be achieved  by the proper choice and location of multiple reference poin ts.	1977	Shapiro	matching, file searching, best match, nearest-neighbor classification
2967	A Comparison of Hardware and Software Associative Memories in the Context of Computer Graphics	The Associative Processing of Line Drawings (APLD) System utilizes a hardware associative memory  and creates, modifies, deletes, stores, and retrieves two-dimensional line drawings consisting of poin ts, lines, rectangles, and triangles. The APLD functions were duplicated on the TX-2 computer at M.'s  Lincoln Laboratory under the LEAP Language and Data Structure,  A comparison of the hardware approach  with the software simulation illustrates the advantages of the hardware associative memory in three areas:  (1) processing speed, (2) storage requirements, and (3) flexibility.  The major problem areas of hardware  associative memory technology, namely input/output and cost effectiveness, are also addressed.	1977	Stillman, Berra	associative memory, associative processor, content-addressable memory, graphics, information retrieval, data structures, software evaluation, hardware evaluation, parallel processing, database management
2968	A Comparison of Tree-Balancing Algorithms	Several algorithms-height-balance (i.e. AVL and extensions), weight-balance (i.e. BB and WB),  and total restructuring-for building balanced binary search trees are compared.  The criteria for comparison  encompass theoretical aspects (e.g. path lengths) and implementation independent and machine/algorithm-dependent  measures (e.g. run time).  A detailed analysis of code is also presented at a level believed to be language-and  compiler-independent.  The quality of the resulting trees and the overhead spent on building them are  analyzed, and some guidelines are given for an efficient use of the methods.  If insertion and subsequent  queries are the only operations of in terest, then "pure" AVL trees present the overall best qualities.	1977	Baer, Schwab	binary search trees, AVL trees, weight-balanced trees, path length, analysis of algorithms, information  storage and retrieval
2969	Optimal Program and Data Locations in Computer Networks	An optimization procedure for the allocation of program and data files in a computer network  is presented.  This algorithm takes into account the dependencies between files and programs such as  occur in real heterogeneous computer networks.  Insights into whether or not to convert programs from  one computer to another can also be gained from the model.  A search procedure for the file location  problem is described, along with an example and a possible application of the model.	1977	Morgan, Levin	computer networks, databases, distributed databases, optimal file location
2970	Achieving Specific Accuracy in Simulation Output Analysis	This paper extends the use of the regenerative property of queueing systems in the analysis  of simulation output.  In particular, it describes a sequential estimation method which when used with  the regenerative property allows results to be obtained with specified statistical accuracy.  This method  includes a test to check the normality assumption on which the sequential procedure relies.  The paper  illustrates the method using the empty and idle state as the regenerative state.  A second example then  describes how using the most frequently entered state as the regenerative state reduces the chance of  making a costly error in a preliminary simulation run.  The paper also described how a variance reduction  method due to Page [9] can be used to obtain a specified accuracy with considerably fewer job completions  than are required when no variance reduction technique is applied.	1977	Fishman	confidence in terval, ratio estimator, regenerative property, sequential estimator, simulation, stopping rule, variance reduction
2971	SP/k: A System for Teaching Computer Programming	SP/k is a compatible subset of the PL/I  language that has been designed for teaching programming.  The features of the SP/k language were chosen to encourage structured problem solving by computers, to  make the language easy to learn and use, to eliminate confusing and redundant constructs, and to make  the language easy to compile.  The resulting language is suitable for in troducing programming concepts  used in various applications, including business data processing, scientific calculations and non-numeric  computation.  SP/k is actually a sequence of language subsets called SP/1, SP/2,..P/8.  Each subset  in troduces new programming language constructs while retaining all the constructs of preceding subsets.  Each subset is precisely defined and can be learned or implemented without the following subsets.	1977	Holt, Wortman, Barnard, Cordy	programmer education, universities, community colleges, high schools, PL/I, SP/k, minicomputers, programming language design, teaching programming, in troductory computing
2972	Proof Techniques for Hierarchically Structured Programs	A method for describing and structuring programs that simplifies proofs of their correctness  is presented.  The method formally represents a program in terms of levels of abstraction, each level  of which can be described by a self-contained nonprocedural specification.  The proofs, like the programs,  are structured by levels.  Although only manual proofs are described in the paper, the method is also  applicable to semi-automatic and automatic proofs.  Preliminary results are encouraging, indicating that  the method can be applied to large programs, such as operating systems.	1977	Robinson, Levitt	hierarchical structure, program verification, structured programming, formal specification, abstraction, and programming methodology
2973	Sorting on a Mesh-Connected Parallel Computer	Two algorithms are presented for sorting n^2 elements on an n X n mesh-connected processor  array that require O(n) routing and comparison steps.  The best previous algorithm takes time O(n log  n).  The algorithms of this paper are shown to be optimal in time within small constant factors.  Extensions  to higher-dimensional arrays are also given.	1977	Thompson, Kung	parallel computer, parallel sorting, parallel merge, routing and comparison steps, perfect shuffle.  processor in terconnection pattern
2974	Comment on Weighted Increment Linear Search for Scatter Tables		1977	Bandyopadhyay	hash address, primary clustering, index, sequence, complementary relation, search
2975	Remark on Uniform Insertion in Structured Data Structures		1977	Hollander	data structures, directed graphs, uniform insertion
2976	Approximating Block Accesses in Database Organizations		1977	Yao	database, inverted file organization, database performance and measurement, information retrieval, query answering
2977	The Stage Hypothesis and the S-Curve: Some Contradictory Evidence	This paper presents the results of a study testing the s-shaped budget curve of Nolan's stage  model of computer development in an organization.  Research on the data processing budgets of California  counties fails to support the s-shaped curve or the use of budgets as a basis for a stage model.  However,  the results do not invalidate the concept of a stage model.  The analysis suggests an alternative model  of budget growth and a separation between models of budgeting growth and growth stages in the development  of the computer resource.	1977	Lucac, Sutton	budgets, stage theories, stage hypothesis
2978	Analysis of Design Alternatives for Virtual Memory Indexes	A class of index structures for use in a virtual memory environment is described.  Design alternatives  within this class of index structures are analyzed.  These alternatives include a choice of search strategy,  whether or not pages in the index are structured, and whether or not keys are compressed.  The average  cost of retrieving entries from these indexes is expressed as a wieghted sum of the cost of a basic key  comparison and the cost of crossing a page boundary in the index structure.  Formulas for the retrieval  costs for possible combinations of design alternatives are given.  These are used in numerical case studies  which compare the retrieval costs of the alternatives.  Qualitative comparisons of the main tenance costs  (insertion, deletion, reorganization) of the design alternatives are also included.	1977	Maruyama, Smith	index, index structure, pages, virtual memory, files, retrieval, main tenance, search strategy, key compression
2979	Studies in Machine Cognition Using The Game of Poker	A progress report is presented of on-going research efforts concerning human decision making  under uncertainly and risk and human problem solving and learning processes on the one hand, and machine  learning, large scale programming systems, and novel programming techniques on the other.  There has  also been in terest in how humans make deductive and inductive inferences and form and optimize heuristic  rules, and how machines can reach similar results.  Although the vehicle of these investigations has  been the game of poker, a conceptual framework has been provided that should have a fairly wide range  of applicability.  The models of human judgment, choice, and decision making are incorporated in a large  scale complex program.  They represent both descriptive and normative theories of behavior. An in teractive  game environment has been recently established which, besides its usefulness for experiments in game  playing, enables humans to construct machine strategies "on-line" in a question answering, advice taking  mode.	1977	Findler	machine learning, game playing programs, decision making under uncertain ty and risk, automatic  forming and optimizing of heuristic rules, automatic inductive and deductive inference making, models  of game learning, poker, gambling and bluffing
2980	The Editing  of Picture Segmentations Using Local Analysis of Graphs	A major problem in picture processing is the elimination of the large number of spurious regions  that result from an initial segmentation by region growing techniques.  Such regions have been eliminated  either on the basis of semantic information or on the basis of size and contrast.  A scheme is presented  which performs eliminations on the basis of local properties of the region adjacency graph.  The scheme  is based on definitions of graph properties which are satisfied when a spurious region is present; then  editing is equivalent to fast graph operations.  A number of examples are shown.	1977	Tanimoto, Pavlidis	picture processing, pattern recognition, segmentation, region editing
2981	Subgoal Induction	A proof method, subgoal induction, is presented as an alternative or supplement to the commonly  used inductive assertion method.  Its major virtue is that it can often be used to prove a loop's correctness  directly from its input-output specification without the use of an invariant.  The relation between subgoal  induction and other commonly used induction rules is explored and, in particular, it is shown that subgoal  induction can be viewed as a specialized form of computation induction.  A set of sufficient conditions  are presented which guarantee that an input-output specification is strong enough for the induction steps  of a proof by subgoal induction to be valid.	1977	Morris, Wegbreit	program verification, proving programs correct, induction rule, computation induction, inductive  assertions, structural induction, proof rule, recursive programs, iterative programs
2982	The Storage Requirement in Precedence Parsing		1977	Bertsch	precedence parsing, storage requirement, value table
2983	A Comparison of Next-fit, First-fit, and Best-fit		1977	Bays	memory allocation, first-fit, best-fit, next-fit
2984	Cost/Utilization: A Measure of System Performance	A method is presented for evaluating computer system performance in terms of a cost/utilization  factor and a measure of imbalance.  These coefficients indicate the extent to which the total system  cost is effectively utilized.  The method includes a technique for the visual representation of system  performance.	1977	Borovits, Ein-Dor	computer system, performance evaluation, cost/utilization, system balance
2985	Effects of Chargeout on User/Manager Attitudes	The relationship of in ternal pricing systems for computer services (chargeout systems) and  user management attitudes about their computer-based information systems is investigated. Evidence is  provided that the relationship conforms to a general pattern that would be expected from the hypothesis  of the four stages of EDP growth [15].  The results also indicate that the chargeout systems characteristic  of advanced EDP stage environments are associated with relatively high levels of positive user attitudes  and marked increases in EDP training for users. Both factors are important to the user/manager involvement  necessary for effective control of computer-based systems.  Development and main tenance of computer-based  systems is asserted to be a category of organizational change.  A "felt need" for the change on the part  of the user/manager is prerequisite to any change taking place.  The research methods of behavioral science  are applied to investigate the user/manager environment and the effects of chargeout.	1977	Nolan	computer management, computer budget, chargeout, stage hypothesis, control
2986	Operations on Sparse Relations	Various computations on relations, Boolean matrices, or directed graphs, such as the computation  of precedence relations for a context-free grammar, can be done by a practical algorithm that is asymptotically  faster than those in common use.  For example, how to compute operator precedence or Wirth-Weber precedence  relations in O(n^2) steps is shown, as well as how to compute linear precedence functions in O(n^2) steps  is shown, as well as how to compute linear precedence functions in O(n) steps, where n is the size of  a grammer.  The heart of the algorithms is a general theorem giving sufficient conditions under which  an expression whose operands are sparse relations and whose operators are composition, transitive closure,  union, and inverse, can be computed efficiently.	1977	Hunt, III Szymanski, Ullman	computational complexity, sparse relation, Boolean matrix, directed graph, Wirth-Weber precedence  relation, linear precedence function, SLR grammar, T-canonical precedence relation
2987	Representation of Many-Sided Polygons and Polygonal Lines for Rapid Processing	A representation for polygons and polygonal lines is described which allows sets of consecutive  sides to be collectively examined.  The set of sides are arranged in a binary tree hierarchy by inclusion.   A fast algorithm for testing the inclusion of a poin t in a many-sided polygon is given.  The speed of  the algorithm is discussed for both ideal and practical examples.  It is shown that the poin ts of intersection  of two polygonal lines can be located by what is essentially a binary tree search.  The algorithm and  a practical example are discussed.  The representation overcomes many of the disadvantages associated  with the various fixed-grid methods for representing curves and regions	1977	Burton W.	boundary line representation, cartography, computer graphics computer-searchable structures, contour  representation, geographic information processing, graphic data retrieval, in tersection of curves, line-drawing  processing, poin ts in polygons, regional boundary representation, spatial information
2988	Memory Management and Response Time	This paper presents a computationally tractable methodology for including accurately the effects  of finite memory size and workload memory requirements in queueing network models of computer systems.   Empirical analyses and analytic studies based on applying this methodology to an actual multiaccess  in teractive system are reported.  Relations between workload variables such as memory requirement distribution  and job swap time, and performance measures such as response time and memory utilization are graphically  displayed. A multiphase, analytically soluble model is proposed as being broadly applicable to the analysis  of in teractive computer systems which use nonpaged memories.	1977	Brown, Browne, Chandy	memory management, system performance, queueing network models, in teractive computer systems
2989	Empirical Evaluation of Some Features of Instruction Set Processor Architectures	This paper presents methods for empirical evaluation of features of Instruction Set Processors  (ISPs).  ISP features are evaluated in terms of the time used or saved by having or not having the feature.   The methods are based on analysis of traces of program executions.  The concept of a register life is  in troduced, and used to answer questions like: How many registers are used simultaneously? How many would  be sufficient all of the time? Most of the time? What would the overhead be if the number of registers  were reduced? What are registers used for during their lives? The paper also discusses the problem of  detecting desirable but non-existing instructions. Other problems are briefly discussed.  Experimental  results are presented, obtained by analyzing 41 programs running on the DEC system 10 ISP.	1977	Lunde	computer architecture, program behavior, instruction sets, op code utilization, register structures, register utilization, simultaneous register lives, instruction tracing, execution time
2990	Effective Information Retrieval Using Term Accuracy	The performance of information retrieval systems can be evaluated in a number of different  ways.  Much of the published evaluation work is based on measuring the retrieval performance of an average  user query.  Unfortunately, formal proofs are difficult to construct for the average case.  In the present  study, retrieval evaluation is based on optimizing the performance of a specific user query.  The concept  of query term accuracy is in troduced as the probability of occurrence of a query term in the documents  relevant to that query.  By relating term accuracy to the frequency of occurrence of the term in the  documents of a collection it is possible to give formal proofs of the effectiveness with respect to a  given user query of a number of automatic indexing systems that have been used successfully in experimental  situations.  Among these are inverse document frequency weighting, thesaurus construction, and phrase  generation.	1977	Yu, Salton	information retrieval, automatic indexing, content analysis, term accuracy, frequency weighting, thesaurus and phrase transformations
2991	Improving the Access Time for Random Access Files	Clustering in the key set is decreased by smoothing the key-to-address transformation, and  by adding shadow buckets to an open chaining file.  The keys are pre-hashed before the address division,  to remove the effect of sequential properties in the key set.  Shadow buckets in the key search sequence  reduce the effect of nonuniformity in file loading, and decrease the number of maximum probes needed  to locate a record.  The combined effects of these techniques lead to improved file performance for secondary  storage devices, as shown by empirical studies.	1977	Clapson	hashing, hashing techniques, hashing methods, hash coding, keys, key transformation, key-to-address  transformation, direct addressing, direct access, direct access method, randomizing, random access, file  addressing, file organizations, file structures, scatter storage, search method, collisions, synonyms, clustering, information retrieval, open addressing, open chaining, buckets, bucket size, shadow buckets, combinatorics
2996	Transient-Free Working-Set Statistics	Transient-free average working set size and transient-free missing-page rate for a finite sample  of a reference string are defined.  Use of these statistics is appropriate if the contents of the working  set at the start of the recorded string are unknown.  If a certain stationarity condition holds, these  statistics provide unbiased estimates of expected working-set sizes, missing-page probabilities, and  in terreference distance probabilities.  Two other pairs of estimators are shown to be biased.  Expressions  for the transient-free statistics are obtained in terms of in terval statistics. Several methods of computation  are discussed, the usefulness of each depending on length of the sample, number of distinct references,  and the amount of main storage available to the computer performing the calculations.  In particular,  methods are described for handling long strings containing many distinct page names.	1977	Easton, Bennett	working set, estimation program behavior
3001	Detection of Combined Occurrences	In this paper it is supposed that the variables X1,...,Xn each have finite range with the variable  Xi taking on Pi possible values and that the values of the variables are changing with time.  It is supposed  further that it is desired to detect occurrences in which some subset of the variables achieve particular  values.  Finally, it is supposed that the problem involves the detection of a large number of combined  occurrences for a large number of changes of values of variables.  Two efficient solutions for this problem  are described.  Both methods have the unusual property of being faster for systems where the sum P1 + ...  + Pn is larger. The first solution is error-free and suitable for most cases.  The second solution  is slightly more elegant and allows negation as well as conjunction, but is subject to the possibility  of errors.  An error analysis is given for the second method and an empirical study is reported.	1977	Zobrist, Carlson	coding, hash coding, retrieval, secondary keys, pattern recognition, artificial in telligence, demons, n-tuples, sorting, chess
3006	Anomalies with Variable Partition Paging Algorithms	Five types of anomalous behavior which may occur in paged virtual memory operating systems  a redefined.  One type of anomaly, for example, concerns the fact that, with certain reference strings  and paging algorithms, an increase in mean memory allocation may result in an increase in fault rate.   Two paging algorithms, are examined in terms of their anomaly potential, and reference string examples  of various anomalies are presented.  Two paging algorithm properties, the inclusion property and the  generalized inclusion property, are discussed and the anomaly implications of these properties presented.	1978	Franklin, Graham, Gupta	anomaly, memory management, program behavior, stack algorithms, virtual memory, working set, page  fault frequency, paging algorithms
3012	The Use of an Interactive Information Storage and Retrieval System in Medical Research	This paper presents the results of a study of the use of an interactive computerized storage  and retrieval system.  A monitor built into the computer system provided usage data for the study.  Additional  data on user reactions were gathe red from a questionnaire.  The results show the important role played  by frequently chosen laboratory reference leaders in influencing the use of this system.  The implications  of the study for the design of similar systems are discussed.	1978	Lucas	implementation, system use, information storage and retrieval system
3013	Some New Methods of Detecting Step Edges in Digital Pictures	This note describes two operators that respond to step edges, but not to ramps.  The first  is similar to the digital Laplacian, but uses the max, rather than the sum, of the x and y second differences.   The second uses the difference between the mean and median gray levels in a neighborhood.  The outputs  obtained from these operators applied to a set of test pictures are compared with each other and with  the standard digital Laplacian and gradient.  A third operator, which uses the distance between the center  and centroid of a neighborhood as an edge value, is also briefly considered; it turns out to be equivalent  to one of the standard digital approximations to the gradient.	1978	Schachter, Rosenfeld	image processing, pattern recognition, edge detection
3039	On-the-Fly Garbage Collection: An Exercise in Cooperation	As an example of cooperation between sequential processes with very little mutual interference  despite frequent manipulations of a large shared data space,  a technique is developed which allows nearly  all of the activity needed for garbage detection and collection to be performed by an additional processor  operating con-currently with the processor devoted to the computation proper.  Exclusion and synchronization  constraints have been kept as weak as could be achieved; the severe complexities engendered by doing  so are illustrated.	1978	Dijkstra, Lamport, Martin, Scholten, Steffens	Multiprocessing, fine-grained interleaving, cooperation between sequential processes with minimized  mutual exclusion, program correctness for multiprogramming tasks, garbage collection
3047	Using Synthetic Images to Register Real Images with Surface Models	A number of image analysis tasks can benefit from registration of the image with a model of  the surface being imaged.  Automatic navigation using visible light or radar images requires exact alignment  of such images with digital terrain models.  In addition, automatic classification of terrain, using  satellite imagery, requires such alignment to deal correctly with the effects of varying sun angle and  surface slope.  Even inspection techniques for certain industrial parts may be improved by this means.  We achieve the required alignment by matching the real image with a synthetic image obtained from a surface  model and known positions of the light sources.  The synthetic image intensity is calculated using the  reflectance map, a convenient way of describing surface reflection as a function of surface gradient.   We illustrate the technique using LANDSAT images and digital terrain models.	1978	Horn, Bachman	Image registration, synthetic images, surface models, automatic hill shading, digital terrain models, image transformation, image matching, shaded images
3048	Performance Evaluation of Highly Concurrent Computers by Deterministic Simulation	Simulation is presented as a practical technique for performance evaluation of alternative  configurations of highly concurrent computers.  A technique is described for constructing a detailed  deterministic simulation model of a system.  In the model a control stream replaces the instruction and  data streams of the real system.  Simulation of the system model yields the timing and resource usage  statistics needed for performance evaluation, without the necessity of emulating the system.  As a case  study, the implementation of a simulator of a model of the CPU-memory subsystem of the IBM 360/91 is  described.  The results of evaluating some alternative system designs are discussed.  The experiments  reveal that, for the case study, the major bottlenecks in the system are the memory unit and the fixed  point unit.  Further, it appears that many of the sophisticated pipelining and buffering technique simplemented  in the architecture of the IBM 360/91 are of little value when high-speed (cache) memory is used, as  in the IBM 360/195.	1978	Kumar, Davidson	Performance evaluation, deterministic simulation, control stream, concurrent computers
3053	Packed Scatter Tables	Scatter tables for open addressing benefit from recursive entry displacements, cutoffs for  unsuccessful searches, and auxiliary cost functions.  Compared with conventional methods, the new techniques  provide substantially improved tables that resemble exact-solution optimal packings.  The displacements  are depth-limited approximations to an enumerative (exhaustive) optimization, although packing costs  remain linear-O(n)-with table size n.  The techniques are primarily suited for important fixed (but possibly  quite large) tables for which reference frequencies may be known: op-code tables,spelling dictionaries,  access arrays.  Introduction of frequency weights further improves retrievals, but the enhancement may  degrade cutoffs.	1978	Lyon	Assignment problem, backtrack programming, hashing, open addressing, recursion, scatter table rearrangements
3059	Models for Parallel Processing WIthin Programs: Application to CPU:I/O and I/O:I/O Overlap	Approximate queueing models for internal parallel processing by individual programs in a multiprogrammed  system are developed in this paper.  The solution technique is developed by network decomposition.  The  models are formulated in terms of CPU:I/O and I/O:I/O overlap and applied to the analysis of these problems.   The percentage performance improvement from CPU:I/O overlap is found to be greatest for systems which  are in approximate CPU:I/O utilization balance and for low degrees of multiprogramming.  The percentage  improvement from I/O:I/O overlap is found to be greatest for systemtems in which the I/O system is more  utilized than the CPU.	1978	Towsley, Chandy, Browne	Multiprogramming, parallel processing, queueing network models, multiprocessing of computation  and I/O
3067	Generalized Working Sets for Segment Reference Strings	The working-set concept is extended for programs that reference segments of different sizes.   The generalized working-set policy (GWS) keeps as its resident set those segments whose retention costs  do not exceed their retrieval costs.  The GWS is a model for the entire class of demand-fetching memory  policies that satisfy a resident-set inclusion property.  A generalized optimal policy (GOPT) is also  defined; at its operating points it minimizes aggregated retention and swapping costs.  Special cases  of the cost structure allow GWS and GOPT to simulate any known stack algorithm, the working set, and  VMIN.  Efficient procedures for computing demand curves showing swapping load as a function of memory  usage are developed for GWS and GOPT policies.  Empirical data from an actual system are included.	1978	Denning, Slutz	Database referencing, memory management, optimal memory policies, paging, program behavior, program  measurement, segmentation, working sets
3073	Communicating Sequential Processes	This paper suggests that input and output are basic primitives of programming and that parallel  composition of communicating sequential processes is a fundamental program structuring method.  When  combined with a development of Dijkstra's guarded command, these concepts are surprisingly versatile.   Their use is illustrated by sample solutions of a variety of familiar programming exercises.	1978	Hoare	Programming, programming languages, programming primitives, program structures, parallel programming, concurrency, input, output, guarded commands, nondeterminacy, coroutines, procedures, multiple entries, multiple exits, classes, data representations, recursion, conditional critical regions, monitors, iterative  arrays
3075	Fast Parallel Sorting Algorithms	A parallel bucket-sort algorithm is presented that requires time O(log n) and the use of n  processors.  The algorithm makes use of a technique that requires more space than the product of processors  and time.  A realistic model is used model is used in which no memory contention is permitted.  A procedure  is also presented to sort n numbers in time O(k log n) using n 1 + 1/k processors, for k an arbitrary  integer.  The model of computation for this procedure permits simultaneous fetches from the same memory  location.	1978	Hirschberg	Parallel processing, sorting, algorithms, bucket sort
3078	Analysis of the Availability of Computer Systems Using Computer- Aided Algebra	Analytical results, related to the availability of a computer system constructed of unreliable  processors, are presented in this paper.  These results are obtained by using various computer-aided  algebraic manipulation techniques.  A major purpose of this paper is to demonstrate that the difficulties  of obtaining analytical solutions to Markov processes can be considerably reduced by the application  of symbol manipulation programs.  Since many physical systems can be modeled by Markov and semi-Markov  processes, the potential range of application of these techniques is much wider than the problem of availability  analyzed here.	1978	Chattergy, Pooch	Computer-aided algebra, symbol manipulation, Markov process, reliability, redundant structures, on-line computer system.
3083	Pseudochaining in Hash Tables	This paper presents pseudochaining as a new collision-resolution method.  Pseudochaining is  half way between open addressing and chaining.  It owes its name to the fact that link fields are present  in each cell of the hash table which permits "chaining" of the first overflow items in the table.  The  efficiency of the method is derived and a tradeoff analysis is given.	1978	Halatsis, Philokyprou	Hash code, scatter storage, open addressing, chaining, pseudochaining, collision resolution, searching, uniform probing.
3088	General Equations for Idealized CPU-I/O Overlap Configurations	General equations are derived for estimating the maximum possible utilization of main storage  partitions, CPU and I/O devices under different conditions in an idealized CPU-I/O overlap model of multiprogrammed  computer systems.  The equations are directly applicable to any configuration consisting  of sets of  identical CPU's I/O processors, main storage partitions and user tasks.  Examples are provided to illustrate  the use of the equations to compute effective processing time per record and expected timesharing response  time under both balanced and unbalanced resource utilization conditions.	1978	Teory	Blocking, buffering, input/output, overlap, performance, resource allocation, throughput, timesharing
3089	Performance of Rollback Recovery Systems under Intermittent Failures	A mathematical model of a transaction-oriented system under intermittent failures is proposed.   The system is assumed to operate with a checkpointing and rollback/recovery method to ensure reliable  information processing.  The model is used to derive the principal performance measures, including availability,  response time, and the system saturation point.	1978	Gelenbe, Derochette	Database reliability, file systems, checkpoints, recovery procedures, checking techniques, reliability  and system performance evaluation
3094	Analyses of Deterministic Parsing Algorithms	This paper describes an approach for determining the minimum, maximum, and average times to  parse sentences acceptable by a deterministic parser.  These quantities are presented in the form of  symbolic formulas, called time-formulas.  The variables in these formulas represent not only the length  of the input string but also the time to perform elementary operations such as pushing, popping, subscripting,  iterating, etc.  By binding to the  variables actual numerical values corresponding to a given compiler-machine  configuration, one can determine the execution time for that configuration.  Time-formulas are derived  by examining the grammar rules and the program representing the algorithm one wishes to analyze.  The  approach is described by using a specific grammar that defines simple arithmetic expressions.  Two deterministic parsers are analyzed: a top-down recursive descent LL(1) parser, and a bottom-up SLR(1) parser.  The  paper provides estimates for the relative efficiencies of the two parsers.  The estimates applicable  to a specific machine, the PDP-10, are presented and substantiated buy benchmarks.  Finally, the paper  illustrates the proposed approach by applying it to the analyses of parsers for a simple programming  language.	1978	Cohen, Roth	Syntactic analysis, analysis of algorithms, top-down and bottom-up parsing, relative efficiencies.
3098	Computer Generation of Gamma Random Variables	A new method for generating random variables from the gamma distribution with nonintegral shape  parameter a is proposed.  This method is similar to two other methods recently given by Wallace and Fishman.   It is compared with Fishman's and Ahrens and Dieter's methods.  The core storage requirements and programming  effort for this method are similar to those of Fishman's method.  The proposed method is the same as  Fishman's method for 1 < a < 2 and is faster than Fishman's method for 3 < a < 19.  Also, the proposed  method is much simpler than Ahrens and Dieter's method and is faster for a < 8.	1978	Tadikamalla	Gamma variables, rejection method, computer methods
3119	The Impact of Distributions and Disciplines on Multiple Processor Systems	Simple queueing models are used to study the performance tradeoffs of  multiple processor systems.  Issues considered include the impact of CPU service disciplines and distributions, level of multiprogramming, multitasking, and job priorities.	1979	Sauer, Chandy	Multiprogramming, multiprocessing, scheduling disciplines, performance evaluation, queueing models
3126	Comments on Perfect Hashing Functions: A Single Probe Retrieving  Method for Static Sets		1979	Anderson	Hashing, hashing methods, hash coding, direct addressing, identifier- to-address transformations, perfect hashing functions, perfect hash coding, reduction, retrieving, scatter storage, searching
3127	Thoth, a Portable Real-Time Operating System	Thoth isa real-time operating system which is designed to be portable over a large set of machines.  It is currently running on two minicomputers with quite different architectures.  Both the system and application programs which use it are written in a high-level language. Because the system is implemented by the same software on different hardware, it has the same interface to user programs.  Hence, application programs which use Thoth are  highly portable.  Thoth encourages structuring programs as networks of communicating processes  by providing efficient interprocess communication primitives.	1979	Cheriton, Malcolm, Melen, Sager	Portability, real time, operating systems, minicomputer
3134	The Use of Normal Multiplication Tables for Information Storage and Retrieval	This paper describes a method for the organization and retrieval of attribute  based information systems, using the normal multiplication table as a directory for the information system.  Algorithms for the organization an d retrieval of information are described.  This method is particularly suitable for queries requesting a group of information items,  all of which possess a particular set of attributes (and possibly some other attributes as well).  Several examples are given; the results with respect to the number of disk accesses and disk space are compared to other common approaches.  Algorithms evaluating the appropriateness of the above approach to a given information system are described.  For a certain class of information systems, the normal multiplication table method yields far more rapid retrieval with a more economical space requirement than conventional systems.   Moreover this method incorporates an improved modification of the inverted  file technique.	1979	Motzkin	Information retrieval, inverted files, multiattribute retrieval, multilist file, normal multiplication table, queries, rapid retrieval, space economy
3135	Detection of Three-Dimensional Patterns of Atoms in Chemical Structures	An algorithm for detecting occurrences of a three-dimensional pattern of objects within a larger structure is presented.  The search technique presented uses the geometric structure of the pattern to define characteristics demanded of candidates for matching. This is useful in cases where the properties of each atom, considered individually, do not adequately limit the number of sets of possible matchings. Several applications of this technique in the field of chemistry are: (1) in pharmacology: searching for a common constellation of atoms in molecules possessing similar biological activities; (2) in X-ray crystallography: fitting a structure or a structural fragment to a set of peaks in the electron-density distribution of a Fourier map; (3) in chemical documentation; retrieving from a file the structures containing specified substructures.	1979	Lesk	Three-dimensional pattern recognition, chemical structure search, information retrieval, crystal -structure analysis, drug analysis and design
3136	Price/Performance Patterns of U. Computer Systems	Econometric models of the U. computer market have been developed to study  the relationships between system price and hardware performance.  Single measures of price/performance such as "Grosch's Law" are shown to be so over simplified as to be meaningless.  Multiple-regression models predicting system cost as a function of several hardware characteristics do, however, reveal a market dichotomy.  On one hand there exists a stable, price predictable market for larger, general purpose computer systems.  The other market is the developing one for small business computer systems, a market which is relatively unstable with low price predictability.	1979	Cale, Gremillion, McKenney	Price/performance, Grosch's law, U. computer market
3137	A Methodology for the Design of Distributed Information Systems	A macro model of a distributed information system in presented.  The model describes the major costs of using an information system from the perspective of the end-user.  The making evident the effect of various design and operating parameters on overall cost per transaction.  The technique is illustrated by application to the design of an interactive transaction processing system.	1979	Bucci, Streeter	Distributed processing, system design, cost minimization, distributed database, interactive computing, economic modeling, transaction processing
3138	A Mathematical Programming Updating Method Using Modified Givens Transformations and Applied to LP Problems	An efficient and numerically stable method is presented for the problem of updating an orthogonal decomposition of a matrix of column (or row) vectors. The fundamental idea is to add a column (or row) analogous to adding an additional row of data in a linear least squares problem. A column (or row) is dropped by a formal scaling with the imaginary unit,  -1, followed by least squares addition of the column (or row).  The elimination process for the procedure is successive ssive application of the Givens transformation in modified (more efficient) form.  These ideas are illustrated with an implementation of the revised simplex method.  The algorithm is a general purpose one that does not account for any particular structure or sparsity in the equations.  Some suggested computational tests for determining signs of various controlling parameters in the revised simplex algorithm are mentioned.  A simple means of constructing test cases and some sample computing times are presented.	1979	Hanson, Wisniewski	Linear programming, numerical linear algebra, modified Givens transformations, linear programming test cases
3139	New Methods to Color the Vertices of a Graph	This paper describes efficient new heuristic methods to color the vertices of a graph which rely upon the comparison of the degrees and structure of a graph.  A method is developed which is exact for bipartite graphs and is an important part of heuristic procedures to find maximal cliques in general graphs.  Finally an exact method is given which performs better than the Randall-Brown algorithm and is able to color larger graphs, and the new heuristic methods, the classical methods, and the exact method are compared.	1979	Brelaz	NP-complete, graph structure, balancing, graph coloring, scheduling, comparison of the methods
3140	Social Processes and Proofs of Theorems and Programs	It is argued that formal verifications of programs, no matter how obtained, will not play the same key role in the development of computer science and software engineering as proofs do in mathematics.  Furthermore the absence of continuity, the inevitability of change, and the complexity of specification of significantly many real programs make the form al verification process difficult to justify and manage.  It is felt that ease of formal verification should not dominate program language design.	1979	De Millo, Lipton, Perlis	Formal mathematics, mathematical proofs, program verification, program specification
3141	An Improved Algorithm for Decentralized Extrema-Finding in Circular Configurations of Processes	This note presents an improvement to LeLann's algorithm for finding the largest (or smallest) of a set of uniquely numbered processes arranged in a circle, in which no central controller exists and the number of processes is not known a priori. This decentralized algorithm uses a technique of selective message extinction in order to achieve an average number of message passes of order (n log n) rather than O(n2).	1979	Chang, Roberts	Decentralized algorithms, distributed systems, operating systems
3142	Consumer Difficulties With Computerized Transactions:  An Empirical Investigation	The prevalence with which errors may be encountered by the  end targets of a computerized process is assessed.  How many and what type of errors occur?  How easily are they corrected?  What is the reaction of consumers to errors-to a failure to correct them?  What can be learned by designers of large management packages from such data? Results show that with the present state of the art, approximately 40 percent of individuals (or households) having average contacts with different types of accounts experience one or more errors per year.  Eighty percent relate to billing.  Attempts to correct errors often turned out to be difficult and not always successful. There appears to be some conflict between computer-using organizations and their public.  Also the role of poor man agement packages including poor software is indicated.  While most management systems may be adequate, results of the survey raise concerns about the timeliness and the number of designs of very large linked program packages (as EFT for instance).	1979	Sterling	Errors, systems errors, billing errors, management systems, consumers
3143	Reasoning About Arrays	A variety of concepts, laws, and notations are presented which facilitate reasoning about arrays.  The basic concepts include intervals and their partitions, functional restriction, images, pointwise extension of relations, ordering, single-point variation of functions, various equivalence relations  for array values, and concatenation.  The effectiveness of these ideas is illustrated by informal descriptions of algorithms for binary search and merging, and by a short formal proof.	1979	Reynolds	Arrays, assertions, program proving, intervals, partitions, pointwise extension, ordering, concatenation, binary search, merging
3144	A Model for and DIscussion of Multi-Interpreter Systems	A multi-interpreter system is a system in which programs execute by virtue of being interpreted by other programs, which themselves may either be interpreted (i.e. nested interpreters) or run directly on the host machine.  The model reveals the anatomy of interpreters and how these differ from procedures, and exhibits links to protection domains and multiprocessor architectures.	1979	Manthey	Interpreters, transfer-of-control, hierarchies
3145	An Implementation of Structured Walk-Throughs in Teaching Cobol Programming	The effectiveness of structured walk-throughs in teaching introductory Cobol programming was empirically assessed with a sample of 215 under-graduate business administration majors.  Cobol proficiency was measured by a final examination testing (a) knowledge of language rules, (b) ability to read and debug a program, and (c) the ability to write a program.  Analysis of multiple covariance was used to statistically adjust test scores for age and conditional reasoning scores. The findings provide empirical support for incorporating structured walk-throughs into the programming learning process more effectively develop student proficiency in writing Cobol programs.	1979	Lemos	Structured walk-throughs, Cobol programming, teaching of programming, testing programming proficiency
3146	An Academic Program Providing Realistic Training in Software Engineering	An academic program at Harvey Mudd College, called the Clinic program, brings projects from industry on  campus to be studied and solved by student teams.  The objective of the Clinic is to provide students, working as small teams under  careful faculty supervision, an opportunity to work on real world problems of sufficient magnitude and complexity.  Under this program, students can acquire essential skills of software engineering, such as team work, software project management, software design methodology, and communication skills, in a realistic environment. Sample software projects undertaken by the Clinic are described.  Experience so far has shown that the program is a viable transition from an academic to industrial world.	1979	Busenberg, Tam	Software engineering, software engineering education, software projects, student teams, software engineering skills
3147	A Model for Automating File and Program Design in Business Application Systems	This paper discusses a model for finding an efficient implementation of a  business application system whose logical specifications have been determined in advance.  The model views file and program design as a problem of systematically coordinating the configurations of datasets and computations.  It uses a straight forward search technique to determine aggregations of computations, aggregations of datasets, device, organization, and key order for each data set, key order for  each computation, and access method  for each dataset-computation pair.  Although computational results are presented for a sample problem involving 54 computations and 49 datasets, the main point of the paper is that the underlying model works computationally an d is simple enough to be adapted to many file design situations.	1979	Alter	System design, automatic programming, search methods, system configurations, design choices
3148	High Level Programming for Distributed Computing	Programming for distributed and other loosely coupled systems is a problem of growing interest.  This paper describes an approach to distributed computing at the level of general purpose programming languages.  Based on primitive notions of module, message, and transaction key, the methodology is shown to be independent of particular languages and machines.  It appears to be useful for programming a wide range of tasks.  This is part of an ambitious program of development in advanced programming languages, and relations with other aspects of the project are also discussed.	1979	Feldman	Distributed computing, modules, messages, assertions
3149	The Cyclic Order Property of Vertices as an Aid in Scene Analysis	A cyclic-order property is defined for bodies bounded by smooth-curved faces. The property is shown to be useful for analyzing pictures of such bodies, particularly when the line data extracted from the pictures are imperfect. This property augments previously known grammatical rules that determine the existence of three-dimensional bodies corresponding to given two-dimensional line-structure data.	1979	Shapira, Freeman	Scene analysis, cyclic order, artificial intelligence, three-dimensional reconstruction , picture processing, computer graphics, pattern recognition.
3150	Beyond Programming Languages	As computer technology matures, our growing ability to create large systems is  leading to basic changes in the nature of programming.  Current programming  language concepts will not be adequate for building and maintaining systems of the complexity called for by the tasks we attempt.  Just as high level languages enabled the programmer to escape from the intricacies of a machine's order code, higher level programming systems can provide the means to understand and manipulate complex systems and components.  In order to develop such systems, we need to shift our attention away from the detailed specification of algorithms, towards the description of the properties of the packages and objects with which we build.  This paper analyzes some of the shortcomings of programming languages as they now exist, and lays out some possible directions for future research.	1979	Winograd	Programming, programming languages, programming systems, systems development
3151	An Optimal Real-Time Algorithm for Planar Convex Hulls	An algorithm is described for the construction in real-time of the convex hull of a set of n points in the plane.   Using an appropriate data structure, the algorithm constructs the convex hull by successive updates, each taking time O(log n), thereby achieving a total processing time O(n log n).	1979	Preparata	Computational geometry, convex hull, planar set of points, real-time algorithms, on-line algorithms.
3152	Storage Reorganization Techniques for Matrix Computation in a Paging Environment	In order to multiply matrices while minimizing the number of page fetches required, it is often more efficient to reorganize the data into submatrix form and to use block multiplication  rather than to use the best known algorithms which leave the matrices stored in row-(or column-)oriented form.  An efficient method for accomplishing this reorganization is given.  This also makes possible the derivation of an asymptotically better bound for multiplication of matrices given in row-oriented form by adapting the technique of Strassen to the reorganized data.  The reorganization/block  multiplication scheme is shown to be advantageous for matrices and pages of realistic size; the Strassen adaptation is not.  The former scheme is also shown to be advantageous even if the transpose of one of the matrices is available at no additional cost.	1979	Fischer, Probert	Matrix multiplication, paging, virtual memory, data reorganization, pagination, transpose.
3153	The Control of Response Times in Multi-Class Systems by Memory Allocations	The possibility of giving different quality of service to jobs of different classes by regulating their memory allocation is examined in the context of a paged computer system.  Two parameterized algorithms which partition the main memory between two classes of jobs are considered.  Initially, a closed system consisting of a process or and paging and file devices, with fixed numbers of jobs, is studied to determine optimal degrees of multiprogramming and the proportion of processor time devoted to each class.  Applying a decomposition approach and treating the closed system as a single server, the response times in an open system with external arrivals are studied.  The object is to investigate the effect of the memory alocation parameters on the expected response times under the two algorithms. Numerical solutions and economical lower bounds for the expected response times as functions of the control parameters are obtained.  A way of applying the results to systems with more than two job classes is indicated.	1979	Hine, Mitrani, Tsur	Queueing networks, paging, virtual memory, performance control
3154	Algorithm = Logic + Control	An algorithm can be regarded as consisting of a logic component, which specifies the knowledge to be used in solving problems, and a control component, which determines the problem-solving strategies by means of which that knowledge is used.  The logic component determines the meaning of the algorithm whereas the control component only affects its efficiency.  The efficiency of an algorithm can often by improving the control component without changing the logic of the algorithm.  We argue that computer programs would be more often correct and more easily improved and modified if their logic and control aspects were identified and separated in the program text.	1979	Kowalski	Control language, logic programming, nonprocedural language, programming methodology, program specification, relational data structures
3155	The Paradigms of Programming		1979	Floyd	
3156	Computing Connected Components on Parallel Computers	We present a parallel algorithm which uses n2 processors to find the connected components of an undirected graph with n vertices in time O(log2n).  An O(log2n) time bound also can be achieved using only n$n/$log2n)) processors. The algorithm can be used to find the transitive closure of a symmetric Boolean matrix.  We assume that the processors have access to a common memory.  Simultaneous access to the same location is permitted for fetch instructions but not for store instructions.	1979	Hirschberg, Chandra, Sarwate	Graph theory, parallel processing, algorithms, transitive closure, connected component
3157	Proving Termination with Multiset Orderings	A common tool for proving the termination of programs is the well-founded set, a set ordered in such a way as to admit no infinite descending sequences. The basic approach is to find a termination function  that maps the values of the program variables into some well-founded set, such that the value of the termination function is repeatedly reduced throughout the computation.  All too often, the termination functions required  are difficult to find and are of a complexity out of proportion to the program under consideration. Multisets (bags) over a given well-founded set S are sets that admit multiple occurrences of elements taken from S.  The given ordering on S induces an ordering on the finite multisets over S.  This multiset ordering is shown to be well-founded.  The multiset ordering enables the use of relatively simple and intuitive termination functions in otherwise difficult termination proofs.  In particular, the multiset ordering is used to prove the termination of production systems, programs defined in terms of sets of rewriting rules.	1979	Dershowitz, Manna	Program correctness, program termination, program verification, well-founded orderings, well-founded sets, multisets, bags, production systems, term rewriting systems, tree replacement systems, reduction rules
3158	Secure Personal Computing in an Insecure Network	A method for implementing secure personal computing in a network with one or more central facilities is proposed.  The method employs a public-key encryption device and hardware keys.  Each user is responsible  for his own security and need not rely on the security of the central facility or the communication links.  A user can safely store confidential files in the central facility or transmit confidential  data to other users on the network.	1979	Denning	Personal computing, security, privacy, networks, public-key encryption
3159	Further Remark on Stably Updating Mean and Standard Deviation Estimates		1979	Nelson	Mean, standard deviation
3160	Rejuvenating Experimental Computer Science	This report is based on the results of an NSF sponsored workshop held in Wasington, D. on November 2, 1978.  The co-authors of the  report are: Gordon Bell, Digital Equipment Corporation; Bernard A. Galler, University of Michigan; Patricia Goldberg, IBM Corporation; John Hamblen, University of Missouri at Rolla; Elliot Pinson, Bell Telephone Laboratories; and Ivan Sutherland, California Institute of Technology.  Also participating in the workshop were representatives of NSF and other government agencies.  In addition to the authors, a number of other people have contributed to the contents of this report.  In preparation for the original workshop, all doctorate-granting  computer science departments in the nation were asked for comments and suggestions on the problems of experimental computer science. A version of the current report dated January 15 was circulated to these  departments and to a number of industrial and government groups for criticism. The editors and authors of this final version gratefully acknowledge the  contribution of a large number of other people at all stages in the preparation  of the report. $Note: Following this presentation of the report, there is a position paper on the crisis in experimental computer science written by the ACM Executive Committee.)	1979	Feldman, Sutherland	
3161	An ACM Executive Committee Position on the Crisis  in Experimental Computer Science		1979		McCracken, D., CCP Denning, P. Brandin, D.
3162	On Improving the Worst Case Running Time of the Boyer-Moore String Matching Algorithm	It is shown how to modify the Boyer-Moore string matching algorithm so that its worst case running time is linear even when multiple occurrences of the  pattern are present in the text.	1979	Galil	Computational complexity, linear time, worst case, string matching, periodicity
3163	An Optimal Insertion Algorithm for One-Sided Height-Balanced BInary Search Trees	An algorithm for inserting an element into a one-sided height-balanced (OSHB) binary search tree is presented.  The algorithm operates in time  O(log n), where n is the number of nodes in the tree.  This represents an improvement over the best previous ly known insertion algorithms of Hirschberg and Kosaraju, which require time O(log 2n).  Moreover, the O(log n) complexity is optimal. Earlier  results have shown that deletion in such a structure can also be performed in O(log n) time.  Thus the result of this paper gives a negative answer to the question of whether such trees should be the first examples of their kind, where deletion has a smaller time  complexity than insertion.  Furthermore, it can now be concluded that insertion, deletion, and retrieval in OSHB trees can be performed in the same time as the corresponding operations for the more general AVL trees, to within a constant factor.  However, the insertion and deletion algorithms for OSHB trees appear much more complicated than the corresponding algorithms for AVL trees.	1979	Raiha, Zweben	Insertion, one-sided height-balanced trees, height-balanced trees, binary trees, search trees.
3164	Progressive Acyclic Digraphs-A Tool for Database Integrity	A progressive acyclic digraph (PAD) algorithm accepts are requests and maintains a graph in an acyclic state.  When a request creates a cycle, nodes are, "detached" until the new are can be entered acyclically This process is important in certain areas of database implementation in which there are constraints on the permissible sequences of actions. Two PAD algorithms are presented; one uses a simple path matrix representation and the other uses a list with an "artificial gradient."  Experiments suggest that for large N the second is considerably faster, though both are asymptotically O(NR), where N is the number of nodes and R is the expected number of nodes reachable along paths from any given node.	1979	Hansen	List processing, data structures, topological sort, acyclic digraph, database integrity, network, deadlock
3165	Approximation of Polygonal Maps by Cellular Maps	The approximation of polygonal thematic maps by cellular maps, an important operation in geographical data processing, is analyzed.  The data organization used for representing the polygonal maps is a widely used segment-based data structure, where class labels identify the regions bordering each segment on either side. The approximation algorithm presented operates on such an organization, eliminating the need for the recognition of region boundaries. Each segment is examined only once.  The versatility of the new organization is further illustrated by the outline of algorithms for area computation and point inclusion.  The algorithm is applied to a set of soil maps converted to computer-readable form by means of a coordinate digitizer.	1979	Nagy, Wagle	Polygon maps, cellularization, gridding, geographic data structures, computational geometry, computer cartography, automated cartography
3166	Computing Standard Deviations: Accuracy	Four algorithms for the numerical computation of the standard deviation of (unweighted) sampled data are analyzed.  Two of the algorithms are well-known in the statistical and computational literature; the other two are new algorithms specifically intended for automatic computation.  Our discussion is  expository, with emphasis on reaching a suitable definition of "accuracy."  Each of the four algorithms is analyzed for the conditions under which it will be accurate.  We conclude that all four algorithms will provide accurate answers for many problems, but two of the algorithms, one new, one old, are substantially more accurate on difficult problems than are the other two.	1979	Chan, Lewis	Mean, standard deviation, least squares, updating estimates, rounding error analysis, condition number.
3167	Updating Mean and Variance Estimates: An Improved Method	A method of improved efficiency is given for updating the mean and variance of weighted sampled data when an additional data value is included in the set.  Evidence is presented that the method is stable and at least as accurate as the best existing updating method.	1979	West	Mean, standard deviation, variance, updating estimates, removing data
3168	Comment on "An Optimal Evaluation of Boolean Expressions in an Online Query System."		1979	Laird	Query, Boolean expression, information retrieval, file organization
3169	Note on "An Optimal Evaluation of Boolean Expressions  in an Online Query System."		1979	Gudes, Hoffman	Query, Boolean expression, optimal evaluation, information retrieval
3170	On the Proof of Correctness of a Calendar Program	A formal specification is given for a simple calendar program, and the derivation and proof of correctness of the program are  sketched.  The specification is easy to understand, and its correctness is  manifest to humans.	1979	Lamport	Program specification, program verification, inductive assertions
3171	Line Numbers Made Cheap	A technique is described for run-time line number administration to be used for implementations of high level languages.  Under suitable circumstances, this method requires absolutely no overhead, in either time or space, during execution of the program.	1979	Klint	Line number administration, diagnostic messages, abstract machine code
3172	An Algorithm for Planning Collision-Free Paths Among Polyhedral Obstacles	This paper describes a collision avoidance algorithm for planning a safe path for a polyhedral object moving among known polyhedral objects.  The algorithm transforms the obstacles so that they represent the locus of forbidden positions for an arbitrary reference point on the moving object.  A trajectory of this reference point which avoids all forbidden regions is free of collisions. Trajectories are found by searching a network which indicates, for each vertex  in the transformed obstacles, which other vertices can be reached safely.	1979	Lozano-Perez, Wesley	Path finding, collision-free paths, polyhedral objects, polyhedral obstacles, graph searching, growing objects
3173	A Psychology of Learning BASIC	This paper addresses the question: What does a person know following learning of BASIC programming?  Several underlying conceptual structures are identified: (1) a transaction is an event that occurs in the computer and involves some operation on some object at some location, (2) a prestatement is a set of transactions corresponding to a line of code, (3) chunks are frequently occurring  configurations of prestatements corresponding to several lines of code.	1979	Mayer	BASIC, Learning, instruction
3174	Password Security: A Case History	This paper describes the history of the design of the password security scheme on a remotely accessed time-sharing system. The present design was the result of countering observed attempts to penetrate the system.  The result is a compromise between extreme security and ease of use.	1979	Morris, Thompson	Operating systems, passwords, computer security
3175	Breaking Substitution Ciphers Using a Relaxation Algorithm	Substitution ciphers are codes in which each letter of the alphabet has one fixed substitute, and the word divisions  do not change.  In this paper the problem of breaking substitution ciphers is represented as a probabilistic labeling problem. Every code letter is assigned probabilities of representing plain text letters.  These probabilities are updated in parallel for all code letters, using joint letter probabilities.  Iterating the updating scheme results in improved estimates that finally lead to breaking the cipher.  The method is applies successfully to two examples.	1979	Peleg, Rosenfeld	Cryptography, substitution ciphers, probabilistic classification, relaxation
3176	Storing a Sparse Table	The problem of storing and searching large sparse tables is ubiquitous in  computer science.  The standard technique for storing such tables is hashing, but hashing has poor worst-case performance.  We propose a good worst-case method for storing a static table of n entries, each an integer between 0 and N - 1.  The method requires 0(n) w words of storage and allows O(logn N) access time.  Although our method is a little complicated to use in practice, our analysis shows why a simpler algorithm used for compressing LR parsing tables works so well.	1979	Tarjan, Yao	Gaussian elimination, parsing, searching, sparse matrix, table compression, table lookup
3177	How to Share a Secret	In this paper we show how to divide data D into n pieces in such a way that D is easily reconstructable from any k pieces, but even complete knowledge of k - 1 pieces reveals olutely no information about D.  This technique enables the construction of robust key management schemes for cryptographic systems that can function securely and reliably even when misfortunes destroy half the pieces and security breaches expose all but one of the remaining pieces.	1979	Shamir	Cryptography, key management, interpolation
3178	Introduction to the EFT Symposium		1979	Kling	
3179	Overview of the EFT Symposium	It is increasingly recognized that large-scale technologies such as EFT have the potential for aiding in the solution of current societal problems. Yet, these technologies also generate problems.  This symposium presents selected papers from a conference that sought to discover what is currently known about EFT impacts in society and what research is needed in the future.	1979	Kraemer, Colton	EFT's, research agenda, conference results, public policy
3180	Costs of the Current U. Payments System	Neither the banking industry nor public policy makers have good information on the comparative costs of alternative payment systems such as cash, checks, credit cards, and EFT transactions.  As a result, EFT systems and services are likely to be implemented without a valid assessment of whether they are cost-justified, lst alone justified in terms of other criteria.	1979	Lipis	EFT's, payment system costs, payment system volumes
3181	Public Protection and Education with EFT	Research has revealed the existence of widespread misinformation and lack of knowledge about EFT among business and government as well as consumers.  As a result, any effort to stimulate meaningful public participation in decisions on the introduction of EFT systems will require a coordinated educational effort of considerable scale.  In addition, research has revealed  shortcomings in the present system for defining responsibilities, liabilities, and avenues of recourse.  THis article presents several possible alternatives for improving the current system, but ongoing research is also needed to assure that actions taken will be responsive to the changing environment and consumer needs.	1979	Long	Electronic funds transfer systems, consumer education, security and fraud, privacy, system reliability, EFT ombudsman
3182	Vulnerabilities of EFTs to Intentionally Caused Losses	The hypothesis that consumers are provided greater accuracy and freedom from error and fraud with electronic funds transfer systems (EFTs) is discussed in light of the technical capabilities and potential of the computer to protect against both accidentally and intentionally caused losses. Although the nomenclature for business crimes remains the same as for manual depository and other financial service systems - for example,  fraud, theft, embezzlement - the characteristics of the crimes are new. The changes resulting from the accelerating use of EFTs and  its continual technological advances broaden the scope of security issues to be examined.  Factors such as backup requirements,  regulatory and legislative actions, and economics give rise to the urgency for immediate research into solutions for emerging EFTs - related vulnerabilities.	1979	Parker	EFTs, computer abuse, crime, security, errors, losses, positions of trust, legislation
3183	Policy, Values, and EFT Research: Anatomy of a Research Agenda	There is an emerging recognition that EFT systems have the potential to vastly alter the payment and fund transfer system in American society. A number of forces and actors are involved in this evolution, and the values vary significantly depending on individual and institutional  perspectives. These value conflicts are highlighted in a six-part research agenda: technological issues in EFT, EFT impacts  on people, economic impact of EFT, regulation and control of EFT, and evaluating and monitoring EFT systems.	1979	Kraemer, Colton	EFTs, research agenda, value conflicts, impacts on people, economic impacts, regulation and control, monitoring EFT
3184	Revised Report on the Algorithmic Language ALGOL 60	The report gives a complete defining description of the international algorithmic language ALGOL 60. This is a language suitable for expressing  a large class of numerical processes in a form sufficiently concise for  direct automatic translation into the language of programmed automatic computers.	1963	Nuar	
3185	The Humble Programmer	We shall do a much better programming job, provided that we approach the task with a full appreciation if its tremendous difficulty, provided that we  stick to modest and elegant programming languages, provided that we respect the intrinsic limitations of the human mind and approach the task as Very Humble Programmers.	1972	Dijkstra	
3186	GO TO Statement Considerd Harmful		1968	Dijkstra	go to statement, jump instruction, branch instruction, conditional clause, repetitive clause, program intelligibility, program sequencing
3187	Certification of Algorithm 271 (QUICKERSORT)	QUICKERSORT compiled and run without correction through the ALDEP translator for the CDC 1604A. Comparison of average sorting items with other recently published algorithms demonstrates QUICKERSORT's superior performance.	1966	Blair	
3188	Semiotics and Programming Languages	I have based my paper on semiotics and its three dimension. I should insert at this point that language has many aspects and that pragmatics, semantics and syntactics do not necessary cover all of them. One can, however, project most  aspects into the three semiotic dimension and there seems to be a strong  tendency to do so today.	1966	Zemanek	
3189	An Algebraic Compiler for the FORTRAN Assembly Program	An algebraic compiler has been written which may be added to the FORTRAN  Assembly Program. This compiler will expand all algebraic statements with the  following operations: addition, subtraction, multiplication and division. It will compile multi-level expressions in floating-point arithmetic (this is easily be revised to fixed-point).	1962	Stiegler	
3190	Correction to Economies of Scale and the IBM System/360	On page 439, a "typical" instruction mix id discussed and the timing computed as outlined in that page. Through an undetected programming error, the times and the resulting regression equation are slightly in error.	1967	Solomon	
3191	Generating Permutations by Nested Cycling	The purpose of this letter is two_fold: first to give due credit to the Tompkins-Paige algorithm, and second to clarify a comment by Hill, CR Review 13891 on "Programs for Permutations".	1968		Langdon, Glen G.
3192	The Lincoln Keyboard - a Typewriter Keyboard Designed  for Computers Input Flexibility	A new typewriter keyboard, for direct and punched paper tape computer input will replace the usual commercial keyboard with 88 characters chosen for the  convenience  of programmers. The Lincoln Keyboard is expected to facilitate the programming of algorithmic process and should allow considerable  flexibility in assembly and utility routines.	1958	Vanderburgh	
3193	Work is in progress on a formula coding technique allowing direct entry into the computer of formulae typed on an 84 character Flexo-writer. This Flexo-writer will be modified for automatic half-line advance and retract, without carriage return, to permit completely general sub and superscripting.		1958		
3194	A Non-heuristic Program for Proving Elementary Logical Theorems	The paper discusses problems involved in designing a device capable of distinguishing among speech events that are normally recognized as different  by native speakers of a particular language. Parallels between these problems and those of chemical analysis are pointed out.	1959	Dunham, Fridshal, Sward	
3195	Reiteration of ACM Policy Toward Standardization	The periodic change in officers, chairman and editors which usually follows as election occasionally results in a change in policy. In the case of this  department there is no radical change, but this is nevertheless the proper time to reiterate ans underline ACM's policy with respect to standardization in the  computer area.	1962	Gorn	
3196	The Reactive Typewriter Program	84-character keyboard including alphabetical upper and lower case for good readability. If the machine is restricted to only a single case, the lower case is preferred. The reactive typewriter should be portable. the reactive  typewriter should operate over any commercially used, dial-type telephone (voice) or telegraph (Telex) line or over leased (nondial) telegraph lines interchangeably.	1963	Mooers	
3197	Structures of Standards-Processing Organizations in the Computer Area	In line with the ACM's policy statement [Comm. ACM 5 (Nov. 1962), 547-549], the following organizational descriptions have been provided in order to describe standardization activities pertinent to computers and information processing.	1966	Gorn, Bemer, Green	
3198	Microprogramming, Emulators and Programming Languages	The problem we have been concerned with is that of converting language to action - or intellectual energy to mechanical energy. The medium that we use  for this purpose is language and therefore we are preoccupied with the subject of language. In the areas of language investigation we have concentrated first  on formalizing syntax and then on semantics.	1966	Greem	
3199	ALGEM - An Algebraic Manipulator	ALGEM is a package of subprograms written in Slip, FORTRAN IV and MAP 7094  II to manipulate algebraic expressions. Algem's basic algebraic operations are  additions, subtractions, multiplications, division and exponentiation. It is  capable of handling any number of single letter variables, variable exponents,  and of finding the highest common factor of two polynomials. Also included are such functions as substitution, differentiation, determining coefficients of specified variables, solving a linear equation, basic I/O routines plus other special purpose and arithmetic routines. The major innovation of Algem over  other manipulators is the assignment of types to all expressions and the use  of a standard ordering procedure.	1966	Gotlieb, Novak	
3200	A FORMAC Program for the Solution of Linear Boundary and Initial Value  Problems	A computer program is described which has been developed for obtaining approximate solutions to linear initial and boundary-value problems involving  differential equations. For each problem, input to the program includes:    1. The equations (in symbolic form) to be satisfied  -  the differential equations, equations describing auxiliary conditions such as boundary  conditions, etc.    2. A numerical description of the regions in which each of the equations are to be satisfied.    3. Sets of functions (in symbolic form) to be used in linear combinations to approximate the solution functions. Give the above input, the program generates an approximation to the solutions of the specified problemm in terms  of the specified functions which is optimum in the least-squares sense.	1966	Cuthill	
3201	Symbolic Manipulation of Poisson Series	Poisson series of three variables are manageable symbolically through as a  set of formal subroutines written partially in the IBM 7094 machine language, but to be called in the FORTRAN language for use in Fortran  programs. An  effort has been made to supply those operations which are most required by celestial mechanics. The routines are entirely self-contained subroutines and require only standard Fortran input/output units 5 and 6; they are design to avoid waste and overflow of core storage space.	1966	Danby, Deprit, Rom	
3202	MANIP: A Computer System for Algebra and Analytic Differentiation	A mathematical expression to be operated upon is written in FORTRAN-like  notation and stored in the computer as a string of BCD characters with all blanks removed. It may be as complicated as desired (parentheses nested without restriction, etc.) so long as the entire expression (or any subsequent form) does not exceed 5000 characters. The problemm of performing algebraic operations and obtaining analytic derivatives was translated into that of identifying and manipulating character sequences. Programs which resulted were written in FORTRAN IV for a CDC 3600 and are discussed in detail.	1966	Bender	
3203	GRAD Assistant - A Program for Symbolic Algebraic Manipulation and  Differentiation	The General Recursive Algebra and Differentiation Assistant (GRAD Assistant) now under development is a set of LISP functions which symbolically manipulate abd differentiate algebraic expressions. It is designed for use with problemms  in which a large amount of routine manipulation is to be done by a program  without human intervention. Thus, GRAD must recognize necessary simplifications without external guidance. While some complicated expressions (notably ones involving nested radicals and trigonometric functions) do not yield completely to the present version, it has proved quite useful indeed.	1966	Fletcher	
3204	An On-Line Program for Non-Numerical Algebra	The goal of this program is to make a step toward te design of an automated mathematical assistant. Some requirements for such a program are: it must be easy to access, and that the result must be obtained in a reasonably short time. Accordingly the program is written for a time-shared computer. The Q-32 computer as System Development Corporation, Santa Monica, California, was  chosen because it also had a LISP 1.5 compiler. Programming and debugging was done from a remote teletype console at Stanford University.	1966	Korsvold	
